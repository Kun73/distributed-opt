# Some results in asymptotic network independence{#sec:referasymnt}

## Compressed Communication

### CHOCO-SGD

@koloskova2019decentralized proposed a decentralized stochastic method Choco-SGD(see algorithm \@ref(exm:chocosgd)) that converges at rate 
\begin{equation}
\mathcal{O}(\frac{1}{nk}+\frac{1}{(\delta^2w^k)^2})
\end{equation} 
where $\delta=1-|\lambda_2(W)|$ denotes the spectral gap of mixing matrix $W$, $w\leq1$ is the compression quality factor. This method conserve the asymptotic network independence and can be applied to a network where the nodes compress their model update with quality $0<w\leq1$. Addtionnally, the noise introduced by compression operator vanishes as $k\to\infty$. 

Different from DSGD, Choco-SGD \@ref(exm:chocosgd) use the averaged iterate $x_{\mathrm{avg}}=\frac{1}{S}\sum\limits_{k=1}^{k_{\mathrm{stop}}}w_k\bar x(k)$, i.e., theorem \@ref(thm:chocorate),

```{theorem, chocorate}
Let assumptions \@ref(exr:muL) and \@ref(exr:chocoF) hold, algorithm \@ref(exm:chocosgd) with SGD stepsize $\alpha_k=\frac{4}{\mu(a+k)}$, $a\geq\max\{\frac{410}{\delta^2w},16\kappa\}$ for $\kappa=\frac{L}{\mu}$, and consensus stepsize $\gamma=\frac{\delta^{2} \omega}{16 \delta+\delta^{2}+4 \beta^{2}+2 \delta \beta^{2}-8 \delta \omega}$ converges with the rate 
$$
\mathbb{E} \left[f(x_{\mathrm{avg}})-f(x^*)\right]=\mathcal{O}\left(\frac{\bar{\sigma}^{2}}{\mu n T}\right)+\mathcal{O}\left(\frac{\kappa G^{2}}{\mu \omega^{2} \delta^{4} T^{2}}\right)+\mathcal{O}\left(\frac{G^{2}}{\mu \omega^{3} \delta^{6} T^{3}}\right)
$$
where $x_{\mathrm{avg}}=\frac{1}{S}\sum\limits_{k=0}^{k_{\mathrm{stop}}}w_k\bar x(k)$ with weight $w_k=(a+k)^2$ and $S=\sum\limits_{k=0}^{k_{\mathrm{stop}}}w_k$
```

In the proof of theorem \@ref(thm:chocorate), the difference is that the author @koloskova2019decentralized deal the term 
$$
\bar x(k)-\alpha_k h(\bar x(k))-x^* 
$$
differently. They estimate $\Vert h(\bar x(k))\Vert^2$ and $\langle\bar x(k)-x^*,h(\bar x(k))\rangle$(from lemma \@ref(lem:Lmini), we have $f(\bar x(k))-f(x^*)$), while in DSGD, we use lemma \@ref(lem:contraction2L). Then based on a lemma from @stich2018sparsified, they finish the proof.

```{lemma, Lmini}
If $f$ has $L-$Lipschitz gradient with minimizer $x^*,s.t.\nabla f(x^*)=\mathbf{0}$, then 
$$
  \|\nabla f(x)\|^{2}=\left\|\nabla f(x)-\nabla f\left(x^{\star}\right)\right\|^{2} \leq 2 L\left(f(x)-f\left(x^{\star}\right)\right)
$$
```

Let $Q(\cdot):\mathbb{R}^{p}\to\mathbb{R}^p$ be a specific compression operator, which is known and satisfy assumption \@ref(exr:contractionEQ). $f_i(x):=E_{\xi_i\sim\mathcal{D}_i} F_i(x,\xi_i)$ for a loss function $F_i:\mathcal{R}^p\times \Omega_\boldsymbol\xi$, $f_i$ satisfy assumption \@ref(exr:muL) and distribution $\mathcal{D_1},...,\mathcal{D_n}$ can be different on every node.

```{exercise,contractionEQ}
The compression operator $Q(\cdot):\mathbb{R}^{p}\to\mathbb{R}^p$ satisfies, 
$$
  E_{Q}\|Q(x)-x\|^{2} \leq(1-\omega)\|x\|^{2}, x\in\mathbb{R}^p,
$$
for a parameter $w>0$. $E_Q$ denotes the expectation over the internal randomness of operator $Q$
```

The CHOCO-SGD method can be seen in algorithm \@ref(exm:chocosgd),
```{example, chocosgd, name = "CHOCO-SGD"}
Initialize $x_i(0)\in \mathbb{R}^{p}, \hat x_i(0)=\mathbf{0}\in\mathbb{R}^{p}$ for $i\in\mathcal{N}$, consensus stepsize $\gamma$, SGD stepsize $\alpha_k\geq0$,and mixing matrix $W=(w_{ij})\in\mathbb{R}^{n\times n}$

For $k=0,1,...$ do in parallel for all agents $i\in\mathcal{N}$,

  * Sample $\xi_i(k)$, compute  $g_i(k)=\nabla F_i(x_i(k),\xi_i(k))$
  
  * $x_i(k+\frac{1}{2})=x_i(k)-\alpha_kg_i(k)$ (stochastic gradient)

  * $q_i(k)=Q(x_i(k)-\hat x_i(k))$ (compression operator)

  * For all the neighbor $j$ of agents $i$, i.e., $(i,j)\in\mathcal{E}$,

    * Send $q_i(k)$ and receive $q_j(k)$
  
    * $\hat x_j(k+1)=\hat x_j(k)+q_j(k)$
  
  * $x_i(k+1)=x_i(k+\frac{1}{2})+\gamma \sum_{j:(i,j)\in\mathcal{E}} w_{ij}(\hat x_j(k+1)-\hat x_i(k+1))$

```


When the compression quality $w=1$,i.e. no communication compression, and consensus stepsize $\gamma=1$, the updation in algorithm \@ref(exm:chocosgd) becomes 
$$
x_i(k+1)=\sum\limits_{i=1}^nw_{ij}(x_i(k)-\alpha_kg_i(k))
$$
which is the DSGD algorithm \@ref(exm:sgd). From the setting 
$$f_i(x):=E_{\xi_i\sim\mathcal{D}_i} F_i(x,\xi_i)$$
$g_i(k)$ satisify the unbiased property in assumption \@ref(exr:estg) automatically when $\nabla$ and expectation on $F_i$ can be interchanged. Despite the second property in assumption \@ref(exr:estg), we need to assum the second moment of $\nabla F_i(x,\xi_i)$ is finite,i.e., assumption \@ref(exr:chocoF). A little difference is that $\sigma$ can be different for each agent.

```{exercise, chocoF}
\begin{align}
E_{\xi_{i}}\left\|\nabla F_{i}\left(x, \xi_{i}\right)-\nabla f_{i}(x)\right\|^{2} &\leq \sigma_{i}^{2}, \quad \forall x \in \mathbb{R}^{d}, i \in\mathcal{N}\\
E_{\xi_{i}}\left\|\nabla F_{i}\left(x, \xi_{i}\right)\right\|^{2} &\leq G^{2},\quad\forall x \in \mathbb{R}^{d}, i \in\mathcal{N}
\end{align}

```

### Stochastic gradient push

The push sum gossip algorithm is robust to stragglers and communication delays. @assran2018stochastic prove that the stochastic gradient push(SGP) converges to a stationary point of smooth, **non-convex** objectives at the same sub-linear rate as SGD, and that all nodes achieve consensus.


