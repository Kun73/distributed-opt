<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Some results in asymptotic network independence | Notes about Distributed Optimization</title>
  <meta name="description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Some results in asymptotic network independence | Notes about Distributed Optimization" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Some results in asymptotic network independence | Notes about Distributed Optimization" />
  
  <meta name="twitter:description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  

<meta name="author" content="Kun Huang" />


<meta name="date" content="2020-04-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-asynt.html"/>
<link rel="next" href="sec-sharp.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="orgnization-of-the-notes.html"><a href="orgnization-of-the-notes.html"><i class="fa fa-check"></i><b>2</b> Orgnization of the Notes</a></li>
<li class="chapter" data-level="3" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html"><i class="fa fa-check"></i><b>3</b> The Push-Pull Method</a><ul>
<li class="chapter" data-level="3.1" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#analysis-of-convergence"><i class="fa fa-check"></i><b>3.2</b> Analysis of Convergence</a><ul>
<li class="chapter" data-level="3.2.1" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#relationship-between-two-iteration-steps"><i class="fa fa-check"></i><b>3.2.1</b> Relationship between two iteration steps</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#inequalities"><i class="fa fa-check"></i><b>3.2.2</b> Inequalities</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#spectral-radius-of-a"><i class="fa fa-check"></i><b>3.2.3</b> Spectral radius of A</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sec-dsgt.html"><a href="sec-dsgt.html"><i class="fa fa-check"></i><b>4</b> Distributed Stochastic Gradient Tracking(DSGT) Method</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-dsgt.html"><a href="sec-dsgt.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="sec-dsgt.html"><a href="sec-dsgt.html#analysis-of-convergence-1"><i class="fa fa-check"></i><b>4.2</b> Analysis of Convergence</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-dsgt.html"><a href="sec-dsgt.html#relationship-between-two-iteration-steps-1"><i class="fa fa-check"></i><b>4.2.1</b> Relationship between two iteration steps</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-dsgt.html"><a href="sec-dsgt.html#inequalities-1"><i class="fa fa-check"></i><b>4.2.2</b> Inequalities</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-dsgt.html"><a href="sec-dsgt.html#spectral-radius-of-a_dsgt"><i class="fa fa-check"></i><b>4.2.3</b> Spectral radius of <span class="math inline">\(A_{dsgt}\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary-of-push-pull-and-dsgt.html"><a href="summary-of-push-pull-and-dsgt.html"><i class="fa fa-check"></i><b>5</b> Summary of PuSh-Pull and DSGT</a><ul>
<li class="chapter" data-level="5.1" data-path="summary-of-push-pull-and-dsgt.html"><a href="summary-of-push-pull-and-dsgt.html#questions"><i class="fa fa-check"></i><b>5.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gossip-like-push-pull-and-dsgt.html"><a href="gossip-like-push-pull-and-dsgt.html"><i class="fa fa-check"></i><b>6</b> Gossip-like Push-Pull and DSGT</a><ul>
<li class="chapter" data-level="6.1" data-path="gossip-like-push-pull-and-dsgt.html"><a href="gossip-like-push-pull-and-dsgt.html#g-push-pull"><i class="fa fa-check"></i><b>6.1</b> G-Push-Pull</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sec-asynt.html"><a href="sec-asynt.html"><i class="fa fa-check"></i><b>7</b> Asymptotic network independence</a><ul>
<li class="chapter" data-level="7.1" data-path="sec-asynt.html"><a href="sec-asynt.html#sgd-and-dsgd"><i class="fa fa-check"></i><b>7.1</b> SGD and DSGD</a></li>
<li class="chapter" data-level="7.2" data-path="sec-asynt.html"><a href="sec-asynt.html#bounds"><i class="fa fa-check"></i><b>7.2</b> Bounds</a></li>
<li class="chapter" data-level="7.3" data-path="sec-asynt.html"><a href="sec-asynt.html#possible-ways-to-achieve-asymptotic-network-independece"><i class="fa fa-check"></i><b>7.3</b> Possible ways to achieve asymptotic network independece</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html"><i class="fa fa-check"></i><b>8</b> Some results in asymptotic network independence</a><ul>
<li class="chapter" data-level="8.1" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#compressed-communication"><i class="fa fa-check"></i><b>8.1</b> Compressed Communication</a><ul>
<li class="chapter" data-level="8.1.1" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#choco-sgd"><i class="fa fa-check"></i><b>8.1.1</b> CHOCO-SGD</a></li>
<li class="chapter" data-level="8.1.2" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#stochastic-gradient-push"><i class="fa fa-check"></i><b>8.1.2</b> Stochastic gradient push</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#d2"><i class="fa fa-check"></i><b>8.2</b> <span class="math inline">\(D^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sec-sharp.html"><a href="sec-sharp.html"><i class="fa fa-check"></i><b>9</b> A sharp estimate of the transient time of DSGD</a><ul>
<li class="chapter" data-level="9.1" data-path="sec-sharp.html"><a href="sec-sharp.html#uk-and-vk"><i class="fa fa-check"></i><b>9.1</b> <span class="math inline">\(U(k)\)</span> and <span class="math inline">\(V(k)\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="sec-sharp.html"><a href="sec-sharp.html#asymptotic-network-independence-of-dsgd"><i class="fa fa-check"></i><b>9.2</b> Asymptotic network independence of DSGD</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-sharp.html"><a href="sec-sharp.html#sublinear-rate"><i class="fa fa-check"></i><b>9.2.1</b> Sublinear rate</a></li>
<li class="chapter" data-level="9.2.2" data-path="sec-sharp.html"><a href="sec-sharp.html#asymptotic-network-independence"><i class="fa fa-check"></i><b>9.2.2</b> Asymptotic network independence</a></li>
<li class="chapter" data-level="9.2.3" data-path="sec-sharp.html"><a href="sec-sharp.html#improved-bound"><i class="fa fa-check"></i><b>9.2.3</b> Improved Bound</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="sec-sharp.html"><a href="sec-sharp.html#transient-time"><i class="fa fa-check"></i><b>9.3</b> Transient time</a></li>
<li class="chapter" data-level="9.4" data-path="sec-sharp.html"><a href="sec-sharp.html#sharpness"><i class="fa fa-check"></i><b>9.4</b> Sharpness</a></li>
<li class="chapter" data-level="9.5" data-path="sec-sharp.html"><a href="sec-sharp.html#summary"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="comparison.html"><a href="comparison.html"><i class="fa fa-check"></i><b>10</b> comparison</a><ul>
<li class="chapter" data-level="10.1" data-path="comparison.html"><a href="comparison.html#assumptions-of-different-schemes"><i class="fa fa-check"></i><b>10.1</b> Assumptions of different schemes</a></li>
<li class="chapter" data-level="10.2" data-path="comparison.html"><a href="comparison.html#convergence-rate"><i class="fa fa-check"></i><b>10.2</b> Convergence rate</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sec-ediff.html"><a href="sec-ediff.html"><i class="fa fa-check"></i><b>11</b> Exact diffusion</a><ul>
<li class="chapter" data-level="11.1" data-path="sec-ediff.html"><a href="sec-ediff.html#decreasing-stepsize"><i class="fa fa-check"></i><b>11.1</b> Decreasing stepsize</a><ul>
<li class="chapter" data-level="11.1.1" data-path="sec-ediff.html"><a href="sec-ediff.html#preliminary-results"><i class="fa fa-check"></i><b>11.1.1</b> Preliminary results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sec-PUDA.html"><a href="sec-PUDA.html"><i class="fa fa-check"></i><b>12</b> Decentralized Proximal Gradient Algorithms with Linear Convergence Rates</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-PUDA.html"><a href="sec-PUDA.html#uda"><i class="fa fa-check"></i><b>12.1</b> UDA</a></li>
<li class="chapter" data-level="12.2" data-path="sec-PUDA.html"><a href="sec-PUDA.html#puda"><i class="fa fa-check"></i><b>12.2</b> PUDA</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="nids.html"><a href="nids.html"><i class="fa fa-check"></i><b>13</b> NIDS</a></li>
<li class="chapter" data-level="14" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>14</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes about Distributed Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:referasymnt" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Some results in asymptotic network independence</h1>
<div id="compressed-communication" class="section level2">
<h2><span class="header-section-number">8.1</span> Compressed Communication</h2>
<div id="choco-sgd" class="section level3">
<h3><span class="header-section-number">8.1.1</span> CHOCO-SGD</h3>
<span class="citation">Koloskova, Stich, and Jaggi (<a href="#ref-koloskova2019decentralized">2019</a>)</span> proposed a decentralized stochastic method Choco-SGD(see algorithm <a href="sec-referasymnt.html#exm:chocosgd">8.1</a>) that converges at rate
<span class="math display">\[\begin{equation}
\mathcal{O}(\frac{1}{nk}+\frac{1}{(\delta^2w^k)^2})
\end{equation}\]</span>
<p>where <span class="math inline">\(\delta=1-|\lambda_2(W)|\)</span> denotes the spectral gap of mixing matrix <span class="math inline">\(W\)</span>, <span class="math inline">\(w\leq1\)</span> is the compression quality factor. This method conserve the asymptotic network independence and can be applied to a network where the nodes compress their model update with quality <span class="math inline">\(0&lt;w\leq1\)</span>. Addtionnally, the noise introduced by compression operator vanishes as <span class="math inline">\(k\to\infty\)</span>.</p>
<p>Different from DSGD, Choco-SGD <a href="sec-referasymnt.html#exm:chocosgd">8.1</a> use the averaged iterate <span class="math inline">\(x_{\mathrm{avg}}=\frac{1}{S}\sum\limits_{k=1}^{k_{\mathrm{stop}}}w_k\bar x(k)\)</span>, i.e., theorem <a href="sec-referasymnt.html#thm:chocorate">8.1</a>,</p>

<div class="theorem">
<span id="thm:chocorate" class="theorem"><strong>Theorem 8.1  </strong></span>Let assumptions <a href="index.html#exr:muL">1.1</a> and <a href="sec-referasymnt.html#exr:chocoF">8.2</a> hold, algorithm <a href="sec-referasymnt.html#exm:chocosgd">8.1</a> with SGD stepsize <span class="math inline">\(\alpha_k=\frac{4}{\mu(a+k)}\)</span>, <span class="math inline">\(a\geq\max\{\frac{410}{\delta^2w},16\kappa\}\)</span> for <span class="math inline">\(\kappa=\frac{L}{\mu}\)</span>, and consensus stepsize <span class="math inline">\(\gamma=\frac{\delta^{2} \omega}{16 \delta+\delta^{2}+4 \beta^{2}+2 \delta \beta^{2}-8 \delta \omega}\)</span> converges with the rate <span class="math display">\[
\mathbb{E} \left[f(x_{\mathrm{avg}})-f(x^*)\right]=\mathcal{O}\left(\frac{\bar{\sigma}^{2}}{\mu n T}\right)+\mathcal{O}\left(\frac{\kappa G^{2}}{\mu \omega^{2} \delta^{4} T^{2}}\right)+\mathcal{O}\left(\frac{G^{2}}{\mu \omega^{3} \delta^{6} T^{3}}\right)
\]</span> where <span class="math inline">\(x_{\mathrm{avg}}=\frac{1}{S}\sum\limits_{k=0}^{k_{\mathrm{stop}}}w_k\bar x(k)\)</span> with weight <span class="math inline">\(w_k=(a+k)^2\)</span> and <span class="math inline">\(S=\sum\limits_{k=0}^{k_{\mathrm{stop}}}w_k\)</span>
</div>

<p>In the proof of theorem <a href="sec-referasymnt.html#thm:chocorate">8.1</a>, the difference is that the author <span class="citation">Koloskova, Stich, and Jaggi (<a href="#ref-koloskova2019decentralized">2019</a>)</span> deal the term <span class="math display">\[
\bar x(k)-\alpha_k h(\bar x(k))-x^* 
\]</span> differently. They estimate <span class="math inline">\(\Vert h(\bar x(k))\Vert^2\)</span> and <span class="math inline">\(\langle\bar x(k)-x^*,h(\bar x(k))\rangle\)</span>(from lemma <a href="sec-referasymnt.html#lem:Lmini">8.1</a>, we have <span class="math inline">\(f(\bar x(k))-f(x^*)\)</span>), while in DSGD, we use lemma <a href="sec-sharp.html#lem:contraction2L">9.2</a>. Then based on a lemma from <span class="citation">Stich, Cordonnier, and Jaggi (<a href="#ref-stich2018sparsified">2018</a>)</span>, they finish the proof.</p>

<div class="lemma">
<span id="lem:Lmini" class="lemma"><strong>Lemma 8.1  </strong></span>If <span class="math inline">\(f\)</span> has <span class="math inline">\(L-\)</span>Lipschitz gradient with minimizer <span class="math inline">\(x^*,s.t.\nabla f(x^*)=\mathbf{0}\)</span>, then <span class="math display">\[
  \|\nabla f(x)\|^{2}=\left\|\nabla f(x)-\nabla f\left(x^{\star}\right)\right\|^{2} \leq 2 L\left(f(x)-f\left(x^{\star}\right)\right)
\]</span>
</div>

<p>Let <span class="math inline">\(Q(\cdot):\mathbb{R}^{p}\to\mathbb{R}^p\)</span> be a specific compression operator, which is known and satisfy assumption <a href="sec-referasymnt.html#exr:contractionEQ">8.1</a>. <span class="math inline">\(f_i(x):=E_{\xi_i\sim\mathcal{D}_i} F_i(x,\xi_i)\)</span> for a loss function <span class="math inline">\(F_i:\mathcal{R}^p\times \Omega_\boldsymbol\xi\)</span>, <span class="math inline">\(f_i\)</span> satisfy assumption <a href="index.html#exr:muL">1.1</a> and distribution <span class="math inline">\(\mathcal{D_1},...,\mathcal{D_n}\)</span> can be different on every node.</p>

<div class="exercise">
<span id="exr:contractionEQ" class="exercise"><strong>Assumption8.1  </strong></span>The compression operator <span class="math inline">\(Q(\cdot):\mathbb{R}^{p}\to\mathbb{R}^p\)</span> satisfies, <span class="math display">\[
  E_{Q}\|Q(x)-x\|^{2} \leq(1-\omega)\|x\|^{2}, x\in\mathbb{R}^p,
\]</span> for a parameter <span class="math inline">\(w&gt;0\)</span>. <span class="math inline">\(E_Q\)</span> denotes the expectation over the internal randomness of operator <span class="math inline">\(Q\)</span>
</div>

The CHOCO-SGD method can be seen in algorithm <a href="sec-referasymnt.html#exm:chocosgd">8.1</a>, 
<div class="example">
<p><span id="exm:chocosgd" class="example"><strong>Algorithm8.1  (CHOCO-SGD)  </strong></span>Initialize <span class="math inline">\(x_i(0)\in \mathbb{R}^{p}, \hat x_i(0)=\mathbf{0}\in\mathbb{R}^{p}\)</span> for <span class="math inline">\(i\in\mathcal{N}\)</span>, consensus stepsize <span class="math inline">\(\gamma\)</span>, SGD stepsize <span class="math inline">\(\alpha_k\geq0\)</span>,and mixing matrix <span class="math inline">\(W=(w_{ij})\in\mathbb{R}^{n\times n}\)</span></p>
<p>For <span class="math inline">\(k=0,1,...\)</span> do in parallel for all agents <span class="math inline">\(i\in\mathcal{N}\)</span>,</p>
<ul>
<li><p>Sample <span class="math inline">\(\xi_i(k)\)</span>, compute <span class="math inline">\(g_i(k)=\nabla F_i(x_i(k),\xi_i(k))\)</span></p></li>
<li><p><span class="math inline">\(x_i(k+\frac{1}{2})=x_i(k)-\alpha_kg_i(k)\)</span> (stochastic gradient)</p></li>
<li><p><span class="math inline">\(q_i(k)=Q(x_i(k)-\hat x_i(k))\)</span> (compression operator)</p></li>
<li><p>For all the neighbor <span class="math inline">\(j\)</span> of agents <span class="math inline">\(i\)</span>, i.e., <span class="math inline">\((i,j)\in\mathcal{E}\)</span>,</p>
<ul>
<li><p>Send <span class="math inline">\(q_i(k)\)</span> and receive <span class="math inline">\(q_j(k)\)</span></p></li>
<li><p><span class="math inline">\(\hat x_j(k+1)=\hat x_j(k)+q_j(k)\)</span></p></li>
</ul></li>
<li><span class="math inline">\(x_i(k+1)=x_i(k+\frac{1}{2})+\gamma \sum_{j:(i,j)\in\mathcal{E}} w_{ij}(\hat x_j(k+1)-\hat x_i(k+1))\)</span></li>
</ul>
</div>

<p>When the compression quality <span class="math inline">\(w=1\)</span>,i.e. no communication compression, and consensus stepsize <span class="math inline">\(\gamma=1\)</span>, the updation in algorithm <a href="sec-referasymnt.html#exm:chocosgd">8.1</a> becomes <span class="math display">\[
x_i(k+1)=\sum\limits_{i=1}^nw_{ij}(x_i(k)-\alpha_kg_i(k))
\]</span> which is the DSGD algorithm <a href="sec-asynt.html#exm:sgd">7.2</a>. From the setting <span class="math display">\[f_i(x):=E_{\xi_i\sim\mathcal{D}_i} F_i(x,\xi_i)\]</span> <span class="math inline">\(g_i(k)\)</span> satisify the unbiased property in assumption <a href="sec-dsgt.html#exr:estg">4.1</a> automatically when <span class="math inline">\(\nabla\)</span> and expectation on <span class="math inline">\(F_i\)</span> can be interchanged. Despite the second property in assumption <a href="sec-dsgt.html#exr:estg">4.1</a>, we need to assum the second moment of <span class="math inline">\(\nabla F_i(x,\xi_i)\)</span> is finite,i.e., assumption <a href="sec-referasymnt.html#exr:chocoF">8.2</a>. A little difference is that <span class="math inline">\(\sigma\)</span> can be different for each agent.</p>

<div class="exercise">
<span id="exr:chocoF" class="exercise"><strong>Assumption8.2  </strong></span>
<span class="math display">\[\begin{align}
E_{\xi_{i}}\left\|\nabla F_{i}\left(x, \xi_{i}\right)-\nabla f_{i}(x)\right\|^{2} &amp;\leq \sigma_{i}^{2}, \quad \forall x \in \mathbb{R}^{d}, i \in\mathcal{N}\\
E_{\xi_{i}}\left\|\nabla F_{i}\left(x, \xi_{i}\right)\right\|^{2} &amp;\leq G^{2},\quad\forall x \in \mathbb{R}^{d}, i \in\mathcal{N}
\end{align}\]</span>
</div>

</div>
<div id="stochastic-gradient-push" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Stochastic gradient push</h3>
<p>The push sum gossip algorithm is robust to stragglers and communication delays. <span class="citation">Assran et al. (<a href="#ref-assran2018stochastic">2018</a>)</span> propose a variant of stochastic gradient push(SGP), called overlap SGP, converges to a stationary point of smooth, <strong>non-convex</strong> objectives at an <span class="math inline">\(\mathcal{O}(\frac{1}{\sqrt{nK}})\)</span>, and that all nodes achieve consensus. Additionally, we use a sequence of mixing matrice <span class="math inline">\(W_k\)</span> here.</p>
We uses a different convergence criteria<span class="citation">(Lian et al. <a href="#ref-lian2017can">2017</a>)</span>, i.e., for the number of iterations <span class="math inline">\(K\)</span>,
<span class="math display" id="eq:lian">\[\begin{equation}
\frac{1}{K} \sum_{k=1}^{K} \mathbb{E}\left\|\nabla f\left(\bar{{x}}(k)\right)\right\|^{2} \leq \varepsilon, \forall \varepsilon
\tag{8.1}
\end{equation}\]</span>
</div>
</div>
<div id="d2" class="section level2">
<h2><span class="header-section-number">8.2</span> <span class="math inline">\(D^2\)</span></h2>
Global scheme, <span class="math display">\[
X_1=W(X_0-\alpha G_0)\\
X_{k+1}=W(X_k-\alpha G_k)+W(X_k-X_{k-1}-\alpha G_{k-1})
\]</span> then we have, <span class="math display">\[
\bar x_{k+1} = \bar x_k - \alpha G_k
\]</span> Thus if we assume each <span class="math inline">\(f_i\)</span> is strongly convex, we can still use lemma <a href="sec-sharp.html#lem:Uk1">9.3</a>. However, for the expected consensus error <span class="math inline">\(V(k)=E\left[\Vert X_{k}-\mathbf{1}\bar x_k\right]\)</span>, we need to use,
<span class="math display" id="eq:d2Vk1">\[\begin{align}
X_{k+1}-\mathbf{1}\bar x_{k+1}&amp;=
W(2X_k-X_{k-1}-\alpha G_k+\alpha G_{k-1})-\mathbf{1}(\bar x_k-\alpha \bar g_k)\\
&amp;=2(WX_{k}-\mathbf{1}\bar x_k)-\alpha (WG_k-\mathbf{1}\bar g_k)\\
&amp;\quad -(WX_{k-1}-\mathbf{1}\bar x_{k-1}) +\alpha(WG_{k-1}-\bar g_{k-1})
\tag{8.2}
\end{align}\]</span>
<p>then the following steps are similar to those in the proof of lemma <a href="sec-sharp.html#lem:Vk1">9.4</a>. Then if we can uniformly bound <span class="math inline">\(R&#39;(k)\)</span>, we can derive the convergence rate of <span class="math inline">\(D^2\)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-assran2018stochastic">
<p>Assran, Mahmoud, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat. 2018. “Stochastic Gradient Push for Distributed Deep Learning.” <em>arXiv Preprint arXiv:1811.10792</em>.</p>
</div>
<div id="ref-koloskova2019decentralized">
<p>Koloskova, Anastasia, Sebastian U Stich, and Martin Jaggi. 2019. “Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication.” <em>arXiv Preprint arXiv:1902.00340</em>.</p>
</div>
<div id="ref-lian2017can">
<p>Lian, Xiangru, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. 2017. “Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent.” In <em>Advances in Neural Information Processing Systems</em>, 5330–40.</p>
</div>
<div id="ref-stich2018sparsified">
<p>Stich, Sebastian U, Jean-Baptiste Cordonnier, and Martin Jaggi. 2018. “Sparsified Sgd with Memory.” In <em>Advances in Neural Information Processing Systems</em>, 4447–58.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-asynt.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-sharp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["DistributedOpt.pdf", "DistributedOpt.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
