<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Exact diffusion | Notes about Distributed Optimization</title>
  <meta name="description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Exact diffusion | Notes about Distributed Optimization" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Exact diffusion | Notes about Distributed Optimization" />
  
  <meta name="twitter:description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  

<meta name="author" content="Kun Huang" />


<meta name="date" content="2020-04-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="comparison.html"/>
<link rel="next" href="sec-PUDA.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="orgnization-of-the-notes.html"><a href="orgnization-of-the-notes.html"><i class="fa fa-check"></i><b>2</b> Orgnization of the Notes</a></li>
<li class="chapter" data-level="3" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html"><i class="fa fa-check"></i><b>3</b> The Push-Pull Method</a><ul>
<li class="chapter" data-level="3.1" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#analysis-of-convergence"><i class="fa fa-check"></i><b>3.2</b> Analysis of Convergence</a><ul>
<li class="chapter" data-level="3.2.1" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#relationship-between-two-iteration-steps"><i class="fa fa-check"></i><b>3.2.1</b> Relationship between two iteration steps</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#inequalities"><i class="fa fa-check"></i><b>3.2.2</b> Inequalities</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#spectral-radius-of-a"><i class="fa fa-check"></i><b>3.2.3</b> Spectral radius of A</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sec-dsgt.html"><a href="sec-dsgt.html"><i class="fa fa-check"></i><b>4</b> Distributed Stochastic Gradient Tracking(DSGT) Method</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-dsgt.html"><a href="sec-dsgt.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="sec-dsgt.html"><a href="sec-dsgt.html#analysis-of-convergence-1"><i class="fa fa-check"></i><b>4.2</b> Analysis of Convergence</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-dsgt.html"><a href="sec-dsgt.html#relationship-between-two-iteration-steps-1"><i class="fa fa-check"></i><b>4.2.1</b> Relationship between two iteration steps</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-dsgt.html"><a href="sec-dsgt.html#inequalities-1"><i class="fa fa-check"></i><b>4.2.2</b> Inequalities</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-dsgt.html"><a href="sec-dsgt.html#spectral-radius-of-a_dsgt"><i class="fa fa-check"></i><b>4.2.3</b> Spectral radius of <span class="math inline">\(A_{dsgt}\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary-of-push-pull-and-dsgt.html"><a href="summary-of-push-pull-and-dsgt.html"><i class="fa fa-check"></i><b>5</b> Summary of PuSh-Pull and DSGT</a><ul>
<li class="chapter" data-level="5.1" data-path="summary-of-push-pull-and-dsgt.html"><a href="summary-of-push-pull-and-dsgt.html#questions"><i class="fa fa-check"></i><b>5.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gossip-like-push-pull-and-dsgt.html"><a href="gossip-like-push-pull-and-dsgt.html"><i class="fa fa-check"></i><b>6</b> Gossip-like Push-Pull and DSGT</a><ul>
<li class="chapter" data-level="6.1" data-path="gossip-like-push-pull-and-dsgt.html"><a href="gossip-like-push-pull-and-dsgt.html#g-push-pull"><i class="fa fa-check"></i><b>6.1</b> G-Push-Pull</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sec-asynt.html"><a href="sec-asynt.html"><i class="fa fa-check"></i><b>7</b> Asymptotic network independence</a><ul>
<li class="chapter" data-level="7.1" data-path="sec-asynt.html"><a href="sec-asynt.html#sgd-and-dsgd"><i class="fa fa-check"></i><b>7.1</b> SGD and DSGD</a></li>
<li class="chapter" data-level="7.2" data-path="sec-asynt.html"><a href="sec-asynt.html#bounds"><i class="fa fa-check"></i><b>7.2</b> Bounds</a></li>
<li class="chapter" data-level="7.3" data-path="sec-asynt.html"><a href="sec-asynt.html#possible-ways-to-achieve-asymptotic-network-independece"><i class="fa fa-check"></i><b>7.3</b> Possible ways to achieve asymptotic network independece</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html"><i class="fa fa-check"></i><b>8</b> Some results in asymptotic network independence</a><ul>
<li class="chapter" data-level="8.1" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#compressed-communication"><i class="fa fa-check"></i><b>8.1</b> Compressed Communication</a><ul>
<li class="chapter" data-level="8.1.1" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#choco-sgd"><i class="fa fa-check"></i><b>8.1.1</b> CHOCO-SGD</a></li>
<li class="chapter" data-level="8.1.2" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#stochastic-gradient-push"><i class="fa fa-check"></i><b>8.1.2</b> Stochastic gradient push</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#d2"><i class="fa fa-check"></i><b>8.2</b> <span class="math inline">\(D^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sec-sharp.html"><a href="sec-sharp.html"><i class="fa fa-check"></i><b>9</b> A sharp estimate of the transient time of DSGD</a><ul>
<li class="chapter" data-level="9.1" data-path="sec-sharp.html"><a href="sec-sharp.html#uk-and-vk"><i class="fa fa-check"></i><b>9.1</b> <span class="math inline">\(U(k)\)</span> and <span class="math inline">\(V(k)\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="sec-sharp.html"><a href="sec-sharp.html#asymptotic-network-independence-of-dsgd"><i class="fa fa-check"></i><b>9.2</b> Asymptotic network independence of DSGD</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-sharp.html"><a href="sec-sharp.html#sublinear-rate"><i class="fa fa-check"></i><b>9.2.1</b> Sublinear rate</a></li>
<li class="chapter" data-level="9.2.2" data-path="sec-sharp.html"><a href="sec-sharp.html#asymptotic-network-independence"><i class="fa fa-check"></i><b>9.2.2</b> Asymptotic network independence</a></li>
<li class="chapter" data-level="9.2.3" data-path="sec-sharp.html"><a href="sec-sharp.html#improved-bound"><i class="fa fa-check"></i><b>9.2.3</b> Improved Bound</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="sec-sharp.html"><a href="sec-sharp.html#transient-time"><i class="fa fa-check"></i><b>9.3</b> Transient time</a></li>
<li class="chapter" data-level="9.4" data-path="sec-sharp.html"><a href="sec-sharp.html#sharpness"><i class="fa fa-check"></i><b>9.4</b> Sharpness</a></li>
<li class="chapter" data-level="9.5" data-path="sec-sharp.html"><a href="sec-sharp.html#summary"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="comparison.html"><a href="comparison.html"><i class="fa fa-check"></i><b>10</b> comparison</a><ul>
<li class="chapter" data-level="10.1" data-path="comparison.html"><a href="comparison.html#assumptions-of-different-schemes"><i class="fa fa-check"></i><b>10.1</b> Assumptions of different schemes</a></li>
<li class="chapter" data-level="10.2" data-path="comparison.html"><a href="comparison.html#convergence-rate"><i class="fa fa-check"></i><b>10.2</b> Convergence rate</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sec-ediff.html"><a href="sec-ediff.html"><i class="fa fa-check"></i><b>11</b> Exact diffusion</a><ul>
<li class="chapter" data-level="11.1" data-path="sec-ediff.html"><a href="sec-ediff.html#decreasing-stepsize"><i class="fa fa-check"></i><b>11.1</b> Decreasing stepsize</a><ul>
<li class="chapter" data-level="11.1.1" data-path="sec-ediff.html"><a href="sec-ediff.html#preliminary-results"><i class="fa fa-check"></i><b>11.1.1</b> Preliminary results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sec-PUDA.html"><a href="sec-PUDA.html"><i class="fa fa-check"></i><b>12</b> Decentralized Proximal Gradient Algorithms with Linear Convergence Rates</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-PUDA.html"><a href="sec-PUDA.html#uda"><i class="fa fa-check"></i><b>12.1</b> UDA</a></li>
<li class="chapter" data-level="12.2" data-path="sec-PUDA.html"><a href="sec-PUDA.html#puda"><i class="fa fa-check"></i><b>12.2</b> PUDA</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="nids.html"><a href="nids.html"><i class="fa fa-check"></i><b>13</b> NIDS</a></li>
<li class="chapter" data-level="14" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>14</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes about Distributed Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:ediff" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Exact diffusion</h1>
Consider the following problem,
<span class="math display" id="eq:eloss">\[\begin{equation}
x^*=\underset{x\in\mathbb{R}^p}{\arg\min}\sum_{i=1}^nf_i(x)\left(=E_{\xi_i}Q(x,\xi_i)\right)
\tag{11.1}
\end{equation}\]</span>
<p>The noisy gradient <span class="math inline">\(\nabla Q(x_{i,k},\xi_{i,k})\)</span> is more concrete than the <span class="math inline">\(g(x_{i,k},\xi_{i,k})\)</span> we use in the previous chapters, like chapter <a href="sec-dsgt.html#sec:dsgt">4</a>. In the following, we use <span class="math inline">\(g(x_{i,k},\xi_{i,k})=\nabla Q(x_{i,k},\xi_{i,k})\)</span>.</p>
<p>We focus on the class of diffusion strategies here. The exact diffusion and the traditional diffusion strategy(take adapt-then-conbine formulartion of diffusion as an example) can be seen in the following table,</p>
<table>
<colgroup>
<col width="37%" />
<col width="62%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Name</th>
<th align="center">Scheme</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">adapt-then-combine of diffusion</td>
<td align="center"><span class="math display">\[X_{k+\frac{1}{2}}=X_{k}-\alpha\nabla G(X_k,\boldsymbol\xi_k)(\text{adaptation})\\X_{k+1}=WX_{k+\frac{1}{2}}(\text{combination})\]</span></td>
</tr>
<tr class="even">
<td align="center">exact diffusion(stochastic version)</td>
<td align="center"><span class="math display">\[X_{k+\frac{1}{3}}=X_{k}-\alpha G(X_k,\boldsymbol\xi_k)(\text{adaptation})\\X_{k+\frac{2}{3}}=X_{k+\frac{1}{3}}+X_k-X_{k-1+\frac{1}{3}}(\text{correction})\\X_{k+1}=WX_{k+\frac{2}{3}}(\text{combination})\]</span></td>
</tr>
</tbody>
</table>
The performance of bias-correction methods under stochastic and adaptive settings remain unclear. <span class="citation">Yuan et al. (<a href="#ref-yuan2019performance">2019</a>)</span> show that the correction-step in exact diffusion leads to better standy-state performance under stochastic scenarios. The exact diffusion assume each <span class="math inline">\(f_i\)</span> to be <strong>differentiable and <span class="math inline">\(\mu-\)</span>strongly convex</strong>. Under sufficiently small step sizes <span class="math inline">\(\alpha\)</span>, the exact diffusion converges exponentially at the rate <span class="math inline">\(1-\mathcal{O}(\alpha\mu)\)</span> to a neiborhood of <span class="math inline">\(x^*\)</span>, which can be characterized as
<span class="math display" id="eq:ned">\[\begin{equation}
\underset{k\to \infty}{\lim\sup}\frac{1}{n}E\left[\Vert X_k-\mathbf{1}x^*\Vert^2\right]_{ed}
=\mathcal{O}\left(\frac{\alpha\bar\sigma^2}{n\mu}+\frac{\delta^2}{n\mu^2}\cdot\frac{\alpha^2\bar\sigma^2}{1-\lambda}\right)
\tag{11.2}
\end{equation}\]</span>
In comparison, the traditional diffusion strategy converges at the similar rate to the following neighborhood of <span class="math inline">\(x^*\)</span>,
<span class="math display" id="eq:nd">\[\begin{equation}
\underset{k\to \infty}{\lim\sup}\frac{1}{n}E\left[\Vert X_k-\mathbf{1}x^*\Vert^2\right]_{d}
=\mathcal{O}\left(\frac{\alpha\bar\sigma^2}{n\mu}+\frac{\delta^2}{n\mu^2}\cdot(\frac{\alpha^2\bar\sigma^2}{1-\lambda}+
\frac{\alpha^2b^2}{(1-\lambda)^2})\right)
\tag{11.3}
\end{equation}\]</span>
<p>where <span class="math inline">\(\bar\sigma^2\)</span> measures the size of gradient noise,<span class="math inline">\(\delta\)</span> bounds the hessian matrix of <span class="math inline">\(f_i\)</span>, <span class="math inline">\(b^2=\sum\limits_{i=1}^n\Vert \nabla f_i(x^*)\Vert^2\)</span>, and <span class="math inline">\(\lambda=\max\{|\lambda_2(W)|,|\lambda_n(W)|\}\in(0,1)\)</span> stands for the spectral gap.</p>

<div class="exercise">
<p><span id="exr:edfi" class="exercise"><strong>Assumption11.1  </strong></span>Each <span class="math inline">\(f_i\)</span> is <span class="math inline">\(\mu-\)</span>strongly convex and twice differentiable, and <span class="math display">\[
\mu I_p\leq\nabla^2 f_i(x)\leq\delta I_p
\]</span></p>
</div>

<p>We have a more smooth function <span class="math inline">\(f_i\)</span> here compared to assuming <span class="math inline">\(\nabla f_i\)</span> is <span class="math inline">\(L-\)</span>Lipschitz continuous.</p>

<div class="exercise">
<span id="exr:edw" class="exercise"><strong>Assumption11.2  </strong></span>The network is undirected and stronly connected, and the mixing matrix <span class="math inline">\(W\)</span> is symmetric and doubly-stochastic
</div>

<p>For the noisy gradient, we assume differently compared to assumption <a href="sec-dsgt.html#exr:estg">4.1</a>, where awe assume the variance of the noisy gradient can be bound be a uniform <span class="math inline">\(\sigma^2\)</span>.</p>

<div class="exercise">
<span id="exr:edg" class="exercise"><strong>Assumption11.3  </strong></span>Each agent <span class="math inline">\(i\)</span> and the iteration step <span class="math inline">\(k\)</span>, we have,
<span class="math display">\[\begin{align}
E\left[\nabla g(x_{i,k},\xi_{i,k})|\mathcal{F}_{k-1}\right]&amp;=\nabla f_i(x_{i,k})\\
E\left[\Vert \nabla g(x_{i,k},\xi_{i,k})-\nabla f_i(x_{i,k})\Vert^2|\mathcal{F}_{k-1}\right]&amp;\leq \beta_i\Vert x_{i,k-1}-x^*\Vert^2 + \sigma^2_i
\end{align}\]</span>
<span class="math inline">\(\bar\sigma^2=\frac{1}{n}\sum_{i=1}^n\sigma_i^2\)</span>
</div>

<p>Compare the two characterization <a href="sec-ediff.html#eq:ned">(11.2)</a> and <a href="sec-ediff.html#eq:nd">(11.3)</a>, we see that the exact diffusion outperforms the traditional diffusion strategy in the following,</p>
<ul>
<li><p>The exact diffusion removes the bias <span class="math inline">\(\frac{\alpha^2b^2}{(1-\lambda^2)}\)</span></p></li>
<li><p>When we can obtain the exact gradient(deterministic setting), i.e. <span class="math inline">\(\sigma_i^2=0,i=1,2,...,n\)</span>, the exact diffusion converge exactly to <span class="math inline">\(x^*\)</span>.</p></li>
<li><p>When the bias <span class="math inline">\(\frac{\alpha^2b^2}{(1-\lambda^2)}\)</span> is significant, i.e. <span class="math inline">\(b^2\)</span> is large or the network is badly-connected(i.e. <span class="math inline">\(\lambda\)</span> is close to 1), and the step size is not sufficiently small(<span class="math inline">\(\alpha\leq d_3(1-\lambda)^{(2+y)},y&gt;0\)</span>), the exact diffusion outperforms the traditional one.</p></li>
</ul>
<div id="decreasing-stepsize" class="section level2">
<h2><span class="header-section-number">11.1</span> Decreasing stepsize</h2>
Here we can derive the information of Hessian matrices of <span class="math inline">\(f_i,i=1,2,...,n\)</span>, then let <span class="math inline">\(\tilde x_{i,k}=x^*-x_{i,k}\)</span> we have,
<span class="math display">\[\begin{equation}
\nabla f_i(x_{i,k})-\nabla f_i(x^*)=-H_{i,k}\tilde x_{i,k}
\end{equation}\]</span>
where
<span class="math display">\[\begin{equation}
H_{i,k}=\int_{0}^1\nabla^2 f_i(x^*-r\tilde x_{i,k})dr\in\mathbb{R}^{p\times p}
\end{equation}\]</span>
then we can denote
<span class="math display">\[\begin{equation}
\nabla F(X_k)-\nabla F(\mathbf{1}x^*):= 
-\left(
\begin{array}{c}
(H_{1,k}\tilde x_{1,k})^T\\
\vdots\\
(H_{n,k}\tilde x_{n,k})^T
\end{array}\right)
\in\mathbb{R}^{n,p}
\end{equation}\]</span>
However, this notation cannot have the form of <span class="math inline">\(\mathcal{H}_k\tilde X_k\)</span>, which helps us to separate the Hessian matrices and the iterated values. To ensure such an advantage, we stack the local vairables of different agents as a long vector, i.e.,
<span class="math display">\[\begin{equation}
\mathcal{X}_k = 
\left(
\begin{array}{c}
x_{1,k}\\
x_{2,k}\\
\vdots\\
x_{n,k}
\end{array}\right):=\mathrm{col}\{x_{1,k},...,x_{n,k}\}\in\mathbb{R}^{np}
\end{equation}\]</span>
Similarly, we denote <span class="math inline">\(\mathcal{G}_k:=\mathrm{col}\{g_1(x_{1,k},\xi_{1,k}),...,g_n(x_{n,k},\xi_{n,k})\}\)</span>. Then the exact diffusion with decreasing stepsize <span class="math inline">\(\alpha_k\)</span> can be rewritten globally as the following,
<span class="math display" id="eq:extraak">\[\begin{align}
\mathcal{X}_{k+1} &amp;= \overline{\mathcal{W}} (2 \mathcal{X}_k-\mathcal{X}_{k-1}-\alpha_k\mathcal{G}_k+\alpha_{k-1}\mathcal{G}_{k-1})\\
\mathcal{X}_1&amp;=\overline{\mathcal{W}} (\mathcal{X}_0-\alpha_0\mathcal{G}_0),\mathcal{X}_0=\mathbf{0}
\tag{11.4}
\end{align}\]</span>
where <span class="math inline">\(\overline{\mathcal{W}}=(\mathcal{W}+I_{np})/2\)</span>, <span class="math inline">\(\mathcal{W}=W\otimes I_{p}\in\mathbb{R}^{np\times np}\)</span>. Essentially, exact diffusion is a primal-dual method. Since <span class="math inline">\(W\)</span> is symmetric and doubly-stochastic, then so does <span class="math inline">\(\overline W\)</span>, thus <span class="math inline">\(I_n-\overline W\)</span> is positive semi-definite(p.s.d.) and symmetric, we have spectral decomposition <span class="math inline">\(I_n-\overline W = U\Sigma U^T:=V^2\)</span>. Let <span class="math inline">\(\mathcal{V}=V\otimes I_p\)</span>, then <span class="math inline">\(\mathcal{V}^2=I_{np}-\overline{\mathcal{W}}\)</span>, we have the primal-dual version of exact diffusion <a href="sec-ediff.html#eq:pdextra">(11.5)</a>.
<span class="math display" id="eq:pdextra">\[\begin{equation}
\begin{cases}
\mathcal{X}_{k+1}=\overline{\mathcal{W}}\left(\mathcal{X}_k-\alpha_k\mathcal{G}_k\right)-\mathcal{V}\mathcal{Y}_k\\
\mathcal{Y}_{k+1}=\mathcal{Y}_k+\mathcal{V}\mathcal{X}_{k+1}
\end{cases}
\tag{11.5}
\end{equation}\]</span>
This is because,
<span class="math display">\[\begin{align}
\mathcal{X}_{k+1}-\mathcal{X}_{k}&amp;=\overline{\mathcal{W}}\left(\mathcal{X}_{k}-\mathcal{X}_{k-1}-\alpha_k\mathcal{G}_k
+\alpha_{k-1}\mathcal{G}_{k-1}\right)-\mathcal{V}\mathcal{Y}_k+\mathcal{V}\mathcal{Y}_{k-1}\\
&amp;=\overline{\mathcal{W}}\left(\mathcal{X}_{k}-\mathcal{X}_{k-1}-\alpha_k\mathcal{G}_k
+\alpha_{k-1}\mathcal{G}_{k-1}\right)-\mathcal{V}(\mathcal{Y}_{k-1}+\mathcal{V}\mathcal{X}_k)+\mathcal{V}\mathcal{Y}_{k-1}\\
&amp;=\overline{\mathcal{W}} (2 \mathcal{X}_k-\mathcal{X}_{k-1}-\alpha_k\mathcal{G}_k+\alpha_{k-1}\mathcal{G}_{k-1})-\mathcal{X}_{k}
\end{align}\]</span>
<p>We can also have <span class="math inline">\(\mathcal{Y}_k=\mathcal{Y}_0+\mathcal{V}\sum\limits_{i=1}^k \mathcal{X}_i\)</span>.</p>
<p>Let <span class="math inline">\(\nabla\mathcal{J}(\mathcal{X}_k):=\mathrm{col}\{\nabla f_1(x_{1,k}),...,\nabla f_n(x_{n,k})\}\)</span>, then lemma <a href="sec-ediff.html#lem:optcond">11.1</a> states the optimality condition for problem <a href="sec-ediff.html#eq:eloss">(11.1)</a>.</p>

<div class="lemma">
<span id="lem:optcond" class="lemma"><strong>Lemma 11.1  </strong></span>Under assumption <a href="sec-ediff.html#exr:edfi">11.1</a>, if some vectors <span class="math inline">\((\mathcal{X}^*,\mathcal{Y}^*)\)</span> exist that satisfy:
<span class="math display">\[\begin{align}
\alpha_k\overline{\mathcal{W}}\nabla J(\mathcal{X}^*)+\mathcal{V}\mathcal{Y}^*&amp;=\mathbf{0}\\
\mathcal{V}\mathcal{X}^*=\mathbf{0}
\end{align}\]</span>
where<span class="math inline">\(\mathcal{X}^*=\mathrm{\{x_1^*,...,x_n^*\}}\)</span>, then it holds that <span class="math display">\[
x_1^*=x_2^*=\cdots=x_n^*=x^*
\]</span> where the <span class="math inline">\(x^*\)</span> is the unique solution to problem <a href="sec-ediff.html#eq:eloss">(11.1)</a>.
</div>

<p>We follow the proof in <span class="citation">(Yuan et al. <a href="#ref-yuan2018exact">2018</a>)</span>, where the authors prove the situation with different stepsizes for each agents and the objective function is a weighted average of <span class="math inline">\(f_i\)</span>.</p>
First, according to <span class="math inline">\(\mathrm{null}(\mathcal{V})=\mathrm{span}\{\mathbf{1}_n\otimes I_p\}\)</span>,
<span class="math display">\[\begin{align}
0=\mathcal{V}\mathcal{X}^*\Longleftrightarrow \mathcal{X}^*\in\mathrm{null}(\mathcal{V})
\end{align}\]</span>
<p>so, <span class="math inline">\(x_1^*=x_2^*=\cdots=x_n^*\)</span>.</p>
Additionally, let <span class="math inline">\(\mathcal{I}=\mathbf{1}_n\otimes I_p\)</span> and multiply <span class="math inline">\(\mathcal{I}^T\)</span> on both sides of <span class="math inline">\(\alpha_k\overline{\mathcal{W}}\nabla J(\mathcal{X}^*)+\mathcal{V}\mathcal{Y}^*=\mathbf{0}\)</span>. Since <span class="math inline">\(\mathcal{V}\)</span> is symmetric and <span class="math inline">\(\mathcal{V}\mathcal{I}=\mathbf{0}\)</span>, we have
<span class="math display">\[\begin{align}
\alpha_k\mathcal{I}^T\overline{\mathcal{W}}\nabla J(\mathcal{X}^*)&amp;=\alpha_k(\mathbf{1}_n\otimes I_p)^T(\overline{W}\otimes I_p)\nabla J(\mathcal{X}^*)\\
&amp;=\alpha_k(\mathbf{1}^T\otimes I_p)(\overline{W}\otimes I_p)\nabla J(\mathcal{X}^*)\\
&amp;=\alpha_k(\mathbf{1}_n\overline{W})\otimes (I_pI_p)\nabla J(\mathcal{X}^*)\\
&amp;=\alpha_k(\mathbf{1}_n^T\otimes I_p)\nabla J(\mathcal{X}^*)\\
&amp;=\alpha_k\sum_{i=1}^n\nabla f_i(x_i^*)\\
&amp;=0
\end{align}\]</span>
where we use the column stochastic property of <span class="math inline">\(\overline W\)</span>. <span class="math inline">\(\sum\limits_{i=1}^n\nabla f_i(x_i^*)=0\)</span> implies that <span class="math inline">\(x_1^*=\cdots=x_n^*\)</span> must coincide with the minimizer <span class="math inline">\(x^*\)</span> of problem <a href="sec-ediff.html#eq:eloss">(11.1)</a>. We can also see from the above proof that changing the stepsize from fixed <span class="math inline">\(\alpha\)</span> to decreasing <span class="math inline">\(\alpha_k\)</span> does not violate too much(<strong>will condtions in lemma <a href="sec-ediff.html#lem:optcond">11.1</a> requires too much?</strong>). From lemma <a href="sec-ediff.html#lem:optcond">11.1</a>, we have the following error dynamics,
<span class="math display" id="eq:edy">\[\begin{equation}
\begin{cases}
\tilde{\mathcal{X}}_{k+1}=\overline{\mathcal{W}}\left[\tilde{\mathcal{X}_{k}}+\alpha_k(\nabla \mathcal{J}(\mathcal{X}_k)-\nabla \mathcal{J}(\mathcal{X}^*))\right]-\mathcal{V}\tilde{\mathcal{Y}_k}+\alpha_k\overline{\mathcal{W}}\mathcal{S}_k(\mathcal{X}_k)\\
\tilde{\mathcal{Y}}_{k+1}=\tilde{\mathcal{Y}_{k}}+\mathcal{V}\tilde{\mathcal{X}}_{k+1}
\end{cases}
\tag{11.6}
\end{equation}\]</span>
<p>Where <span class="math inline">\(\mathcal{S}_k(\mathcal{X}_k)=\mathcal{G}_k-\nabla \mathcal{J}(\mathcal{X}_k)\)</span>, <span class="math inline">\(\tilde{\mathcal{X}}_k:=\mathcal{X}^*-\mathcal{X}_k\)</span>, and <span class="math inline">\(\tilde{\mathcal{Y}}_k:=\mathcal{Y}^*-\mathcal{Y}_k\)</span>.</p>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> <br />
From <span class="math inline">\(\mathcal{V}\mathcal{X}^*=0\)</span>, we have <span class="math inline">\(0=\mathcal{V}^2\mathcal{X}^*=(I_{np}-\overline{\mathcal{W}})\mathcal{X}^*\)</span>, hence <span class="math inline">\(\mathcal{X}^*=\overline{\mathcal{W}}\mathcal{X}^*\)</span>, then
<span class="math display">\[\begin{align}
\tilde{\mathcal{X}}_{k+1}&amp;=\mathcal{X}^*-\mathcal{X}_{k+1}\\
&amp;=\overline{\mathcal{W}}\mathcal{X}^*-\overline{\mathcal{W}}\left(\mathcal{X}_k-\alpha_k\mathcal{G}_k\right)+\mathcal{V}\mathcal{Y}_k\\
&amp;=\overline{\mathcal{W}}\left[\tilde{\mathcal{X}}_{k}+\alpha_k(\nabla\mathcal{J}(\mathcal{X}_k)-\nabla \mathcal{J}(\mathcal{X}^*))\right]+\alpha_k\overline{\mathcal{W}}\mathcal{S}_k(\mathcal{X}_k)+\alpha_k\overline{\mathcal{W}}\nabla \mathcal{J}(\mathcal{X}^*)+\mathcal{V}\mathcal{Y}_k\\
&amp;=\overline{\mathcal{W}}\left[\tilde{\mathcal{X}}_{k}+\alpha_k(\nabla\mathcal{J}(\mathcal{X}_k)-\nabla \mathcal{J}(\mathcal{X}^*))\right]-\mathcal{V}\tilde{\mathcal{Y}_k}+\alpha_k\overline{\mathcal{W}}\mathcal{S}_k(\mathcal{X}_k)
\end{align}\]</span>
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> <br />
In the above proof, we use the condition <span class="math inline">\(\alpha_k\overline{\mathcal{W}}\nabla J(\mathcal{X}^*)+\mathcal{V}\mathcal{Y}^*=\mathbf{0}\)</span> in lemma <a href="sec-ediff.html#lem:optcond">11.1</a> so that we can have <span class="math inline">\(\alpha_k\overline{\mathcal{W}}\nabla \mathcal{J}(\mathcal{X}^*)=-mathcal{V}\mathcal{Y}^*\)</span>. On the other hand, if requiring the condtions in lemma <a href="sec-ediff.html#lem:optcond">11.1</a> are satisfied for each <span class="math inline">\(k\in\mathbb{N}\)</span> is too strong, we may instead require <span class="math inline">\(\overline{\mathcal{W}}\nabla J(\mathcal{X}^*)+\mathcal{V}\mathcal{Y}^*=\mathbf{0}\\ \mathcal{V}\mathcal{X}^*=\mathbf{0}\)</span>, which also leads to <span class="math inline">\(\sum\limits_{i=1}^n\nabla f_i(x_i^*)=0\)</span>. Then the error dynamics should be
<span class="math display">\[\begin{align}
\tilde{\mathcal{X}}_{k+1}&amp;=\overline{\mathcal{W}}\left[\tilde{\mathcal{X}}_{k}+\alpha_k(\nabla \mathcal{J}(\mathcal{X}_k)-\nabla \mathcal{J}(\mathcal{X}^*))\right]-\mathcal{V}\tilde{\mathcal{Y}_k}+\alpha_k\overline{\mathcal{W}}\mathcal{S}_k(\mathcal{X}_k)+(1-\alpha_k)\mathcal{V}\mathcal{Y}^*\\
&amp;=\overline{\mathcal{W}}\left[\tilde{\mathcal{X}}_{k}+\alpha_k(\nabla \mathcal{J}(\mathcal{X}_k)-\nabla \mathcal{J}(\mathcal{X}^*))\right]-\mathcal{V}\tilde{\mathcal{Y}_k}+\alpha_k\overline{\mathcal{W}}\mathcal{S}_k(\mathcal{X}_k)-(1-\alpha_k)\overline{W}\nabla \mathcal{J}(\mathcal{X}^*)
\end{align}\]</span>
</div>

Then we can see a nice property of using the above notation as we have mentioned at the beginning. Let
<span class="math display">\[\begin{equation}
\mathcal{H}_k=\mathrm{diag}\{H_{1,k},...,H_{n,k}\}\in\mathbb{R}^{np\times np}
\end{equation}\]</span>
Then <span class="math inline">\(\nabla \mathcal{J}(\mathcal{X}_k)-\nabla \mathcal{J}(\mathcal{X}^*)=-\mathcal{H}_k\tilde{\mathcal{X}}_k\)</span>, the error dynamics <a href="sec-ediff.html#eq:edy">(11.6)</a> become
<span class="math display" id="eq:edyH">\[\begin{equation}
\begin{cases}
\tilde{\mathcal{X}}_{k+1}=\overline{\mathcal{W}}\left(\tilde{\mathcal{X}_{k}}-\alpha_k\mathcal{H}_k\tilde{\mathcal{X}_k}\right)-\mathcal{V}\tilde{\mathcal{Y}_k}+\alpha_k\overline{\mathcal{W}}\mathcal{S}_k(\mathcal{X}_k)\\
\tilde{\mathcal{Y}}_{k+1}=\tilde{\mathcal{Y}_{k}}+\mathcal{V}\tilde{\mathcal{X}}_{k+1}
\end{cases}
\tag{11.7}
\end{equation}\]</span>
<p>We can further write the above <a href="sec-ediff.html#eq:edyH">(11.7)</a> as the matrix form,</p>
<span class="math display" id="eq:edyHm">\[\begin{align}
\left(
\begin{array}{c}
\tilde{\mathcal{X}_{k+1}}\\
\tilde{\mathcal{Y}_{k+1}}\\
\end{array}
\right)
&amp;=
\left(\begin{array}{cc}
\overline{\mathcal{W}}&amp;-\mathcal{V}\\
\mathcal{V}\overline{\mathcal{W}}&amp;\overline{\mathcal{W}}
\end{array}
\right)
\left[
\left(\begin{array}{cc}
I_{np}&amp;0\\
0&amp;I_{np}
\end{array}
\right)
-
\alpha_k\left(\begin{array}{cc}
\mathcal{H}_k&amp;0\\
0&amp;0
\end{array}
\right)
\right]
\left(
\begin{array}{c}
\tilde{\mathcal{X}_{k}}\\
\tilde{\mathcal{Y}_{k}}\\
\end{array}
\right)
+\alpha_k
\left(
\begin{array}{c}
\overline{\mathcal{W}}\\
\mathcal{V}\overline{\mathcal{W}}\\
\end{array}
\right)
\mathcal{S}_k(\mathcal{X}_k)
\\
&amp;:=\mathcal{B}(I_{2np}-\alpha_k\mathcal{T}_k)
\left(
\begin{array}{c}
\tilde{\mathcal{X}_{k}}\\
\tilde{\mathcal{Y}_{k}}\\
\end{array}
\right)
+\alpha_k \mathcal{B}_l
\mathcal{S}_k(\mathcal{X}_k)
\tag{11.8}
\end{align}\]</span>
Note that
<span class="math display">\[\begin{equation}\mathcal{B}_l=\mathcal{B}\left(
\begin{array}{c}
I_{np}\\
0
\end{array}
\right)
\end{equation}\]</span>
<p>and for <span class="math inline">\(\mathcal{B}\)</span> we have the following decomposition(see <span class="citation">(Yuan et al. <a href="#ref-yuan2018exact">2018</a>)</span>). In general, the following decomposition is a eigendecomposition.</p>

<div class="lemma">
<span id="lem:funddecom" class="lemma"><strong>Lemma 11.2  </strong></span>Under assumptions <a href="sec-ediff.html#exr:edfi">11.1</a> and <a href="sec-ediff.html#exr:edw">11.2</a>, the matrix <span class="math inline">\(\mathcal{B}\)</span> can be decomposed as
<span class="math display">\[\begin{align}
\mathcal{B}&amp;=
\left(\mathcal{R}_1,\mathcal{R}_2,c\mathcal{K}_R\right)\mathrm{diag}(I_p,I_p,\mathcal{D}_1)
\left(
\begin{array}{c}
 \mathcal{L}_1^T\\
 \mathcal{L}_2^T\\
 \frac{1}{c}\mathcal{K}_L
\end{array}
\right)\\
&amp;:=\mathcal{K}\mathcal{D}\mathcal{K}^{-1}
\end{align}\]</span>
<span class="math inline">\(\forall c&gt;0\)</span>, and
<span class="math display">\[\begin{align}
&amp;\mathcal{R}_{1}=\left(\begin{array}{l}
\mathcal{I} \\
0
\end{array}\right) \in \mathbb{R}^{2 n p \times p}, \quad \mathcal{R}_{2}=\left(\begin{array}{l}
0 \\
\mathcal{I}
\end{array}\right) \in \mathbb{R}^{2 n p \times p}\\
&amp;\mathcal{L}_{1}=\left(\begin{array}{c}
\frac{1}{n} \mathcal{I} \\
0
\end{array}\right) \in \mathbb{R}^{2 n p \times p},\quad \mathcal{L}_{2}=\left(\begin{array}{c}
0 \\
\frac{1}{n} \mathcal{I}
\end{array}\right) \in \mathbb{R}^{2 n p\times p}\\
&amp;\mathcal{K}_R=
\left(
\begin{array}{c}
 \mathcal{K}_{R,u}\\
 \mathcal{K}_{R,l}
\end{array}
\right)\in\mathbb{R}^{2np\times 2(n-1)p},\\

&amp;\mathcal{K}_L=(\mathcal{K}_{L,l},\mathcal{K}_{L,r})\in\mathbb{R}^{2(n-1)p\times 2np}
\end{align}\]</span>
where <span class="math inline">\(\mathcal{I}=\mathbf{1}_n\otimes I_p\in\mathbb{R}^{np\times p}\)</span>
</div>

From the error dynamics <a href="sec-ediff.html#eq:edyHm">(11.8)</a> and the decomposition of <span class="math inline">\(\mathcal{B}\)</span>, multiply <span class="math inline">\(\mathcal{K}^{-1}\)</span> on the both sides of <a href="sec-ediff.html#eq:edyHm">(11.8)</a>, we have,
<span class="math display">\[\begin{align}
\mathcal{K}^{-1}
\left(
\begin{array}{c}
\tilde{\mathcal{X}_{k+1}}\\
\tilde{\mathcal{Y}_{k+1}}\\
\end{array}
\right)&amp;:=
\left(
\begin{array}{c}
\bar z_{k+1}\\
\hat z_{k+1}\\
\check z_{k+1}
\end{array}
\right)\\
&amp;=
\mathcal{D}(\mathcal{K}^{-1}I_{2np}\mathcal{K}-\alpha_k\mathcal{K}^{-1}\mathcal{T}_k\mathcal{K})\mathcal{K}^{-1}
\left(
\begin{array}{c}
\tilde{\mathcal{X}_{k}}\\
\tilde{\mathcal{Y}_{k}}\\
\end{array}
\right)
+
\alpha_k\mathcal{K}^{-1}\mathcal{B}_l\mathcal{S}_k(\mathcal{X}_k)\\
&amp;=
\mathcal{D}(I_{2np}-\alpha_k\mathcal{K}^{-1}\mathcal{T}_k\mathcal{K})
\left(
\begin{array}{c}
\bar z_{k}\\
\hat z_{k}\\
\check z_{k}
\end{array}
\right) + 
\alpha_k\mathcal{K}^{-1}\mathcal{B}_l\mathcal{S}_k(\mathcal{X}_k)
\end{align}\]</span>
Moreover,
<span class="math display">\[\begin{align}
\mathcal{K}^{-1}\mathcal{T}_k\mathcal{K}&amp;=
\left(
\begin{array}{cc}
\frac{1}{n}\mathcal{I}^T&amp;0\\
0&amp;\frac{1}{n}\mathcal{I}^T\\
\frac{1}{c}\mathcal{K}_{L,l}&amp;\frac{1}{c}\mathcal{K}_{L,r}
\end{array}
\right)
\left(
\begin{array}{cc}
\mathcal{H}_k&amp;0\\
0&amp;0
\end{array}
\right)
\left(
\begin{array}{ccc}
\mathcal{I}&amp;0&amp;c\mathcal{K}_{R,u}\\
0&amp;\mathcal{I}&amp;c\mathcal{K}_{R,l}
\end{array}
\right)\\
&amp;=\left(\begin{array}{ccc}
\frac{1}{n} \mathcal{I}^T \mathcal{H}_{k}\mathcal{I} &amp; 0 &amp; \frac{c}{n} \mathcal{I}^T\mathcal{H}_k\mathcal{K}_{R,u} \\
0 &amp; 0 &amp; 0 \\
\frac{1}{c} \mathcal{K}_{L, l} \mathcal{H}_{k}\mathcal{I} &amp; 0 &amp; \mathcal{K}_{L, l} \mathcal{H}_{k} \mathcal{K}_{R,u}
\end{array}\right)\\
&amp;=\left(
\begin{array}{ccc}
\frac{1}{n}\sum_{i=1}^n H_{i,k} &amp; 0 &amp; \frac{c}{n}\mathcal{I}^T\mathcal{H}_k\mathcal{K}_{R,u}\\
0&amp;0&amp;0\\
\frac{1}{c}\mathcal{K}_L\mathcal{T}_k\mathcal{R}_1 &amp; 0 &amp; \mathcal{K}_L\mathcal{T}_k\mathcal{K}_R
\end{array}\right)

\end{align}\]</span>
This is because,
<span class="math display">\[\begin{align}
\mathcal{T}_k=\left(
\begin{array}{cc}
\mathcal{H}_k &amp; 0\\
0 &amp;0
\end{array}
\right)
\end{align}\]</span>
<p>We also have,</p>
<span class="math display">\[\begin{align}
\mathcal{K}^{-1}\mathcal{B}_l\mathcal{S}_k(\mathcal{X}_k)&amp;=
\mathcal{K}^{-1}\mathcal{B}\left(
\begin{array}{c}
I_{np}\\
0
\end{array}
\right)
\mathcal{S}_k(\mathcal{X}_k)\\
&amp;=\mathcal{D}\mathcal{K}^{-1}
\left(
\begin{array}{c}
I_{np}\\
0
\end{array}
\right)\mathcal{S}_k(\mathcal{X}_k)\\
&amp;=\left(
\begin{array}{c}
\frac{1}{n}\mathcal{I}^T\\
0\\
\frac{1}{c}D_1\mathcal{K}_{L,l}
\end{array}
\right)\mathcal{S}_k(\mathcal{X}_k)\\
&amp;=\left(\begin{array}{c}
\frac{1}{n}\mathcal{I}^T\\
0\\
\frac{1}{c}\mathcal{D}_1\mathcal{K}_L\mathcal{B}_l
\end{array}
\right)\mathcal{S}_k(\mathcal{X}_k)
\end{align}\]</span>
We can see that, for <span class="math inline">\(\hat z_k\)</span>, we have
<span class="math display">\[\begin{equation}
\hat z_{k+1} = \hat z_k
\end{equation}\]</span>
<p>Hence we only check <span class="math inline">\((\bar z_k,\check z_k)^T\)</span>, where <span class="math inline">\(\bar z_k\in\mathbb{R}^{p}\)</span>, and <span class="math inline">\(\check z_k\in\mathbb{R}^{2(n-1)p}\)</span> in the following.</p>

<div class="lemma">
<span id="lem:tedy" class="lemma"><strong>Lemma 11.3  (Transformed Error Dynamics)  </strong></span>Under assumption <a href="sec-ediff.html#exr:edfi">11.1</a> and <a href="sec-ediff.html#exr:edw">11.2</a>, the transformed error dynamics for exact diffusion is,
<span class="math display">\[\begin{align}
\left(\begin{array}{c}
       \bar z_{k+1}\\
       \check z_{k+1}
       \end{array}
\right)
&amp;=\left(\begin{array}{cc}
        I_p - \frac{\alpha_k}{n}\sum_{i=1}^n H_{i,k} &amp; -\frac{\alpha_k c}{n}\mathcal{I}^T\mathcal{H}_k\mathcal{K}_{R,u}\\
        -\frac{\alpha_k}{c}\mathcal{D}_1\mathcal{K}_L\mathcal{T}_k\mathcal{R}_1&amp;
          \mathcal{D}_1-\alpha_k\mathcal{D}_1\mathcal{K}_L\mathcal{T}_k\mathcal{K}_R
        \end{array}
\right)
\left(\begin{array}{c}
       \bar z_{k}\\
       \check z_{k}
       \end{array}
\right)\\
&amp;\quad + \alpha_k \left(
\begin{array}{c}
\frac{1}{n}\mathcal{I}^T\\
\frac{1}{c}D_1\mathcal{K}_{L}\mathcal{B}_l
\end{array}
\right)\mathcal{S}_k(\mathcal{X}_k)\\
&amp;:=A_k 
\left(\begin{array}{c}
       \bar z_{k}\\
       \check z_{k}
       \end{array}
\right)
+\alpha_k \left(
\begin{array}{c}
\frac{1}{n}\mathcal{I}^T\\
\frac{1}{c}D_1\mathcal{K}_{L}\mathcal{B}_l
\end{array}
\right)\mathcal{S}_k(\mathcal{X}_k)

\end{align}\]</span>
and,
<span class="math display">\[\begin{align}
\left(
\begin{array}{c}
\tilde{\mathcal{X}_{k}}\\
\tilde{\mathcal{Y}_{k}}\\
\end{array}
\right)
=(\mathcal{R}_1, c\mathcal{K}_R)
\left(\begin{array}{c}
       \bar z_k\\
       \check z_k
       \end{array}
\right)
\end{align}\]</span>
</div>

<p>Next we do a non-asymptotic analysis to check the convergence of <span class="math inline">\((\bar z_k,\check z_k)^T\)</span> under a decreasing stepsize <span class="math inline">\(\alpha_k:=\frac{\theta}{k+m}\)</span>, for some <span class="math inline">\(\theta\)</span>.</p>
<div id="preliminary-results" class="section level3">
<h3><span class="header-section-number">11.1.1</span> Preliminary results</h3>
We first check a less restrictive situation where assumption <a href="sec-ediff.html#exr:edg">11.3</a> becomes <a href="sec-dsgt.html#exr:estg">4.1</a>. Then,
<span class="math display" id="eq:betaeq0">\[\begin{align}
E\left[\mathcal{S}_k(\mathcal{X}_k)|\mathcal{F}_k\right]&amp;=0,\forall k\\
E\left[\Vert \mathcal{S}_{k}(\mathcal{X}_{k})\Vert^2|\mathcal{F}_k\right]\leq \frac{\sigma^2}{n}
\tag{11.9}
\end{align}\]</span>
<p>where <span class="math inline">\(\mathcal{F}_k\)</span> is the <span class="math inline">\(\sigma\)</span>-algebra generated by <span class="math inline">\(\{\xi_0,...,\xi_{k-1}\}\)</span>.</p>
Then from lemma <a href="sec-ediff.html#lem:tedy">11.3</a>,
<span class="math display" id="eq:barz">\[\begin{align}
E\left[\Vert \bar z_{k+1}\Vert^2|\mathcal{F}_k\right]&amp;=\Vert (I_p-\frac{\alpha_k}{n}\sum_{i=1}^n H_{i,k})\bar z_k-\frac{\alpha_kc}{n}\mathcal{I}^T\mathcal{H}_k\mathcal{K}_{R,u}\check z_k\Vert^2\\
&amp;\quad + \frac{\alpha_k^2}{n^2}\Vert \mathcal{I}^T\Vert^2 E\left[\Vert \mathcal{S}_k(\mathcal{X}_k)\Vert^2|\mathcal{F}_k\right]\\
\tag{11.10}
\end{align}\]</span>
For the first term, according to <span class="math inline">\(2\langle a,b\rangle\leq \gamma\Vert a\Vert^2+\gamma^{-1}\Vert b\Vert^2,\forall \gamma&gt;0\)</span>, <span class="math inline">\(\Vert \mathcal{I}\Vert^2=n\)</span>, <span class="math inline">\(\Vert \mathcal{H}_k\Vert^2\leq\delta^2\)</span>, and <span class="math inline">\(\Vert I_p-\frac{\alpha_k}{n}\sum\limits_{i=1}^n H_{i,k}\Vert^2\leq(1-\alpha_k\mu)^2\)</span> when <span class="math inline">\(\alpha_k\leq\frac{1}{\delta}\)</span>, we have,
<span class="math display" id="eq:barz1">\[\begin{align}
&amp;\quad\Vert (I_p-\frac{\alpha_k}{n}\sum_{i=1}^n H_{i,k})\bar z_k-\frac{\alpha_kc}{n}\mathcal{I}^T\mathcal{H}_k\mathcal{K}_{R,u}\check z_k\Vert^2\\
&amp;\leq (1+\gamma)\Vert (I_p-\frac{\alpha_k}{n}\sum_{i=1}^n H_{i,k})\bar z_k\Vert^2
+ (1+\frac{1}{\gamma})\Vert \frac{\alpha_kc}{n}\mathcal{I}^T\mathcal{H}_k\mathcal{K}_{R,u}\check z_k\Vert^2\\
&amp;\leq (1+\gamma)(1-\alpha_k\mu)^2\Vert \bar z_k\Vert^2+(1+\frac{1}{\gamma})\frac{\alpha_k^2c^2\delta^2}{n}\Vert \mathcal{K}_{R,u}\Vert^2\Vert \check z_k\Vert^2
\tag{11.11}
\end{align}\]</span>
We further let <span class="math inline">\(\alpha_k\leq \frac{1}{3\mu}\)</span> and choose <span class="math inline">\(\gamma = \frac{3}{8}\alpha_k\mu\)</span>, (see the proof of lemma 2.6 in <span class="citation">(Pu, Olshevsky, and Paschalidis <a href="#ref-pu2019sharp">2019</a><a href="#ref-pu2019sharp">a</a>)</span>), then <span class="math inline">\((1+\gamma)(1-\alpha_k\mu)^2\leq 1-\frac{3}{2}\alpha_k\mu\)</span> and <span class="math inline">\((1+\frac{1}{\gamma})\alpha_k\leq\frac{3}{\mu}\)</span>, then when <span class="math inline">\(\alpha_k\leq\min\{\frac{1}{\delta},\frac{1}{3\mu}\}\)</span> <a href="sec-ediff.html#eq:barz1">(11.11)</a> becomes
<span class="math display" id="eq:barz2">\[\begin{align}
&amp;\quad\Vert (I_p-\frac{\alpha_k}{n}\sum_{i=1}^n H_{i,k})\bar z_k-\frac{\alpha_kc}{n}\mathcal{I}^T\mathcal{H}_k\mathcal{K}_{R,u}\check z_k\Vert^2\\
&amp;\leq(1-\frac{3}{2}\alpha_k\mu)\Vert \bar z_k\Vert^2 + \frac{3\alpha_k c^2\Vert \mathcal{K}_{R,u}\Vert^2\delta^2}{n\mu}\Vert \check z_k\Vert^2
\tag{11.12}
\end{align}\]</span>
Substituting <a href="sec-ediff.html#eq:barz2">(11.12)</a> and <a href="sec-ediff.html#eq:betaeq0">(11.9)</a> into <a href="sec-ediff.html#eq:barz">(11.10)</a> and take full expectation on both sides, we have lemma <a href="sec-ediff.html#lem:barzk">11.4</a>. Notice that
<span class="math display">\[\begin{equation}
\Vert \mathcal{K}_{R,u}\Vert^2=
\Vert \left(I_{np},0\right)\mathcal{K}_{R}\Vert^2\leq \Vert \mathcal{K}_{R}\Vert^2
\end{equation}\]</span>

<div class="lemma">
<span id="lem:barzk" class="lemma"><strong>Lemma 11.4  </strong></span>Under assumptions <a href="sec-ediff.html#exr:edfi">11.1</a>, <a href="sec-ediff.html#exr:edw">11.2</a>, and <a href="sec-dsgt.html#exr:estg">4.1</a>, supposing <span class="math inline">\(\alpha_k\leq \min\{\frac{1}{\delta},\frac{1}{3\mu}\}\)</span>, then
<span class="math display">\[\begin{align}
E \left[\Vert \bar z_{k+1}\Vert^2\right]\leq (1-\frac{3}{2}\alpha_k\mu)\Vert E\left[\bar z_k\Vert^2\right] + \frac{3\alpha_k c^2\Vert \mathcal{K}_{R}\Vert^2\delta^2}{n\mu}\Vert E\left[\check z_k\Vert^2\right] + \frac{\alpha_k^2\sigma^2}{n}
\end{align}\]</span>
</div>

<p>Similarly, for <span class="math inline">\(E\left[\Vert \check z_{k+1}\Vert^2|\mathcal{F}_{k}\right]\)</span>, we have,</p>
<span class="math display" id="eq:checkz">\[\begin{align}

E\left[\Vert \check z_{k+1}\Vert^2|\mathcal{F}_{k}\right]&amp;=\Vert \mathcal{D}_1\check z_k-\frac{\alpha_k}{c}\mathcal{D}_1\mathcal{K}_L\mathcal{T}_k(\mathcal{R}_1\bar z_k+c\mathcal{K}_R\check z_k)\Vert^2\\
&amp;\quad + \frac{\alpha_k^2\Vert\mathcal{D}_1\mathcal{K}_L\mathcal{B}_l\Vert^2}{c^2}E\left[\Vert \mathcal{S}_k(\mathcal{X}_k)\Vert^2|\mathcal{F}_k\right]
\tag{11.13}
\end{align}\]</span>
Let <span class="math inline">\(\lambda :=\max \{|\lambda_2(W)|,|\lambda_n(W)|\}\)</span>, denote <span class="math inline">\(\lambda&#39; = (1+\lambda_2(W))/2\in(0,1)\)</span>, then from lemma 4 in <span class="citation">(Yuan et al. <a href="#ref-yuan2018exact">2018</a>)</span>, <span class="math inline">\(\Vert \mathcal{D}_1\Vert=\sqrt{\lambda&#39;}\)</span>. Additionally,
<span class="math display">\[\begin{align}
\Vert \mathcal{T}_k\Vert^2&amp;=
\Vert 
\left(
\begin{array}{cc}
\mathcal{H}_k&amp;0\\
0&amp;9
\end{array}
\right)
\Vert^2\leq\delta^2\\
\Vert \mathcal{R}_1\Vert^2 &amp;= 
\Vert\left(
\begin{array}{c}
\mathcal{I}\\
0
\end{array}
\right)\Vert^2=\Vert \mathcal{I}\Vert^2=n
\end{align}\]</span>
Moreover, from the decomposition of <span class="math inline">\(\mathcal{B}\)</span> and <span class="math inline">\(\mathcal{B}_l^T=\mathcal{B}^T(I_{np},0)\)</span>, we have,
<span class="math display">\[\begin{equation}
\vert \mathcal{B}_l\Vert^2\leq \Vert \mathcal{B}\Vert^2\leq 1
\end{equation}\]</span>
Then similar as <a href="sec-ediff.html#eq:barz1">(11.11)</a>, we have
<span class="math display" id="eq:checkz2">\[\begin{align}
&amp;\quad \Vert \mathcal{D}_1\check z_k-\frac{\alpha_k}{c}\mathcal{D}_1\mathcal{K}_L\mathcal{T}_k(\mathcal{R}_1\bar z_k+c\mathcal{K}_R\check z_k)\Vert^2\\
&amp;\leq (1+\gamma)\lambda&#39; \Vert \check z_k\Vert^2 + (1+\frac{1}{\gamma})\frac{\alpha_k^2\lambda&#39;\Vert \mathcal{K}_L\Vert^2\delta^2}{c}\Vert \mathcal{R}_1\bar z_k+c\mathcal{K}_R\check z_k\Vert^2\\
&amp;\leq (1+\gamma)\lambda&#39; \Vert \check z_k\Vert^2 + (1+\frac{1}{\gamma})\frac{\alpha_k^2\lambda&#39;\Vert \mathcal{K}_L\Vert^2\delta^2}{c}\left(2n\Vert \bar z_k\Vert^2+2c^2\Vert \mathcal{K}_R\Vert^2\Vert\check z_k\Vert^2\right)\\
&amp;\leq (1+\frac{1}{\gamma})\frac{2n\alpha_k^2\lambda&#39;\Vert \mathcal{K}_L\Vert^2\delta^2}{c}\Vert \bar z_k\Vert^2  \\
&amp;\quad + \left[(1+\gamma)\lambda&#39;+(1+\frac{1}{\gamma})({2c\alpha_k^2\lambda&#39;\Vert \mathcal{K}_L\Vert^2\Vert \mathcal{K}_R\Vert^2\delta^2})\right]\Vert \check z_k\Vert^2
\tag{11.14}
\end{align}\]</span>
<!--
Before choosing $\gamma$, we first check the original error dynamics $(\tilde{\mathcal{X}_k},\tilde{\mathcal{Y}_k})^T$ according to 
\begin{align}
\left(\begin{array}{c}
\tilde{\mathcal{X}_k}\\
\tilde{\mathcal{Y}_k}
\end{array}
\right)&=
\left(\begin{array}{cc}
\mathcal{R}_1&c\mathcal{K}_R
\end{array}
\right)\left(\begin{array}{c}
\bar z_k\\
\check z_k
\end{array}
\right)\\
&=
\left(\begin{array}{cc}
\mathcal{I}&c\mathcal{K}_{R,u}\\
0&c\mathcal{K}_{R,l}
\end{array}
\right)\left(\begin{array}{c}
\bar z_k\\
\check z_k
\end{array}
\right)
\end{align}

Hence,

\begin{align}
&\quad E\left[\Vert \tilde{\mathcal{X}_{k+1}}\Vert^2\right]=E\left[\Vert \mathcal{I}\bar z_{k+1}+c\mathcal{K}_{R,u}\check z_{k+1}\Vert^2\right]\\
&\leq 2nE\left[\Vert \bar z_{k+1}\Vert^2\right]+2c^2\Vert \mathcal{K}_R\Vert^2E\left[\Vert \check z_{k+1}\Vert^2\right]\\
&\leq \left[2n(1+\gamma_1)(1-\alpha_k\mu)^2+2c^2\Vert \mathcal{K}_R\Vert^2(1+\frac{1}{\gamma_2})\frac{2n\alpha_k^2\lambda'\Vert\mathcal{K}_L\Vert^2\delta^2}{c}\right]E\left[\Vert \bar z_k\Vert^2\right]\\
&\quad+ \left[2n(1+\frac{1}{\gamma_1})\frac{\alpha_k^2c^2\delta^2\Vert \mathcal{K}_R\Vert^2}{n} + 2c^2\Vert \mathcal{K}_R\Vert^2\lambda'\left((1+\gamma_2)+2(1+\frac{1}{\gamma_2})c\alpha_k^2\Vert\mathcal{K}_L\Vert^2\Vert \mathcal{K}_R\Vert^2\delta^2\right)\right]\\
&\cdot E\left[\Vert \check z_k\Vert^2\right]

(\#eq:xbarzcheckz)
\end{align}
-->
Then we have,
<span class="math display" id="eq:checkzkgamma">\[\begin{align}
&amp;\quad E\left[\Vert \check z_{k+1}\Vert^2\right]\\
&amp;\leq (1+\frac{1}{\gamma})\frac{2n\alpha_k^2\lambda&#39;\Vert \mathcal{K}_L\Vert^2\delta^2}{c}E\left[\Vert \bar z_k\Vert^2\right]  
+ \alpha_k^2\frac{\lambda&#39;\Vert\mathcal{K}_L\Vert^2\sigma^2}{nc^2}\\
&amp;\quad + \left[(1+\gamma)\lambda&#39;+(1+\frac{1}{\gamma})({2c\alpha_k^2\lambda&#39;\Vert \mathcal{K}_L\Vert^2\Vert \mathcal{K}_R\Vert^2\delta^2})\right]E\left[\Vert \check z_k\Vert^2\right]
\tag{11.15}
\end{align}\]</span>
Denote,
<span class="math display">\[\begin{equation}
B(k)=E\left[\Vert \bar z_k\Vert^2\right], H(k) = E\left[\Vert\check z_k\Vert^2\right]
\end{equation}\]</span>
We want to derive uniform bounds <span class="math inline">\(\hat B\)</span> and <span class="math inline">\(\hat H\)</span> for <span class="math inline">\(B(k)\)</span> and <span class="math inline">\(H(k)\)</span> respectively, such that
<span class="math display">\[\begin{equation}
B(k)\leq \frac{\hat B}{(m+k)^a},\quad H(k)\leq \frac{\hat H}{(m+k)^b}
\end{equation}\]</span>
where <span class="math inline">\(a,b\)</span> are some positive integers. Noting that the relationship between <span class="math inline">\((\tilde{\mathcal{X}_k},\tilde{\mathcal{Y}_k})^T\)</span> and <span class="math inline">\((\bar z_k,\check z_k)^T\)</span> is
<span class="math display">\[\begin{align}
\left(\begin{array}{c}
\bar z_k\\
\check z_k
\end{array}
\right)&amp;=\left(\begin{array}{c}
\mathcal{L}_1^T\\
\frac{1}{c}\mathcal{K}_L\end{array}\right)
\left(\begin{array}{c}
\tilde{\mathcal{X}_k}\\
\tilde{\mathcal{Y}_k}
\end{array}
\right)\\
&amp;=\left(\begin{array}{cc}
\frac{1}{n}\mathcal{I}^T&amp;0\\
\frac{1}{c}\mathcal{K}_{L,l}&amp;\frac{1}{c}\mathcal{K}_{L,r}
\end{array}
\right)
\left(\begin{array}{c}
\tilde{\mathcal{X}_k}\\
\tilde{\mathcal{Y}_k}
\end{array}
\right)
\end{align}\]</span>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-pu2019sharp">
<p>Pu, Shi, Alex Olshevsky, and Ioannis Ch. Paschalidis. 2019a. “A Sharp Estimate on the Transient Time of Distributed Stochastic Gradient Descent.”</p>
</div>
<div id="ref-yuan2019performance">
<p>Yuan, Kun, Sulaiman A Alghunaim, Bicheng Ying, and Ali H Sayed. 2019. “On the Performance of Exact Diffusion over Adaptive Networks.” In <em>2019 Ieee 58th Conference on Decision and Control (Cdc)</em>, 4898–4903. IEEE.</p>
</div>
<div id="ref-yuan2018exact">
<p>Yuan, Kun, Bicheng Ying, Xiaochuan Zhao, and Ali H Sayed. 2018. “Exact Diffusion for Distributed Optimization and Learning—Part Ii: Convergence Analysis.” <em>IEEE Transactions on Signal Processing</em> 67 (3). IEEE: 724–39.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="comparison.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-PUDA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["DistributedOpt.pdf", "DistributedOpt.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
