[
["index.html", "Notes about Distributed Optimization Chapter 1 Introduction", " Notes about Distributed Optimization Kun Huang 2020-02-29 Chapter 1 Introduction We summarize some distributed gradient based algorithms solving the problem (1.1), namely Push-Pull Gradient Method1 (Pu et al. 2018), and Distributed Stochastic Gradient Tracking Methods(DSGT) (Pu and Nedić 2018). \\[\\begin{equation} \\underset{x\\in\\mathbb{R}^p}{\\min} f(x):=\\frac{1}{n}\\sum_{i=1}^n f_i(x) \\tag{1.1} \\end{equation}\\] Where \\(f_i:\\mathbb{R}^p\\to \\mathbb{R}\\) is known by agent \\(i\\) only, and all the agents communicate and exchange information over a network. In general, these two methods use a decision variable \\(x\\in \\mathbb{R}^p\\) and an auxiliary variable \\(y\\in\\mathbb{R}^p\\) and have a form of (1.2) for the \\((k+1)\\)th iteration. \\[\\begin{align} X_{k+1} &amp;= S_1(X_k-\\boldsymbol\\alpha Y_k)\\\\ Y_{k+1} &amp;= S_2Y_k + T(X_{k+1}) - T(X_k) \\tag{1.2} \\end{align}\\] Where \\(S_1\\) and \\(S_2\\) are the matrices inducing the graphs, \\(T(\\cdot)\\) is the estimate of gradient, and \\(\\boldsymbol\\alpha = \\text{diag}(\\alpha_1,...,\\alpha_n)\\in\\mathbb{R}^{n\\times n}\\), \\(\\alpha_i\\) is the step size initialized for agent \\(i\\). \\[\\begin{align*} X_k &amp;= \\left(x_{1,k},x_{2,k},...,x_{n,k}\\right)^T\\in\\mathbb{R}^{n\\times p}\\\\ Y_k &amp;= \\left(y_{1,k},y_{2,k},...,y_{n,k}\\right)^T\\in\\mathbb{R}^{n\\times p}\\\\ \\end{align*}\\] \\(x_{i,k}\\in\\mathbb{R}^p\\) denotes the decision variable of agent \\(i\\) of the \\(k\\)th iteration, \\(y_{i,k}\\) represents the auxiliary variable of agent \\(i\\) of the \\(k\\)th iteration. Under assumption 1.1 , there exists an unique solution to (1.1) \\(x^*\\in\\mathbb{R}^{1\\times p}\\). To prove the convergence of those methods, the idea is to bound three quntities, namely \\(\\bar x_{k+1}-x^*\\), \\(X_{k+1}-\\mathbf{1}\\bar x_{k+1}\\), and \\(Y_{k+1}-\\mathbf{1}\\bar y_{k+1}\\) by the linear combination of their previous values under corresponding measurements. This will introduce a matrix \\(A\\). In order to make it converge, we need to make \\(\\rho(A)&lt;1\\), i.e. the spectral radius of \\(A\\) to be less than \\(1\\)(similar idea with contraction mapping), which will derive constraints to the stepsize \\(\\alpha\\). By doing so, the authors prove the convergence of those three methods and derive their convergence rate. Assumption1.1 Each \\(f_i\\) is \\(\\mu\\)-strongly convex and its gradient is \\(L-\\)Lipschitz continuous, i.e. \\(\\forall x,x&#39;\\in\\mathbb{R}^{p}\\) \\[ \\left\\langle\\nabla f_{i}(x)-\\nabla f_{i}\\left(x^{\\prime}\\right), x-x^{\\prime}\\right\\rangle \\geq \\mu\\left\\|x-x^{\\prime}\\right\\|_{2}^{2} \\] \\[ \\left\\|\\nabla f_{i}(x)-\\nabla f_{i}\\left(x^{\\prime}\\right)\\right\\|_{2} \\leq L\\left\\|x-x^{\\prime}\\right\\|_{2} \\] We also use the following notation, \\[\\begin{equation*} F(X) = \\sum_{i=1}^n f_i(x_i), X\\in\\mathbb{R}^{n\\times p} \\end{equation*}\\] \\[\\begin{equation*} \\nabla F(X) = \\left(\\nabla f_1(x_1), ..., \\nabla f_n(x_n)\\right)^T\\in\\mathbb{R}^{n\\times p} \\end{equation*}\\] If not stated otherwise, we use the capital letter to denote a matrix, and use column vector. References "],
["orgnization-of-the-notes.html", "Chapter 2 Orgnization of the Notes", " Chapter 2 Orgnization of the Notes Currently contents include The Push-Pull method (Pu et al. 2018) The distributed stochastic gradient tracking method(DSGT)(Pu and Nedić 2018) References "],
["the-push-pull-method.html", "Chapter 3 The Push-Pull Method 3.1 Introduction 3.2 Analysis of Convergence", " Chapter 3 The Push-Pull Method 3.1 Introduction Suppose we have two nonnegative matrices \\(R,C^T\\in\\mathbb{R}^{n\\times n}\\) and two induced digraph \\(\\mathcal{G}_R, \\mathcal{G}_{C^T}\\). Suppose each agent \\(i\\) can actively and reliably push information out to its neighbor \\(l\\in\\mathcal{N}^{out}_{C,i}\\subset\\mathcal{N}\\) and pull information from its neighbor \\(j\\in\\mathcal{N}^{in}_{R,i}\\subset\\mathcal{N}\\). Matrix \\(R=(r_{ij})\\in\\mathbb{R}^{n\\times n}\\) denotes the pulling weights that agent \\(i\\) pulls information from agent \\(j\\). Thus the row sum of \\(R\\) should be \\(1\\), i.e. \\(R\\boldsymbol 1 = \\boldsymbol 1\\) and \\(r_{ij}\\geq 0\\). That is to say, matrix \\(R\\) is row-stochastic. Similarly, \\(C = (c_{ij})\\in\\mathbb{R}^{n\\times n}\\) denotes the pushing weights that agent \\(i\\) pushes information to agent \\(j\\). In other words, it denotes the pulling weights that agent \\(j\\) pulls information to agent \\(i\\). Hence \\(C^T\\boldsymbol 1=\\boldsymbol 1\\), i.e. \\(\\boldsymbol 1^T C=\\boldsymbol 1^T, c_{ij}\\geq 0\\). Moreover, for agent \\(i\\), it will have no problem getting information from itself, hence \\(r_{ii}&gt;0, c_{ii}&gt;0\\). As a result, we have assumption 3.1. Assumption3.1 The matrix \\(R\\in\\mathbb{R}^{n\\times n}\\) is nonnegative row-stochastic and \\(C\\in\\mathbb{R}^{n\\times n}\\) is nonnegative column-stochastic, i.e., \\(R\\mathbf{1}=\\mathbf{1}\\) and \\(\\mathbf{1}^TC=\\mathbf{1}\\). \\(r_{ii}&gt;0,c_{ii}&gt;0,\\forall i\\). Assumption3.2 The graphs \\(\\mathcal{G}_R\\) and \\(\\mathcal{G}_{C^T}\\) each contain at least one spaning tree. Moreover, there exists at least one node that is a root of spanning trees for both \\(\\mathcal{G}_{R}\\) and \\(\\mathcal{G}_{C^T}\\), i.e. \\(\\mathcal{R}_{R} \\cap \\mathcal{R}_{\\mathbf{C}^{\\top}} \\neq \\emptyset\\), where \\(\\mathcal{R}_R\\) is the set of roots of all possible spanning trees in the graph \\(\\mathcal{G}_R\\). Assumption 3.2 is to say that at least one agent is connected to all other agents in this system(thus they should be both “pulled” and “pushed”), which is weaker to assume that the system is connected. Hence these agents are significant and we must assume at least one of them contribute to the update, i.e. assumption 3.3. Assumption3.3 \\(\\exists i\\in \\mathcal{R}_{R} \\cap \\mathcal{R}_{\\mathbf{C}^{\\top}}\\) whose step size \\(\\alpha_i&gt;0\\) Assumption 3.2 and 3.1 would lead to the following lemma 3.1, Lemma 3.1 Under assumptions 3.1 and 3.2, the matrix \\(R\\) has a unique nonnegative left eigenvector \\(u^T\\)(w.r.t. eigenvalue 1) with \\(u^T\\boldsymbol1 = n\\), and the matrix \\(C\\) has a unique nonnegative right eigenvector \\(v\\) (w.r.t. eigenvalue 1) with \\(\\boldsymbol1^T v = n\\), i.e., \\[ u^T R = 1\\cdot u^T \\] \\[ Cv = 1\\cdot v \\] Moreover, \\(u^T\\) (resp., \\(v\\)) is nonzero only on the entries associated with agents \\(i\\in\\mathcal{R}_R\\)(resp., \\(j\\in\\mathcal{R}_{C^T}\\)), and \\(u^Tv&gt;0\\). The idea of Push-Pull Gradient Methods is that, at each iteration step \\(k\\), agent \\(i\\) updates its local copy of decision variable \\(x_{i,k+1}\\in\\mathbb R^p\\) according to the information it pulls from its nearby agents based on the corresponding pulling weights. Then it will also update the information stored in an auxiliary variable \\(y_{i, k+1}\\in\\mathbb{R}^p\\) Algorithm3.1 (Push-Pull Method) Each agent \\(i\\) chooses its local step size \\(\\alpha_i\\geq0\\) and initilized with an arbitary \\(x_{i,0}\\in\\mathbb{R}^p, y_{i,0}=\\nabla f_i(x_{i,0})\\). For k = 0, 1, …, For \\(i\\in\\mathcal{N}\\), \\(x_{i, k+1} = \\sum\\limits_{j=1}^nr_{ij}(x_{j, k}-\\alpha_j y_{j, k})\\) (Pull) \\(y_{i, k+1} = \\sum\\limits_{j=1}^nc_{ij}y_{j,k}+\\nabla f_i(x_{i,k+1})-\\nabla f_i(x_{i,k})\\)(Push) Or in matrix form using \\(R=(r_{ij})\\in\\mathbb{R}^{n\\times n}, C=(c_{ij})\\in\\mathbb{R}^{n\\times n}, X_k\\in\\mathbb{R}^{n\\times p}, Y_k\\in\\mathbb{R}^{n\\times p}, \\boldsymbol\\alpha = \\text{diag}(\\alpha_1,...,\\alpha_n)\\). \\[\\begin{align} X_{k+1} &amp;= R(X_{k}-\\boldsymbol\\alpha Y_k),\\\\ Y_{k+1} &amp;= CY_k+\\nabla F(X_{k+1})-\\nabla F(X_k) \\tag{3.1} \\end{align}\\] Remark. In the initialization of algorithm 3.1, \\(y_{i,0}=\\nabla f_i(x_{i,0}),i=1,2,...,n\\),i.e., \\(Y_0=\\nabla F(X_0)\\) is important since \\[\\begin{align*} \\mathbf{1}^T Y_{1} &amp;= \\mathbf{1}^T(CY_0+\\nabla F(X_1)-\\nabla F(X_0))\\\\ &amp;= \\mathbf{1}^T\\nabla F(X_1)+Y_0-\\nabla F(X_0)\\\\ &amp;= \\mathbf{1}^T\\nabla F(X_1) \\end{align*}\\] Then by induction, we can have \\[\\begin{equation} \\frac{1}{n} \\mathbf{1}^{\\top} Y_{k}=\\frac{1}{n} \\mathbf{1}^{\\top} \\nabla F\\left(X_{k}\\right), \\quad \\forall k \\tag{3.2} \\end{equation}\\] 3.2 Analysis of Convergence we first bound \\((\\Vert\\bar x_{k+1}-x^*\\Vert_2, \\Vert X_{k+1}-\\boldsymbol1\\bar x_{k+1}\\Vert_R,\\Vert Y_{k+1}-v\\bar y_{k+1}\\Vert_C)^T\\) by linear system of inequalities in terms of their previous values. Then based on lemma 3.2, we derive how should we choose the step size \\(\\alpha_i\\) so that \\(\\rho(A)&lt;1\\). Lemma 3.2 Given a nonnegative, irreducible matrix \\(M=(m_{ij})\\in \\mathbb{R}^{n\\times n}\\) with \\(m_{ii}&lt;\\lambda, i=1,2,3\\) for some \\(\\lambda&gt;0\\). \\(\\rho(M)&lt;\\lambda\\Leftrightarrow \\text{det}(\\lambda I-M)&gt;0\\) In this chapter, we define the matrix norm of \\(X\\in\\mathbb R^{n\\times p }\\) as, Definition 3.1 Given an arbitary vector norm \\(\\Vert\\cdot\\Vert\\) on \\(\\mathbb{R}^n\\), \\(\\forall X\\in\\mathbb{R}^{n\\times p }, \\Vert X\\Vert:= \\left\\|\\left[\\left\\|\\mathbf{x}^{(1)}\\right\\|,\\left\\|\\mathbf{x}^{(2)}\\right\\|, \\ldots,\\left\\|\\mathbf{x}^{(p)}\\right\\|\\right]\\right\\|_{2}\\), where \\(x^{(j)},j=1,2,...,p\\) denote the \\(j\\)th column of \\(X\\), \\(\\Vert \\cdot\\Vert_2\\) denotes \\(2-\\)norm. For example, when \\(\\Vert\\cdot\\Vert\\) is the \\(2-\\)norm, then the matrix norm under definition 3.1 is the Frobenius norm. While in the chapter of distributed stochastic gradient tracking method, we use Frobenius norm. 3.2.1 Relationship between two iteration steps We first give definition of \\(\\bar x_k\\) and \\(\\bar y_k\\). \\[\\begin{equation} \\bar x_k := \\frac{1}{n}u^TX_k\\in\\mathbb{R}^{1\\times p},\\quad \\bar y_k:= \\frac{1}{n}\\boldsymbol 1 \\nabla F(X_k)\\in\\mathbb{R}^{1\\times p} \\tag{3.3} \\end{equation}\\] The authors do not define \\(\\bar x_k\\) as \\(\\frac{1}{n}\\boldsymbol 1^TX_k\\) is because the pulling information is subject to the graph \\(\\mathcal{G}_R\\), which is not connected. For the pull step, \\[\\begin{equation} \\bar x_{k+1} = \\frac{1}{n}u^TX_{k+1}\\stackrel{\\text{pull step}}{=}\\frac{1}{n}u^TR(X_k-\\boldsymbol\\alpha Y_k)=\\bar x_k-\\frac{1}{n}u^T\\boldsymbol\\alpha Y_k \\tag{3.4} \\end{equation}\\] Hence, \\[\\begin{align} X_{k+1}-\\boldsymbol1\\bar x_{k+1}&amp;= R(X_k-\\boldsymbol\\alpha Y_k)-\\boldsymbol1(\\bar x_k-\\frac{1}{n}u^T\\boldsymbol\\alpha Y_k)\\\\ &amp;=(R-\\frac{\\boldsymbol 1 u^T}{n})(X_k-\\boldsymbol 1\\bar x_k)- (R-\\frac{\\boldsymbol 1 u^T}{n})\\boldsymbol\\alpha Y_k+\\frac{\\boldsymbol 1 u^T}{n}(X_k-\\boldsymbol 1\\bar x_k)\\\\ &amp;=(R-\\frac{\\boldsymbol 1 u^T}{n})(X_k-\\boldsymbol 1\\bar x_k)- (R-\\frac{\\boldsymbol 1 u^T}{n})\\boldsymbol\\alpha Y_k \\tag{3.5} \\end{align}\\] This is because \\(\\frac{\\boldsymbol 1 u^T}{n}(X_k-\\boldsymbol 1\\bar x_k)=\\bar x_k-\\frac{u^T\\boldsymbol1}{n}\\bar x_k=0\\) according to lemma 3.1. To see what does this difference mean, we rewrite \\(X_{k+1}-\\boldsymbol1\\bar x_{k+1}\\) as \\[\\begin{equation} (x_1-\\frac{1}{n}\\sum_{i=1}^n u_i x_i,...,x_n-\\frac{1}{n}\\sum_{i=1}^n u_i x_i)^T\\in\\mathbb{R}^{n\\times p} \\end{equation}\\] where \\(u=(u_1,...,u_n)^T\\in\\mathbb{R}^n,x_i\\in\\mathbb{R}^p\\). It denotes the difference between each agent \\(i\\)’s decision variable and the overall weighted mean. Remark. The interpretation of \\(R-\\frac{\\boldsymbol 1 u^T}{n}\\)? For the push step, \\[\\begin{align} Y_{k+1}-v\\bar y_{k+1}&amp;\\stackrel{\\text{push step}}=CY_k+\\nabla F(X_{k+1})-\\nabla F(X_k)-v[\\frac{1}{n}\\boldsymbol1(CY_k+\\nabla F(X_{k+1})-\\nabla F(X_k))]\\\\ &amp;=CY_k-v\\bar y_k+(I-\\frac{v\\boldsymbol1^T}{n})(\\nabla F(X_{k+1})-\\nabla F(X_k))\\\\ &amp;=(C-\\frac{v\\boldsymbol1^T}{n})(Y_k-v\\bar y_k)+(I-\\frac{v\\boldsymbol1^T}{n})(\\nabla F(X_{k+1})-\\nabla F(X_k))+Cv\\bar y_k+\\frac{v\\boldsymbol1^T}{n}Y_k-\\frac{v\\boldsymbol1^Tv}{n}\\bar y_k-v\\bar y_k\\\\ &amp;=(C-\\frac{v\\boldsymbol1^T}{n})(Y_k-v\\bar y_k)+(I-\\frac{v\\boldsymbol1^T}{n})(\\nabla F(X_{k+1})-\\nabla F(X_k)) \\tag{3.6} \\end{align}\\] where \\[ \\frac{1}{n}v\\boldsymbol1^TCY_k=v\\bar y_k \\] \\[ Cv\\bar y_k+\\frac{v\\boldsymbol1^T}{n}Y_k-\\frac{v\\boldsymbol1^Tv}{n}\\bar y_k-v\\bar y_k=v\\bar y_k+v\\bar y_k-v\\bar y_k-v\\bar y_k=0 \\] This is because the column-stochastic of \\(C\\) and lemma 3.1. Similarly, we can rewrite \\(Y_{k+1}-v\\bar y_{k+1}\\) as \\[\\begin{equation} (y_1-\\frac{1}{n}v_1\\sum_{i=1}^ny_i,...,y_n-\\frac{1}{n}v_n\\sum_{i=1}^ny_i)^T \\end{equation}\\] Where \\(v=(v_1,...,v_n)^T\\in\\mathbb{R}^n,y_i\\in\\mathbb R^p\\). Additionally, recall our goal is to bound those three distance, from (3.4), we separate \\(Y_k\\) as \\(Y_k-v\\bar y_k\\) and \\(v\\bar y_k\\), then \\[\\begin{align} \\bar x_{k+1} &amp;=\\bar x_k -\\underbrace{\\frac{1}{n}u^T\\boldsymbol\\alpha v}_{\\alpha&#39;}\\bar y_k-\\frac{1}{n}u^T\\boldsymbol \\alpha(Y_k-v\\bar y_k)\\\\ &amp;=\\bar x_k-\\alpha&#39;(\\bar y_k-\\underbrace{\\frac{1}{n}\\boldsymbol 1^T\\nabla F(\\boldsymbol 1\\bar x_k)}_{g_k})-\\frac{1}{n}\\alpha&#39;\\boldsymbol 1^T\\nabla F(\\boldsymbol 1\\bar x_k)-\\frac{1}{n}u^T\\boldsymbol\\alpha(Y_k-v\\bar y_k)\\\\ &amp;=\\bar x_k-\\alpha&#39;(\\bar y_k-g_k)-\\alpha&#39;g_k-\\frac{1}{n}u^T\\boldsymbol\\alpha(Y_k-v\\bar y_k) \\tag{3.7} \\end{align}\\] The auther introduce \\(g_k=\\frac{1}{n}\\boldsymbol 1^T\\nabla F(\\boldsymbol 1\\bar x_k)\\) is because \\(\\bar y_k =\\frac{1}{n}\\boldsymbol 1^T \\nabla F(X_k)\\), i.e, equation (3.2). It is the gradient of the obejective function at \\(\\bar x_k\\). 3.2.2 Linear system of inequalities we then bound \\((\\Vert\\bar x_{k+1}-x^*\\Vert_2, \\Vert X_{k+1}-\\boldsymbol1\\bar x_{k+1}\\Vert_R,\\Vert Y_{k+1}-v\\bar y_{k+1}\\Vert_C)^T\\) by linear system of inequalities. Lemma 3.3 Let the assumption 1.1, 3.1, and 3.2 hold and \\(\\alpha&#39;\\leq \\frac{2}{\\mu+L}\\), then \\(\\exists A\\in \\mathbb R^{3\\times 3}, s.t.\\) \\[\\begin{equation} \\left( \\begin{array}{c} \\Vert\\bar x_{k+1}-x^*\\Vert_2,\\\\ \\Vert X_{k+1}-\\boldsymbol1\\bar x_{k+1}\\Vert_R,\\\\ \\Vert Y_{k+1}-v\\bar y_{k+1}\\Vert_C)^T \\end{array} \\right) \\leq A \\left( \\begin{array}{c} \\Vert\\bar x_{k}-x^*\\Vert_2,\\\\ \\Vert X_{k}-\\boldsymbol1\\bar x_{k}\\Vert_R,\\\\ \\Vert Y_{k}-v\\bar y_{k}\\Vert_C \\end{array} \\right) \\tag{3.8} \\end{equation}\\] In general, assumption 3.1 and 3.2 is used to derive the relationships in (3.5), (3.6), and (3.7). Assumption 1.1 is needed for lemma 3.4. Next we derive the elements of \\(A\\), which can be seen in (3.13). We add supported lemmas during derivation. First, for \\(\\Vert\\bar x_{k+1}-x^*\\Vert_2\\), substitute \\(\\bar x_{k+1}\\) using (3.7), we have \\[\\begin{align} \\Vert\\bar x_{k+1}-x^*\\Vert_2&amp;\\leq \\left\\|\\bar{x}_{k}-\\alpha^{\\prime} g_{k}-x^{*}\\right\\|_{2}+\\alpha^{\\prime}\\left\\|\\bar{y}_{k}-g_{k}\\right\\|_{2}+\\frac{1}{n}\\left\\|u^{\\top} \\boldsymbol{\\alpha}\\left(Y_{k}-v \\bar{y}_{k}\\right)\\right\\|_{2} \\tag{3.9} \\end{align}\\] On the right hand side, \\(\\Vert \\bar{x}_{k}-\\alpha^{\\prime} g_{k}-x^{*}\\Vert_2\\) is the distance between the optimal and iterated value, \\(\\vert \\bar{y}_{k}-g_{k}\\Vert_2\\) is the distance between average gradient and gradient of iterated value. Lemma 3.4 connects them with \\(\\Vert X_{k}-\\boldsymbol1\\bar x_{k}\\Vert_2\\) and \\(\\Vert\\bar x_{k+1}-x^*\\Vert_2\\) and add conditions on \\(f_i\\) and \\(\\alpha&#39;\\). Lemma 3.4 Let assumption 1.1 hold, \\[ \\left\\|\\bar{y}_{k}-g_{k}\\right\\|_{2} \\leq \\frac{L}{\\sqrt{n}}\\left\\|X_{k}-\\mathbf{1} \\bar{x}_{k}\\right\\|_{2}, \\quad\\left\\|g_{k}\\right\\|_{2} \\leq L\\left\\|\\bar{x}_{k}-x^{*}\\right\\|_{2} \\] In addition, when \\(\\alpha&#39;\\leq \\frac{2}{\\mu+L}\\), we have \\[ \\left\\|\\bar{x}_{k}-\\alpha^{\\prime} g_{k}-x^{*}\\right\\|_{2} \\leq\\left(1-\\alpha^{\\prime} \\mu\\right)\\left\\|\\bar{x}_{k}-x^{*}\\right\\|_{2}, \\quad \\forall k \\] However, notice that our final goal involves norm \\(\\Vert\\cdot\\Vert_R\\) and \\(\\Vert\\cdot\\Vert_C\\). We need to transform them, which is ensured from the equivalence of norms. To make the notation more easily, the author gives lemma 3.5. Lemma 3.5 \\(\\exists \\delta_{\\mathrm{C}, \\mathrm{R}}, \\delta_{\\mathrm{C}, 2}, \\delta_{\\mathrm{R}, \\mathrm{C}}, \\delta_{\\mathrm{R}, 2}&gt;0,s.t. \\forall X\\in\\mathbb{R}^{n\\times p}\\), we have \\(\\Vert X\\Vert_{\\mathrm{C}} \\leq \\delta_{\\mathrm{C}, \\mathrm{R}}\\Vert X\\Vert_{\\mathrm{R}},\\Vert X\\Vert_{\\mathrm{C}} \\leq \\delta_{\\mathrm{C}, 2}\\Vert X\\Vert_{2},\\Vert X\\Vert_{\\mathrm{R}} \\leq \\delta_{\\mathrm{R}, \\mathrm{C}}\\Vert X\\Vert_{\\mathrm{C}}\\), and \\(\\|X\\|_{\\mathrm{R}} \\leq\\delta_{\\mathrm{R}, 2}\\Vert X\\Vert_{2}\\). In addition, with a proper rescaling of the norms \\(\\Vert\\cdot\\Vert_R\\) and \\(\\Vert\\cdot\\Vert_C\\), we have \\(\\Vert X\\Vert_{2} \\leq\\Vert X\\Vert_{\\mathrm{R}} \\text { and }\\Vert X\\Vert_{2} \\leq\\Vert X\\Vert_{\\mathrm{C}}\\) On the other hand, \\(\\boldsymbol\\alpha=\\text{diag}(\\alpha_1,...,\\alpha_n)\\in \\mathbb{R}^{n\\times n}\\), then \\(\\Vert \\boldsymbol\\alpha\\Vert_2=\\sigma_{\\max}(\\boldsymbol\\alpha)=\\underset{i}{\\max}\\alpha_i:=\\hat\\alpha\\),since \\(\\alpha_i\\in\\mathbb{R}^+,i=1,2,...,n\\). \\(\\sigma(A)\\) denotes the singular value of \\(A\\). Finally, (3.9) can be written as \\[\\begin{equation} \\Vert\\bar x_{k+1}-x^*\\Vert_2\\leq \\left(1-\\alpha^{\\prime} \\mu\\right)\\left\\|\\bar{x}_{k}-x^{*}\\right\\|_{2}+\\frac{\\alpha^{\\prime} L}{\\sqrt{n}}\\left\\|X_{k}-\\mathbf{1} \\bar{x}_{k}\\right\\|_{\\mathrm{R}}+\\frac{\\hat{\\alpha}\\|u\\|_{2}}{n}\\left\\|Y_{k}-v \\bar{y}_{k}\\right\\|_{\\mathrm{C}} \\tag{3.10} \\end{equation}\\] Where the first and second parts come from lemma 3.4, which adds constraints on \\(f_i\\) and \\(\\alpha&#39;\\). The second part also uses lemma 3.5 in transforming different norms, as well as the last part. Additionally, the last part uses lemma 3.6 when separating \\(u,\\boldsymbol\\alpha\\) out of norm, which can be seen as a further result of consistency of norms. Lemma 3.6 Given an arbitrary norm \\(\\Vert\\cdot\\Vert\\), \\(\\forall W\\in\\mathbb{R}^{n\\times n}\\) and \\(X\\in\\mathbb{R}^{n\\times p}\\), we have \\(\\Vert WX\\Vert\\leq\\Vert W\\vert\\Vert X\\Vert\\). \\(\\forall w\\in\\mathbb{R}^{n\\times 1},x\\in\\mathbb{R}^{1\\times p},\\Vert wx\\Vert = \\Vert w\\Vert \\Vert x\\Vert_2\\) For \\(\\Vert X_{k+1}-\\boldsymbol1\\bar x_{k+1}\\Vert_R\\), from (3.5), we have \\[\\begin{align} \\Vert X_{k+1}-\\boldsymbol1\\bar x_{k+1}\\Vert_R&amp;\\leq \\underbrace{\\Vert R-\\frac{\\mathbf{1} u^{T}}{n}\\Vert_R}_{\\sigma_R}\\cdot\\Vert X_{k}-\\mathbf{1} \\bar{x}_{k}\\Vert_R+\\Vert R-\\frac{\\mathbf{1} u^{T}}{n}\\Vert_R\\cdot \\Vert\\boldsymbol{\\alpha}\\Vert_R\\cdot\\Vert Y_{k}-v\\bar y_k+v\\bar y_k)\\Vert_R\\\\ &amp;\\leq \\sigma_R\\Vert X_{k}-\\mathbf{1} \\bar{x}_{k}\\Vert_R + \\sigma_R\\Vert \\boldsymbol\\alpha\\Vert_2(\\delta_{R,C}\\Vert Y_{k}-v\\bar y_k\\Vert_C + \\Vert v\\Vert_R\\cdot \\Vert \\bar y_k\\Vert_2)\\\\ &amp;\\leq \\sigma_R\\Vert X_{k}-\\mathbf{1} \\bar{x}_{k}\\Vert_R + \\sigma_R\\Vert \\alpha&#39;\\Vert_2[\\delta_{R,C}\\Vert Y_{k}-v\\bar y_k\\Vert_C + \\Vert v\\Vert_R (\\frac{L}{\\sqrt{n}}\\left\\|X_{k}-\\mathbf{1} \\bar{x}_{k}\\right\\|_{2}+L\\left\\|\\bar{x}_{k}-x^{*}\\right\\|_{2})]\\\\ &amp;\\leq \\sigma_R\\left(1+\\hat{\\alpha}\\|v\\|_{\\mathrm{R}} \\frac{L}{\\sqrt{n}}\\right)\\left\\|X_{k}-\\mathbf{1} \\bar{x}_{k}\\right\\|_{\\mathrm{R}} + \\hat{\\alpha} \\sigma_{\\mathrm{R}} \\delta_{\\mathrm{R}, \\mathrm{C}}\\left\\|Y_{k}-v \\bar{y}_{k}\\right\\|_{\\mathrm{C}}+ \\hat{\\alpha} \\sigma_{\\mathrm{R}}\\|v\\|_{\\mathrm{R}} L\\left\\|\\bar{x}_{k}-x^{*}\\right\\|_{2} \\tag{3.11} \\end{align}\\] Where the second inquality is derived from lemma 3.7 in transforming \\(\\Vert\\boldsymbol\\alpha\\Vert_R=\\Vert\\boldsymbol\\alpha\\Vert_2=\\hat\\alpha\\) since \\(\\boldsymbol\\alpha\\) is diagonal and lemma 3.5 in transforming \\(\\Vert\\cdot\\Vert_R\\) into \\(\\Vert\\cdot\\Vert_C\\). Next we use lemma 3.6 and 3.4 to transform \\(\\Vert \\bar y_k\\Vert_R\\) into the two parts. Finally, we choose a proper rescaling of the norm \\(\\Vert\\cdot\\Vert_R\\) to derive \\(\\Vert X_{k}-\\mathbf{1} \\bar{x}_{k}\\Vert_2\\leq\\Vert X_{k}-\\mathbf{1} \\bar{x}_{k}\\Vert_R\\). Lemma 3.7 There exist matrix norms \\(\\Vert\\cdot\\Vert_R\\) and \\(\\Vert\\cdot\\Vert_C\\) such that \\(\\sigma_R:=\\Vert R-\\frac{\\mathbf1u^T}{n}\\Vert_R&lt;1,\\sigma_{\\mathrm{C}}:=\\left\\|\\mathbf{C}-\\frac{v \\mathbf{1}^{\\mathrm{T}}}{n}\\right\\|_{\\mathrm{C}}&lt;1\\), and \\(\\sigma_R\\) and \\(\\sigma_C\\) are arbitrarily close to \\(\\rho_R\\) and \\(\\rho_C\\), respectively. In addition, given any diagnal matrix \\(W\\in\\mathbb{R}^{n\\times n}\\), we have \\(\\|W\\|_{\\mathrm{R}}=\\|W\\|_{\\mathrm{C}}=\\|W\\|_{2}\\). For \\(\\Vert Y_{k+1}-v\\bar y_{k+1}\\Vert_C\\), denote \\(\\sigma_{\\mathrm{C}}:=\\left\\|\\mathbf{C}-\\frac{v \\mathbf{1}^{\\mathrm{T}}}{n}\\right\\|_{\\mathrm{C}}\\) and \\(c_0 :=\\Vert I-\\frac{v\\boldsymbol1^T}{n}\\Vert_C\\), from (3.6), we have \\[\\begin{align} \\Vert Y_{k+1}-v\\bar y_{k+1}\\Vert_C&amp;\\leq \\sigma_C\\Vert Y_k-v\\bar y_k\\Vert_C+c_0\\Vert\\nabla F(X_{k+1})-\\nabla F(X_k)\\Vert_C\\\\ &amp;\\leq \\sigma_C \\Vert Y_k-v\\bar y_k\\Vert_C + c_0L\\delta_{C,2}\\Vert X_{k+1} - X_k\\Vert_2 \\end{align}\\] For \\(\\Vert X_{k+1}-X_k\\Vert_2\\), we have \\[\\begin{align} \\Vert X_{k+1}-X_k\\Vert_2 &amp;=\\Vert R(X_k-\\boldsymbol\\alpha Y_k)-X_k\\Vert_2\\\\ &amp;=\\Vert (R-I)(X_k-\\mathbf 1\\bar x_k)+R\\boldsymbol\\alpha (Y_k-v\\bar y_k+v\\bar y_k)\\Vert_2\\\\ &amp;\\leq \\Vert R- I\\Vert_2\\cdot \\Vert X_k-\\mathbf 1\\bar x_k\\Vert_R + \\Vert R\\Vert_2\\hat\\alpha(\\Vert Y_k-v\\bar y_k+v\\bar y_k\\Vert_2)\\\\ \\end{align}\\] The inequality is based on lemma 3.5 by choosing a proper rescaling of \\(\\Vert \\cdot\\Vert_R\\). Then by lemma 3.4 and unite like terms, we have \\[\\begin{align} \\Vert Y_{k+1}-v\\bar y_{k+1}\\Vert_C&amp;\\leq \\left(\\sigma_{\\mathrm{C}}+\\hat{\\alpha} c_{0} \\delta_{\\mathrm{C}, 2}\\|R\\|_{2} L\\right)\\left\\|Y_{k}-v \\bar{y}_{k}\\right\\|_{\\mathrm{C}} +\\\\ &amp;c_{0} \\delta_{\\mathrm{C}, 2} L\\left(\\|R-I\\|_{2}+\\hat{\\alpha}\\|R\\|_{2}\\|v\\|_{2} \\frac{L}{\\sqrt{n}}\\right)\\left\\|X_{k}-\\mathbf{1} \\bar{x}_{k}\\right\\|_{\\mathrm{R}} + \\hat{\\alpha} c_{0} \\delta_{\\mathrm{C}, 2}\\|R\\|_{2}\\|v\\|_{2} L^{2}\\left\\|\\bar{x}_{k}-x^{*}\\right\\|_{2} \\tag{3.12} \\end{align}\\] In short, \\(A\\) can be written as \\[\\begin{equation} A_{pp} = \\left( \\begin{array}{ccc} 1-\\alpha&#39;\\mu &amp; \\frac{\\alpha&#39;L}{\\sqrt{n}} &amp; \\frac{\\hat\\alpha\\Vert \\mu\\Vert_2}{n}\\\\ \\hat{\\alpha} \\sigma_{\\mathrm{R}}\\|v\\|_{\\mathrm{R}} L &amp; \\sigma_{\\mathrm{R}}\\left(1+\\hat{\\alpha}\\|v\\|_{\\mathrm{R}} \\frac{L}{\\sqrt{n}}\\right) &amp; \\hat{\\alpha} \\sigma_{\\mathrm{R}} \\delta_{\\mathrm{R}, \\mathrm{C}}\\\\ \\hat{\\alpha} c_{0} \\delta_{\\mathrm{C}, 2}\\|R\\|_{2}\\|v\\|_{2} L^{2} &amp; c_{0} \\delta_{\\mathrm{C}, 2} L\\left(\\|R-\\mathbf{I}\\|_{2}+\\hat{\\alpha}\\|R\\|_{2}\\|v\\|_{2} \\frac{L}{\\sqrt{n}}\\right) &amp; \\sigma_{\\mathrm{C}}+\\hat{\\alpha} c_{0} \\delta_{\\mathrm{C}, 2}\\|R\\|_{2} L \\end{array} \\right) \\tag{3.13} \\end{equation}\\] 3.2.3 Spectral radius of A Lemma 3.2 lead us to give conditions on \\(A\\) so that \\(\\rho(A_{pp})&lt;1\\). Hence we need to make \\(a_{ii}&lt;1,i=1,2,3\\) and \\(\\det(I-A_{pp})&lt;1\\). From 3.7, \\(\\sigma_R&lt;1\\), so it is sufficient to have \\(a_{22}\\leq\\frac{1+\\sigma_R}{2}\\) so that \\(a_{22}&lt;1\\). Similar for \\(a_{33}&lt;1\\), we let \\(a_{33}\\leq\\frac{1+\\sigma_C}{2}\\). This may explain why the authors let \\(1-a_{22}\\geq \\frac{1-\\sigma_R}{2}\\) and \\(1-a_{33}\\geq\\frac{1-\\sigma_C}{2}\\). For \\(a_{11}&lt;1\\), i.e. \\(\\alpha&#39;\\mu&gt;0\\), we have \\[\\begin{equation} \\alpha&#39; = \\frac{1}{n}u^T\\boldsymbol{\\alpha}v =\\frac{1}{n}\\sum_{i=1}^n\\alpha_iu_iv_i&gt;0 \\end{equation}\\] Which can be seen from lemma 3.1 that \\(u,v\\in\\mathbb{R}^{n}\\) are nonnegative vectors and \\(\\alpha_i\\geq0\\). Remark. Lemma 3.2 also requires nonnegative matrix, so we also need \\(a_{11}=1-\\alpha&#39;\\mu\\geq 0\\), which is satisfied according to lemma 3.4 by requring \\(\\alpha&#39;\\leq \\frac{2}{\\mu+L}\\). Next we deive the sufficient conditions on \\(\\hat\\alpha:=\\underset{i}{\\max}\\alpha_i\\) so that \\(\\det{I-A_{pp}}&gt;0\\). Additionally, since \\(\\alpha&#39;=\\frac{1}{n}\\sum\\limits_{i=1}^nu_iv_i\\alpha_i=\\frac{1}{n}\\sum\\limits_{i\\in\\mathcal{R}_R\\cap\\mathcal{R}_C}u_iv_i\\alpha_i\\) and \\(\\hat\\alpha=\\underset{i}{\\max}\\alpha_i\\), then \\(\\exists M,s.t.\\alpha&#39;=M\\hat\\alpha\\). \\(M\\) is determined once we know the graphs \\(\\mathcal{G}_R\\) and \\(\\mathcal{G}_C\\) and choose the step sizes for each agent. Then \\(\\det(I-A)\\) becomes a function of \\(\\hat\\alpha\\), thus we can derive the requirement for the step sizes \\(\\alpha_i\\) by letting \\(\\det(I-A)=f(\\hat\\alpha)&gt;0\\). \\[\\begin{align} \\det(I-A) &amp;= \\left(1-a_{11}\\right)\\left(1-a_{22}\\right)\\left(1-a_{33}\\right)-a_{12} a_{23} a_{31}-a_{13} a_{21} a_{32}-a_{12} a_{23} a_{31}-a_{13} a_{21} a_{32}\\\\ &amp;-\\left(1-a_{22}\\right) a_{13} a_{31}-\\left(1-a_{11}\\right) a_{23} a_{32}-\\left(1-a_{33}\\right) a_{12} a_{21} &amp;\\geq \\alpha&#39;\\mu \\frac{(1-\\sigma_R)(1-\\sigma_C)}{4}\\\\ &amp;-a_{12} a_{23} a_{31}-a_{13} a_{21} a_{32}-a_{12} a_{23} a_{31}-a_{13} a_{21} a_{32}\\\\ &amp;-a_{13} a_{31}-a_{23} a_{32}- a_{12} a_{21}\\\\ :=\\hat\\alpha(c_3-c_2\\hat\\alpha-c_1\\hat\\alpha^2)&gt;0 \\tag{3.14} \\end{align}\\] Where the inequality holds for \\(1&gt;(1-a_{22})\\geq\\frac{1-\\sigma_R}{2}\\),and \\(1&gt;(1-a_{33})\\geq\\frac{1-\\sigma_C}{2}\\), which gives, \\[\\begin{equation} \\hat{\\alpha} \\leq \\min \\left\\{\\frac{\\left(1-\\sigma_{\\mathrm{R}}\\right) \\sqrt{n}}{2 \\sigma_{\\mathrm{R}}\\|v\\|_{\\mathrm{R}} L}, \\frac{\\left(1-\\sigma_{\\mathrm{C}}\\right)}{2 c_{0} \\delta_{\\mathrm{C}, 2}\\|R\\|_{2} L}\\right\\} \\end{equation}\\] Since \\(\\hat\\alpha&gt;0\\), let (3.14) be positive is equivalent to have \\(c_3-c_2\\hat\\alpha-c_1\\hat\\alpha^2&gt;0\\). Hence, \\[\\begin{equation} \\hat\\alpha&lt; \\frac{\\sqrt{c_2^2+4c_1c_2}-c_2}{2c_1} =\\frac{2 c_{3}}{c_{2}+\\sqrt{c_{2}^{2}+4 c_{1} c_{3}}} \\end{equation}\\] So when \\[\\begin{equation} \\hat\\alpha\\leq \\min \\left\\{\\frac{2 c_{3}}{c_{2}+\\sqrt{c_{2}^{2}+4 c_{1} c_{3}}}, \\frac{\\left(1-\\sigma_{\\mathrm{C}}\\right)}{2 \\sigma_{\\mathrm{C}} \\delta_{\\mathrm{C}, 2}\\|R\\|_{2} L}\\right, \\frac{2 c_{3}}{c_{2}+\\sqrt{c_{2}^{2}+4 c_{1} c_{3}}}\\} \\end{equation}\\] \\(\\rho(A_{pp})&lt;1\\). Todo: \\(\\min \\left\\{\\frac{\\left(1-\\sigma_{\\mathrm{R}}\\right) \\sqrt{n}}{2 \\sigma_{\\mathrm{R}}\\|v\\|_{\\mathrm{R}} L}, \\frac{\\left(1-\\sigma_{\\mathrm{C}}\\right)}{2 c_{0} \\delta_{\\mathrm{C}, 2}\\|R\\|_{2} L}\\right\\}\\stackrel{?}=\\frac{\\left(1-\\sigma_{\\mathrm{C}}\\right)}{2 \\sigma_{\\mathrm{C}} \\delta_{\\mathrm{C}, 2}\\|\\mathbf{R}\\|_{2} L}\\) Remark. \\ If we do not use the inequality in (3.14), we can still have \\(b_3-b_2\\hat\\alpha-b_1\\hat\\alpha^2&gt;0\\). However, it is not easy to determine the sign of \\(b_i,i=1,2,3\\) now. Todo:When \\(\\hat\\alpha\\) is sufficiently small, show \\(\\rho(A_{pp})\\approx 1-\\alpha&#39;\\mu\\) "],
["dsgt.html", "Chapter 4 Distributed Stochastic Gradient Tracking(DSGT) Method 4.1 Introduction 4.2 Analysis of Convergence", " Chapter 4 Distributed Stochastic Gradient Tracking(DSGT) Method 4.1 Introduction The idea is similar to that in the Push-Pull method. However, since \\(Y_k\\in\\mathbb{R}^{n\\times p}\\) is used to track the average stochastic gradients in the \\(k\\)th iteration, i.e. \\(\\bar y_k = \\frac{1}{n}\\mathbf{1}^T Y_k=\\frac{1}{n}\\sum\\limits_{i=1}^ng_i(x_{i,k},\\xi_{i,k})\\) provided \\(y_{i,0}=g(x_{i,0},\\xi_{i,0})\\), which is random. Hence we now bound \\(E\\left[\\left\\|\\bar{x}_{k+1}-x^{*}\\right\\|^{2}\\right]\\), \\(E\\left[\\left\\|X_{k+1}-1 \\bar{x}_{k+1}\\right\\|^{2}\\right]\\), and \\(E\\left[\\left\\|Y_{k+1}-\\mathbf{1} \\bar{y}_{k+1}\\right\\|^{2}\\right]\\), which can be seen as variances of \\(\\bar x_k, X_k\\), and \\(Y_k\\). Thus we need to assume \\(g_i(x,\\xi_i),i\\in\\mathcal{N}\\) have the finite variances and also assume they are good estimates of \\(\\nabla f_i(x), i\\in\\mathcal{N}\\). Insead of the definition 3.1 used in the Push-Pull method, we denote \\(\\Vert\\cdot\\Vert\\) as the \\(\\ell_2-\\)norm for vectors and as Frobenius norms for matrices. Hence they are consistent. 4.2 Analysis of Convergence Assumption4.1 \\(\\forall i\\in\\mathcal{N},x\\in\\mathbb{R}^p\\), eahc random vector \\(\\xi_i\\in\\mathbb{R}^m\\) is independent, and \\[\\begin{align} E[g_i(x,\\xi_i)|x]&amp;=\\nabla f_i(x)\\\\ E[\\Vert g_i(x,\\xi_i)-\\nabla f_i(x)\\Vert^2 |x]&amp;\\leq \\sigma, \\exists \\sigma \\tag{4.1} \\end{align}\\] Denote \\(\\mathcal{F}_k\\) as the \\(\\sigma-\\)algebra generated by \\(\\{\\xi_0,...,\\xi_{k-1}\\}\\). We first reveal some properties of the introduced auxiliary variable \\(\\bar y_k=\\frac{1}{n}g_i(x_{i,k},\\xi_{i,k})\\) provided \\(y_{i,0}=g(x_{i,0},\\xi_{i,0})\\). Lemma 4.1 Under Assumption (4.1), \\(h(X)=\\frac{1}{n}\\mathbf{1}^T\\nabla F(X), X\\in\\mathbb{R}^{n\\times p},\\forall k\\geq0\\), we have \\[\\begin{align} E\\left[ \\bar y_k - h(X_k)|\\mathcal{F}_k\\right]&amp;=0\\\\ E\\left[\\left\\|\\bar{y}_{k}-h(X_k)\\right\\|^{2} | \\mathcal{F}_{k}\\right] &amp;\\leq \\frac{\\sigma^{2}}{n} \\end{align}\\] 4.2.1 Relationship between two iteration steps From (3.7), we make \\(\\alpha&#39;=\\frac{1}{n}\\mathbf{1}^T\\boldsymbol\\alpha\\mathbf{1}=\\alpha,\\boldsymbol\\alpha=\\text{diag}(\\alpha,\\alpha,...,\\alpha)\\) and \\(g_k=\\nabla f(\\bar x_k)=\\frac{1}{n}\\sum\\limits_{i=1}^n\\nabla f_i(\\bar x_k)=\\frac{1}{n}\\mathbf{1}^T\\nabla F(\\mathbf{1}\\bar x_k)\\), we have, \\[\\begin{equation} \\bar x_{k+1}-x^* = \\bar x_k - \\alpha(h(X_k)-\\nabla f(\\bar x_k))-\\alpha \\nabla f(\\bar x_k)-\\alpha (\\bar y_k-h(X_k))-x^* \\tag{4.2} \\end{equation}\\] Take norm at both sides, we have \\[\\begin{align} \\Vert \\bar x_{k+1}-x^*\\Vert^2&amp;=\\Vert (\\bar x_k-\\alpha \\nabla f(\\bar x_k)-x^*)-\\alpha(h(X_k)-\\nabla f(\\bar x_k))-\\alpha(\\bar y_k-h(X_k))\\Vert^2\\\\ &amp;=\\Vert \\bar x_k-\\alpha \\nabla f(\\bar x_k)-x^*\\Vert^2 + \\alpha^2\\Vert h(X_k)-\\nabla f(\\bar x_k)\\Vert ^2 + \\alpha^2\\Vert \\bar y_k-h(X_k)\\Vert^2\\\\ &amp;+ 2\\alpha\\left\\langle\\bar x_k-\\alpha \\nabla f(\\bar x_k)-x^*, \\nabla f(\\bar x_k)-h(X_k)\\right\\rangle\\\\&amp;-2\\alpha\\left\\langle\\bar x_k-\\alpha \\nabla f(\\bar x_k)-x^*, \\bar y_k-h(X_k)\\right\\rangle\\\\ &amp;+ 2\\alpha^2\\left\\langle h(X_k)-\\nabla f(\\bar x_k), \\bar y_k-h(X_k)\\right\\rangle \\tag{4.3} \\end{align}\\] Where \\(\\left\\langle\\cdot\\right\\rangle\\) denotes the Frobinus inner product. At both sides, take conditional expectation given \\(\\mathcal{F}_k\\), use lemma 3.4 and lemma 4.1, we have \\[\\begin{align} E\\left[\\Vert \\bar x_{k+1}-x^*\\Vert^2|\\mathcal{F}_k\\right]&amp;\\leq (1-\\alpha\\mu)^2\\Vert \\bar x_k - x^*\\Vert^2 + \\alpha^2\\frac{L^2}{n}\\Vert X_k-\\mathbf{1}\\bar x_k\\Vert^2+\\alpha^2\\frac{\\sigma^2}{n}\\\\&amp;+ 2\\alpha (1-\\alpha\\mu)\\Vert \\bar x_k-x^*\\Vert\\cdot\\frac{L}{\\sqrt{n}}\\Vert X_k-\\mathbf{1}\\bar x_k\\Vert\\\\ &amp;\\leq (1-\\alpha\\mu)^2\\Vert \\bar x_k - x^*\\Vert^2 + \\alpha^2\\frac{L^2}{n}\\Vert X_k-\\mathbf{1}\\bar x_k\\Vert^2+\\alpha^2\\frac{\\sigma^2}{n}\\\\ &amp;+ \\alpha\\left((1-\\alpha\\mu)^{2} \\mu\\left\\|\\bar{x}_{k}-x^{*}\\right\\|^{2}+\\frac{L^{2}}{\\mu n}\\left\\|X_{k}-1 \\bar{x}_{k}\\right\\|^{2}\\right)\\\\ &amp;=(1-\\alpha\\mu)(1-(\\alpha\\mu)^2)\\Vert \\bar x_k-x^*\\Vert^2+\\frac{\\alpha L^2}{\\mu n}(1+\\alpha\\mu)\\Vert X_k-\\mathbf{1}\\bar x_k\\Vert^2 + \\frac{\\alpha^2\\sigma^2}{n}\\\\ &amp;\\leq (1-\\alpha\\mu)\\Vert \\bar x_k-x^*\\Vert^2+\\frac{\\alpha L^2}{\\mu n}(1+\\alpha\\mu)\\Vert X_k-\\mathbf{1}\\bar x_k\\Vert^2 + \\frac{\\alpha^2\\sigma^2}{n} \\tag{4.4} \\end{align}\\] Remark. \\ When each agent \\(i\\) takes different step size \\(\\alpha_i,i\\in\\mathcal{N}\\), \\(\\alpha(\\bar y_k-h(\\bar x_k))\\) in (4.2) becomes \\(\\frac{1}{n}\\mathbf{1}^T\\boldsymbol\\alpha(Y_k-\\mathbf{1}h(\\bar x_k))\\), then we may use the following to continuou the steps in (4.4). \\[\\begin{align} \\frac{1}{n}\\mathbf{1}^T\\boldsymbol\\alpha(Y_k-\\mathbf{1}h(\\bar x_k))&amp;=\\frac{1}{n}\\mathbf{1}^T\\boldsymbol\\alpha\\left[(Y_k-\\mathbf{1}\\bar y_k)+\\mathbf{1}(\\bar y_k-h(\\bar x_k))\\right] \\end{align}\\] \\(0&lt;(1-(\\alpha\\mu)^2)&lt;1\\) will be guranteed by lemma 3.2. This also hints us how to separate \\(\\frac{2 \\alpha (1-\\alpha\\mu) L}{\\sqrt{n}}\\left\\|\\bar{x}_{k}-x^{*}\\right\\|\\left\\|X_{k}-1 \\bar{x}_{k}\\right\\|\\) In the Push-Pull method, we see matrices \\(R-\\frac{\\mathbf{1}u^T}{n}\\) and \\(C-\\frac{v\\mathbf{1}^T}{n}\\). Here for \\(W\\in\\mathbb{R}^{n\\times n}\\), \\(W-\\frac{\\mathbf{1}\\mathbf{1}^T}{n}\\) also has significant uses. \\(\\mathbf{1}\\) can be seen as left and right eigenvalue of \\(W\\), and from \\(W\\mathbf{1}=\\mathbf{1}\\) we have Lemma 4.2 Given the graph \\(\\mathcal{G}\\) corresponding to the network of agents is undirected and connected, and \\(W\\) is doubly stochastic and \\(w_{ii}&gt;0,\\exists i\\in\\mathcal{N}\\), we have the spectral norm \\(\\rho_W\\) of \\(W-\\frac{\\mathbf{1}\\mathbf{1}^T}{n}\\), \\(\\rho_W&lt;1\\), and \\[ \\|W \\omega-\\mathbf{1} \\bar{\\omega}\\| \\leq \\rho_{w}\\|\\omega-1 \\bar{\\omega}\\|, \\forall \\omega\\in\\mathbb{R}^{n\\times p}, \\bar\\omega = \\frac{1}{n}\\mathbf{1}\\omega \\] Then \\[\\begin{align} \\Vert X_{k+1}-\\mathbf{1}\\bar x_{k+1}\\Vert^2&amp;\\stackrel{\\text{iterate}}{=} \\Vert WX_k-\\alpha WY_k-\\mathbf{1}(\\bar x_k-\\alpha\\bar y_k)\\Vert^2\\\\ &amp;=\\left\\|W X_{k}-1 \\bar{x}_{k}\\right\\|^{2}-2 \\alpha\\left\\langle W X_{k}-1 \\bar{x}_{k}, W Y_{k}-1 \\bar{y}_{k}\\right\\rangle+\\alpha^{2}\\left\\|W Y_{k}-1 \\bar{y}_{k}\\right\\|^{2}\\\\ &amp;\\leq \\rho_{w}^{2}\\left\\|\\mathrm{x}_{k}-1 \\bar{x}_{k}\\right\\|^{2}+\\alpha \\rho_{w}^{2}\\left[\\frac{\\left(1-\\rho_{w}^{2}\\right)}{2 \\alpha \\rho_{w}^{2}}\\left\\|\\mathrm{x}_{k}-1 \\bar{x}_{k}\\right\\|^{2}+\\frac{2 \\alpha \\rho_{w}^{2}}{\\left(1-\\rho_{w}^{2}\\right)}\\left\\|\\mathrm{y}_{k}-1 \\bar{y}_{k}\\right\\|^{2}\\right]+\\alpha^{2} \\rho_{w}^{2}\\left\\|\\mathrm{y}_{k}-1 \\bar{y}_{k}\\right\\|^{2}\\\\ &amp;= \\frac{\\left(1+\\rho_{w}^{2}\\right)}{2}\\left\\|X_{k}-1 \\bar{x}_{k}\\right\\|^{2}+\\alpha^{2} \\frac{\\left(1+\\rho_{w}^{2}\\right) \\rho_{w}^{2}}{\\left(1-\\rho_{w}^{2}\\right)}\\left\\|Y_{k}-1 \\bar{y}_{k}\\right\\|^{2} \\tag{4.5} \\end{align}\\] Remark. For lemma 4.2, a counter example for not assuming assumption connected graph is that graph \\(\\mathcal{G}\\) induced by the identity matrix \\(I\\), then the spectral norm of \\(I-\\frac{\\mathbf{1}\\mathbf{1}^T}{n}\\) is \\(\\rho_w = 1\\). For \\(E\\left[\\Vert Y_{k+1}-\\mathbf{1}\\bar y_{k+1}\\Vert^2|\\mathcal{F}_k\\right]\\). We write \\(G(X_k,\\xi_k):=G_k,\\nabla_k:=\\nabla F(X_k)\\) for simplicity, then we have, \\[\\begin{align} \\Vert Y_{k+1}-\\mathbf{1}\\bar y_{k+1}\\Vert^2&amp;= \\Vert WY_k + G_{k+1} - G_k -\\mathbf{1}\\bar y_k + \\mathbf{1}(\\bar y_k-\\bar y_{k+1})\\Vert^2\\\\ &amp;= \\Vert WY_k - \\mathbf{1}\\bar y_k\\Vert^2 + \\Vert G_{k+1} - G_k\\Vert^2 + \\Vert \\mathbf{1}(\\bar y_k-\\bar y_{k+1})\\Vert^2 + 2\\langle WY_k - \\mathbf{1}\\bar y_k, G_{k+1}-G_k\\rangle\\\\ &amp;+ 2\\langle WY_k-\\mathbf{1}\\bar y_k + G_{k+1}-G_k, \\mathbf{1}(\\bar y_k-\\bar y_{k+1})\\rangle \\\\ &amp;=\\Vert WY_k - \\mathbf{1}\\bar y_k\\Vert^2 + \\Vert G_{k+1} - G_k\\Vert^2 + \\Vert \\mathbf{1}(\\bar y_k-\\bar y_{k+1})\\Vert^2 + 2\\langle WY_k - \\mathbf{1}\\bar y_k, G_{k+1}-G_k\\rangle\\\\ &amp;+ 2\\langle Y_{k+1}-\\mathbf{1}\\bar y_{k+1}-\\mathbf{1}(\\bar y_k-\\bar y_{k+1}) , \\mathbf{1}(\\bar y_k-\\bar y_{k+1})\\rangle\\\\ &amp;\\stackrel{?}{=} \\Vert WY_k - \\mathbf{1}\\bar y_k\\Vert^2 + \\Vert G_{k+1} - G_k\\Vert^2 - n\\Vert \\bar y_k-\\bar y_{k+1}\\Vert^2 + 2\\langle WY_k - \\mathbf{1}\\bar y_k, G_{k+1}-G_k\\rangle \\\\ &amp;\\leq \\rho^2_w\\Vert Y_k-\\mathbf{1}\\bar y_k\\Vert^2+\\Vert G_{k+1} - G_k\\Vert^2 + 2\\langle WY_k - \\mathbf{1}\\bar y_k, G_{k+1}-G_k\\rangle \\tag{4.6} \\end{align}\\] Todo: \\(\\Vert Y_{k+1}-\\mathbf{1}\\bar y_{k+1}\\Vert^2\\) 4.2.2 Linear system of inequalities Theorem 4.1 \\[\\begin{equation} A_{dsgt}=\\left[\\begin{array}{ccc} {1-\\alpha \\mu} &amp; {\\frac{\\alpha L^{2}}{\\mu n}(1+\\alpha \\mu)} &amp; {0} \\\\ {0} &amp; {\\frac{1}{2}\\left(1+\\rho_{w}^{2}\\right)} &amp; {\\alpha^{2} \\frac{\\left(1+\\rho_{w}^{2}\\right) \\rho_{w}^{2}}{\\left(1-\\rho_{w}^{2}\\right)}} \\\\ 2 \\alpha n L^{3} &amp; \\left(\\frac{1}{\\beta}+2\\right) {\\|W-I\\|^{2} L^{2}+3 \\alpha L^{3}} &amp; {\\left(1+4 \\alpha L+2 \\alpha^{2} L^{2}+\\beta\\right) \\rho_{w}^{2}} \\end{array}\\right] \\end{equation}\\] where \\(\\beta=\\frac{1-\\rho_{w}^{2}}{2 \\rho_{w}^{2}}-4 \\alpha L-2 \\alpha^{2} L^{2}\\) After getting the element of the matrix \\(A\\) of the linear system of inequalities, we again use lemma 3.2 to build conditions on step size \\(\\alpha\\) so that the spectral radius of \\(A\\), \\(\\rho(A)&lt;1\\). 4.2.3 Spectral radius of A Next, we derive the conditions on \\(\\alpha\\) so that \\(\\rho(A_{dsgt})&lt;1\\) by computing \\(det(I-A_{dsgt})\\) and make it greater than 0. We expand \\(det(I-A_{dsgt})\\) according to the first column, \\[\\begin{align} det(I-A_{dsgt}) &amp;= (1-a_{11})[(1-a_{22})(1-a_{33})-a_{32}a_{23}]-a_{12}a_{23}a_{31} \\tag{4.7} \\end{align}\\] Notice that in lemma 3.2, it requires \\(\\lambda-a_{ii}&gt;0, i=1,2,3\\) and in (4.7), we already have \\((1-a_{11})(1-a_{22})(1-a_{33})-C\\). Hence we may expect \\(C\\) to be bounded by the term of \\(c_0(1-a_{11})(1-a_{22})(1-a_{33})\\), where \\(c_0&lt;1\\) is a positive number. So when we make \\[\\begin{align} a_{23}a_{32}&amp;\\leq \\frac{1}{\\Gamma}(1-a_{22})(1-a_{33})=\\frac{1}{\\Gamma}(\\frac{1-\\rho_w^2}{2})^2,\\\\ \\tag{4.8} \\end{align}\\] \\[\\begin{align} a_{12}a_{23}a_{31}&amp;\\leq \\frac{1}{\\Gamma+1}(1-a_{11})[(1-a_{22})(1-a_{33})-a_{32}a_{23}] \\tag{4.9} \\end{align}\\] we have \\[\\begin{equation} det(I-A_{dsgt}) \\geq \\frac{\\Gamma-1}{\\Gamma+1}(1-a_{11})(1-a_{22})(1-a_{33})&gt;0 \\end{equation}\\] Next, we derive what exactly conditions \\(\\alpha\\) should satisfy to make the inequalities (4.8) and (4.9) hold. Additionally, in the proof of building linear inequality system of \\(E\\left[\\Vert Y_{k+1}-\\mathbf{1}\\bar y_{k+1}\\Vert^2|\\mathcal{F}_k\\right]\\), the author uses \\[\\begin{align} 2\\|\\mathbf{W}-\\mathbf{I}\\| L \\rho_{w}\\left\\|\\mathbf{y}_{k}-\\mathbf{1} \\bar{y}_{k}\\right\\|\\left\\|\\mathbf{x}_{k}-1 \\bar{x}_{k}\\right\\|\\leq \\beta \\rho_{w}^{2} \\left\\|\\mathbf{y}_{k}-1 \\bar{y}_{k}\\right\\|^{2} | \\mathcal{F}_{k}+\\frac{1}{\\beta}\\|\\mathbf{W}-\\mathbf{I}\\|^{2} L^{2}\\left\\|\\mathbf{x}_{k}-1 \\bar{x}_{k}\\right\\|^{2} \\end{align}\\] Thus we also need \\(\\beta&gt;0\\). Since\\(\\beta\\) is the quadratic about \\(\\alpha&gt;0\\), so \\(\\beta&gt;0\\) when \\[\\begin{equation} \\alpha &lt; \\frac{\\sqrt{1+3\\rho_w^2}}{2\\rho_wL}-\\frac{1}{L} \\tag{4.10} \\end{equation}\\] The author uses \\(\\alpha\\leq \\frac{1-\\rho_w^2}{12\\rho_w^2L}\\) to gurantee \\(\\beta&gt;0\\) since \\(0&lt;\\rho_w&lt;1\\). \\[\\begin{align} \\beta \\geq \\frac{1-\\rho_{w}^{2}}{2 \\rho_{w}^{2}}-\\frac{1-\\rho_{w}^{2}}{3 \\rho_{w}}-\\frac{\\left(1-\\rho_{w}^{2}\\right)^{2}}{72 \\rho_{w}^{2}} \\geq \\frac{11\\left(1-\\rho_{w}^{2}\\right)}{72 \\rho_{w}^{2}} \\geq \\frac{1-\\rho_{w}^{2}}{8 \\rho_{w}^{2}}&gt;0 \\tag{4.11} \\end{align}\\] The coefficient \\(C&gt;0\\) of \\(\\frac{1-\\rho_w^2}{C\\rho_w^2L}\\) can be chosen from \\((\\frac{2}{\\sqrt{5}-2}\\approx8.47,+\\infty)\\). Figure 4.1 plots the function \\(\\frac{\\sqrt{1+3\\rho_w^2}}{2\\rho_w}-1-\\frac{1-\\rho_w^2}{12\\rho_w^2}\\), we see from figure (fig:f1) that the constraint in (4.10) is not better than \\(\\alpha\\leq \\frac{1-\\rho_w^2}{12\\rho_w^2L}\\), especilly when \\(\\rho_w\\) is close to \\(0\\). Figure 4.1: The difference between two constraints In short, a less strict \\(\\alpha\\) can be chosen as \\(\\alpha\\leq\\frac{1-\\rho_w^2}{9\\rho_wL}\\). For inequality (4.8), substitute \\(\\beta\\geq\\frac{1-\\rho_w^2}{C&#39;\\rho_w^2}\\), where \\(C&#39;\\) is subject to the chioce of \\(C\\in(\\frac{2}{\\sqrt{5}-2}\\approx8.47,+\\infty)\\). We follow the author’s choice here, then the LHS of (4.8) is less than the following, \\[\\begin{equation} \\frac{\\left(1+\\rho_{w}^{2}\\right) \\rho_{w}^{2}}{\\left(1-\\rho_{w}^{2}\\right)}\\alpha^{2}\\left[\\frac{\\left(2+6 \\rho_{w}^{2}\\right)}{\\left(1-\\rho_{w}^{2}\\right)}\\|\\mathbf{W}-\\mathbf{I}\\|^{2} L^{2}+\\frac{\\left(1-\\rho_{w}^{2}\\right)}{4 \\rho_{w}^{2}} L^{2}\\right]\\leq RHS=\\frac{\\left(1-\\rho_{w}^{2}\\right)^{2}}{4 \\Gamma} \\tag{4.12} \\end{equation}\\] Hence, it suffice to make \\(\\alpha\\), \\[\\begin{equation} \\alpha\\leq \\frac{\\left(1-\\rho_{w}^{2}\\right)^{2}}{2 \\sqrt{\\Gamma} L \\max \\left(6 \\rho_{w}\\|\\mathbf{W}-\\mathbf{I}\\|, 1-\\rho_{w}^{2}\\right)}\\leq \\frac{\\left(1-\\rho_{w}^{2}\\right)^{2}}{L \\sqrt{\\Gamma\\left(1+\\rho_{w}^{2}\\right)} \\sqrt{4 \\rho_{w}^{2}\\left(2+6 \\rho_{w}^{2}\\right)\\|\\mathbf{W}-\\mathbf{I}\\|^{2}+\\left(1-\\rho_{w}^{2}\\right)^{2}}} \\end{equation}\\] The latter inequality comes from \\(\\rho_w^2&lt;1&lt;\\frac{7}{6}\\) and \\(a+b\\leq 2\\max(a,b)\\). For the inequality (4.9), from the inequality (4.8), it is sufficient to have \\[\\begin{equation} a_{12} a_{23} a_{31}\\leq \\frac{(\\Gamma-1)}{\\Gamma(\\Gamma+1)}\\left(1-a_{11}\\right)\\left(1-a_{22}\\right)\\left(1-a_{33}\\right) \\end{equation}\\] Thus, \\[\\begin{equation} \\alpha \\leq \\frac{\\left(1-\\rho_{w}^{2}\\right)}{3 \\rho_{w}^{2 / 3} L}\\left[\\frac{\\mu^{2}}{L^{2}} \\frac{(\\Gamma-1)}{\\Gamma(\\Gamma+1)}\\right]^{1 / 3} \\end{equation}\\] Then, when the step size \\(\\alpha\\) is chosen such that \\[\\begin{equation} \\alpha \\leq \\min \\left\\{\\frac{\\left(1-\\rho_{w}^{2}\\right)}{12 \\rho_{w} L}, \\frac{\\left(1-\\rho_{w}^{2}\\right)^{2}}{2 \\sqrt{\\Gamma} L \\max \\left\\{6 \\rho_{w}\\|\\mathbf{W}-\\mathbf{I}\\|, 1-\\rho_{w}^{2}\\right\\}}, \\frac{\\left(1-\\rho_{w}^{2}\\right)}{3 \\rho_{w}^{2 / 3} L}\\left[\\frac{\\mu^{2}}{L^{2}} \\frac{(\\Gamma-1)}{\\Gamma(\\Gamma+1)}\\right]^{1 / 3}\\right\\} \\tag{4.13} \\end{equation}\\] we have \\(\\rho(A_{dsgt})&lt;1\\). Remark. \\ From the proof above, making \\(\\beta = \\frac{1-\\rho_{w}^{2}}{2 \\rho_{w}^{2}}-4 \\alpha L-2 \\alpha^{2} L^{2}\\) is a little tricky. \\((1-a_{22})=\\frac{1-\\rho_w^2}{2}\\) can give us hints about such a choice. "],
["summary-of-push-pull-and-dsgt.html", "Chapter 5 Summary of Push-Pull and DSGT", " Chapter 5 Summary of Push-Pull and DSGT "],
["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
