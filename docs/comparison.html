<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 comparison | Notes about Distributed Optimization</title>
  <meta name="description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 comparison | Notes about Distributed Optimization" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 comparison | Notes about Distributed Optimization" />
  
  <meta name="twitter:description" content="These are some notes about distributed optimization, including some algorithms, their analysis of convergence, and some understandings of my own. Although the authors of those literature already provide proofs, I complement some details and try to figure out why should we prove in such a way. Hence they could be more easy to understand, especially for myself." />
  

<meta name="author" content="Kun Huang" />


<meta name="date" content="2020-04-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-sharp.html"/>
<link rel="next" href="sec-ediff.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="orgnization-of-the-notes.html"><a href="orgnization-of-the-notes.html"><i class="fa fa-check"></i><b>2</b> Orgnization of the Notes</a></li>
<li class="chapter" data-level="3" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html"><i class="fa fa-check"></i><b>3</b> The Push-Pull Method</a><ul>
<li class="chapter" data-level="3.1" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#analysis-of-convergence"><i class="fa fa-check"></i><b>3.2</b> Analysis of Convergence</a><ul>
<li class="chapter" data-level="3.2.1" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#relationship-between-two-iteration-steps"><i class="fa fa-check"></i><b>3.2.1</b> Relationship between two iteration steps</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#inequalities"><i class="fa fa-check"></i><b>3.2.2</b> Inequalities</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-push-pull-method.html"><a href="the-push-pull-method.html#spectral-radius-of-a"><i class="fa fa-check"></i><b>3.2.3</b> Spectral radius of A</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sec-dsgt.html"><a href="sec-dsgt.html"><i class="fa fa-check"></i><b>4</b> Distributed Stochastic Gradient Tracking(DSGT) Method</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-dsgt.html"><a href="sec-dsgt.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="sec-dsgt.html"><a href="sec-dsgt.html#analysis-of-convergence-1"><i class="fa fa-check"></i><b>4.2</b> Analysis of Convergence</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-dsgt.html"><a href="sec-dsgt.html#relationship-between-two-iteration-steps-1"><i class="fa fa-check"></i><b>4.2.1</b> Relationship between two iteration steps</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-dsgt.html"><a href="sec-dsgt.html#inequalities-1"><i class="fa fa-check"></i><b>4.2.2</b> Inequalities</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-dsgt.html"><a href="sec-dsgt.html#spectral-radius-of-a_dsgt"><i class="fa fa-check"></i><b>4.2.3</b> Spectral radius of <span class="math inline">\(A_{dsgt}\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary-of-push-pull-and-dsgt.html"><a href="summary-of-push-pull-and-dsgt.html"><i class="fa fa-check"></i><b>5</b> Summary of PuSh-Pull and DSGT</a><ul>
<li class="chapter" data-level="5.1" data-path="summary-of-push-pull-and-dsgt.html"><a href="summary-of-push-pull-and-dsgt.html#questions"><i class="fa fa-check"></i><b>5.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gossip-like-push-pull-and-dsgt.html"><a href="gossip-like-push-pull-and-dsgt.html"><i class="fa fa-check"></i><b>6</b> Gossip-like Push-Pull and DSGT</a><ul>
<li class="chapter" data-level="6.1" data-path="gossip-like-push-pull-and-dsgt.html"><a href="gossip-like-push-pull-and-dsgt.html#g-push-pull"><i class="fa fa-check"></i><b>6.1</b> G-Push-Pull</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sec-asynt.html"><a href="sec-asynt.html"><i class="fa fa-check"></i><b>7</b> Asymptotic network independence</a><ul>
<li class="chapter" data-level="7.1" data-path="sec-asynt.html"><a href="sec-asynt.html#sgd-and-dsgd"><i class="fa fa-check"></i><b>7.1</b> SGD and DSGD</a></li>
<li class="chapter" data-level="7.2" data-path="sec-asynt.html"><a href="sec-asynt.html#bounds"><i class="fa fa-check"></i><b>7.2</b> Bounds</a></li>
<li class="chapter" data-level="7.3" data-path="sec-asynt.html"><a href="sec-asynt.html#possible-ways-to-achieve-asymptotic-network-independece"><i class="fa fa-check"></i><b>7.3</b> Possible ways to achieve asymptotic network independece</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html"><i class="fa fa-check"></i><b>8</b> Some results in asymptotic network independence</a><ul>
<li class="chapter" data-level="8.1" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#compressed-communication"><i class="fa fa-check"></i><b>8.1</b> Compressed Communication</a><ul>
<li class="chapter" data-level="8.1.1" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#choco-sgd"><i class="fa fa-check"></i><b>8.1.1</b> CHOCO-SGD</a></li>
<li class="chapter" data-level="8.1.2" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#stochastic-gradient-push"><i class="fa fa-check"></i><b>8.1.2</b> Stochastic gradient push</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec-referasymnt.html"><a href="sec-referasymnt.html#d2"><i class="fa fa-check"></i><b>8.2</b> <span class="math inline">\(D^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sec-sharp.html"><a href="sec-sharp.html"><i class="fa fa-check"></i><b>9</b> A sharp estimate of the transient time of DSGD</a><ul>
<li class="chapter" data-level="9.1" data-path="sec-sharp.html"><a href="sec-sharp.html#uk-and-vk"><i class="fa fa-check"></i><b>9.1</b> <span class="math inline">\(U(k)\)</span> and <span class="math inline">\(V(k)\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="sec-sharp.html"><a href="sec-sharp.html#asymptotic-network-independence-of-dsgd"><i class="fa fa-check"></i><b>9.2</b> Asymptotic network independence of DSGD</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-sharp.html"><a href="sec-sharp.html#sublinear-rate"><i class="fa fa-check"></i><b>9.2.1</b> Sublinear rate</a></li>
<li class="chapter" data-level="9.2.2" data-path="sec-sharp.html"><a href="sec-sharp.html#asymptotic-network-independence"><i class="fa fa-check"></i><b>9.2.2</b> Asymptotic network independence</a></li>
<li class="chapter" data-level="9.2.3" data-path="sec-sharp.html"><a href="sec-sharp.html#improved-bound"><i class="fa fa-check"></i><b>9.2.3</b> Improved Bound</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="sec-sharp.html"><a href="sec-sharp.html#transient-time"><i class="fa fa-check"></i><b>9.3</b> Transient time</a></li>
<li class="chapter" data-level="9.4" data-path="sec-sharp.html"><a href="sec-sharp.html#sharpness"><i class="fa fa-check"></i><b>9.4</b> Sharpness</a></li>
<li class="chapter" data-level="9.5" data-path="sec-sharp.html"><a href="sec-sharp.html#summary"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="comparison.html"><a href="comparison.html"><i class="fa fa-check"></i><b>10</b> comparison</a><ul>
<li class="chapter" data-level="10.1" data-path="comparison.html"><a href="comparison.html#assumptions-of-different-schemes"><i class="fa fa-check"></i><b>10.1</b> Assumptions of different schemes</a></li>
<li class="chapter" data-level="10.2" data-path="comparison.html"><a href="comparison.html#convergence-rate"><i class="fa fa-check"></i><b>10.2</b> Convergence rate</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sec-ediff.html"><a href="sec-ediff.html"><i class="fa fa-check"></i><b>11</b> Exact diffusion</a><ul>
<li class="chapter" data-level="11.1" data-path="sec-ediff.html"><a href="sec-ediff.html#decreasing-stepsize"><i class="fa fa-check"></i><b>11.1</b> Decreasing stepsize</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sec-PUDA.html"><a href="sec-PUDA.html"><i class="fa fa-check"></i><b>12</b> Decentralized Proximal Gradient Algorithms with Linear Convergence Rates</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-PUDA.html"><a href="sec-PUDA.html#uda"><i class="fa fa-check"></i><b>12.1</b> UDA</a></li>
<li class="chapter" data-level="12.2" data-path="sec-PUDA.html"><a href="sec-PUDA.html#puda"><i class="fa fa-check"></i><b>12.2</b> PUDA</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="nids.html"><a href="nids.html"><i class="fa fa-check"></i><b>13</b> NIDS</a></li>
<li class="chapter" data-level="14" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>14</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes about Distributed Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="comparison" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> comparison</h1>
<p>We consider DSGT<span class="citation">(Pu and Nedić <a href="#ref-pu2018distributed">2018</a>)</span>, DSGD<span class="citation">(Pu, Olshevsky, and Paschalidis <a href="#ref-pu2019sharp">2019</a><a href="#ref-pu2019sharp">a</a>)</span>, DGD, EXTRA<span class="citation">(Shi et al. <a href="#ref-shi2015extra">2015</a>)</span>, D-PSGD<span class="citation">(Lian et al. <a href="#ref-lian2017can">2017</a>)</span> and <span class="math inline">\(D^2\)</span><span class="citation">(Tang et al. <a href="#ref-tang2018d">2018</a>)</span> here. Generally, EXTRA, DSGT, and <span class="math inline">\(D^2\)</span> achieve better convergence properties because of adding some correction terms. The following table lists the schemes of these algorithms.</p>
<table style="width:94%;">
<colgroup>
<col width="9%" />
<col width="84%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Name</th>
<th align="center">Scheme</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">c-SGD</td>
<td align="center"><span class="math inline">\(x_{k+1}=x_k-\alpha_k\tilde g(k)\)</span></td>
</tr>
<tr class="even">
<td align="center">DGD</td>
<td align="center"><span class="math inline">\(X_{k+1}=WX_k-\alpha_k\nabla f(X_k)\)</span></td>
</tr>
<tr class="odd">
<td align="center">D-PSGD</td>
<td align="center"><span class="math inline">\(X_{k+1}=WX_{k}-\alpha G_k\)</span></td>
</tr>
<tr class="even">
<td align="center">EXTRA</td>
<td align="center"><span class="math inline">\(X_{k+1}=WX_k-\alpha \nabla f(X_k)+\sum\limits_{t=0}^{k-1}(W-\tilde W)X_t\)</span></td>
</tr>
<tr class="odd">
<td align="center">DSGD</td>
<td align="center"><span class="math inline">\(X_{k+1}=W(X_k-\alpha_{k}G_k)\)</span></td>
</tr>
<tr class="even">
<td align="center">DSGT</td>
<td align="center"><span class="math display">\[X_{k+1}=W(X_k-\alpha_k Y_k)\\Y_{k+1}=WY_k+G_{k+1}-G_k\]</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(D^2\)</span></td>
<td align="center"><span class="math display">\[X_1=W(X_0-\alpha G_0)\\X_{k+1}=W(X_k-\alpha G_k)+W(X_k-X_{k-1}-\alpha G_{k-1})\]</span></td>
</tr>
</tbody>
</table>
To see the influence of the correction term <span class="math inline">\(H\)</span>, suppose we have a scheme,
<span class="math display" id="eq:Hcor">\[\begin{equation}
X_{k+1}=W(X_k-\alpha_k G_k+H)
\tag{10.1}
\end{equation}\]</span>
We assume <span class="math inline">\(X_k\)</span> has achieved the optimum <span class="math inline">\(X_k=\mathbf{1}{x^*}\)</span>, then from <span class="math inline">\(W\mathbf{1}=\mathbf{1}\)</span>, we have
<span class="math display">\[\begin{align}
X_{k+1}=\mathbf{1}x^*-\alpha_kWG_k+WH
\end{align}\]</span>
<p>Thus <span class="math inline">\(nR&#39;(k+1)=E\left[\Vert X_{k+1}-\mathbf{1}x^*\Vert^2\right]\)</span> is affected by <span class="math display">\[
E\left[\Vert H-\alpha_k G_k\Vert^2\right]
\]</span> Similar for DSGD, we have <span class="math inline">\(nR&#39;(k+1)\)</span> is affected by <span class="math display">\[
E\left[\Vert\alpha_k G_k\Vert^2 \right]=\alpha_k E\left[\Vert G_k\Vert^2 \right]
\]</span></p>
We can see that a sequence of dimishing <span class="math inline">\(\alpha_k\)</span> can decrease <span class="math inline">\(E\left[\Vert G_k\Vert^2 \right]\)</span> as <span class="math inline">\(k\)</span> gets large, which explains why dimishing stepsizes can improve fixed stepsize scheme to an exact convergence. On the other hand, we can also design a distributed algorithm <span class="math inline">\(s.t.\)</span>
<span class="math display" id="eq:design">\[\begin{equation}
E\left[\Vert H-\alpha_k G_k\Vert^2\right]\leq \left[\Vert\alpha_k G_k\Vert^2 \right]
\tag{10.2}
\end{equation}\]</span>
<p>and conserve the consensus property.</p>
<div id="assumptions-of-different-schemes" class="section level2">
<h2><span class="header-section-number">10.1</span> Assumptions of different schemes</h2>
<p>The following table summarizes the assumptions needed for each schemes. The column of <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\zeta^2\)</span> denote <span class="math display">\[
E_{\xi }\left\|g_{i}(x ; \xi)-\nabla f_{i}(x)\right\|^{2} \leqslant \sigma^{2}, \quad \forall i, \forall x
\]</span> and <span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n}\left\|\nabla f_{i}(x)-\nabla f(x)\right\|^{2} \leqslant \zeta^{2}, \quad \forall i, \forall x
\]</span> respectively.</p>
<p><span class="math inline">\(\zeta_0=\frac{1}{n} \sum\limits_{i=1}^{n}\left\|\nabla f_{i}(\mathbf{0})-\nabla f(\mathbf{0})\right\|^{2}\)</span>; <span class="math inline">\(\rho_w\)</span> is the spectral norm of <span class="math inline">\(W-\frac{\mathbf{1}\mathbf{1}^T}{n}\)</span></p>
<table>
<colgroup>
<col width="3%" />
<col width="27%" />
<col width="11%" />
<col width="18%" />
<col width="23%" />
<col width="7%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Name</th>
<th align="center"><span class="math inline">\(f_i(x)\)</span></th>
<th align="center"><span class="math inline">\(\nabla f_i(x)\)</span></th>
<th align="center"><span class="math inline">\(W\)</span></th>
<th align="center">Spectral gap</th>
<th align="center"><span class="math inline">\(\sigma^2\)</span></th>
<th align="center"><span class="math inline">\(\zeta^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">c-SGD</td>
<td align="center"></td>
<td align="center">Lipschitz continuous</td>
<td align="center"><span class="math inline">\(\backslash\)</span></td>
<td align="center"><span class="math inline">\(\backslash\)</span></td>
<td align="center"><span class="math inline">\(\backslash\)</span></td>
<td align="center"><span class="math inline">\(\backslash\)</span></td>
</tr>
<tr class="even">
<td align="center">DGD</td>
<td align="center"></td>
<td align="center">Lipschitz continuous</td>
<td align="center">Sym and <span class="math inline">\(W\mathbf{1}=\mathbf{1}\)</span></td>
<td align="center"><span class="math inline">\(\rho_w&lt;1\)</span></td>
<td align="center"><span class="math inline">\(\backslash\)</span></td>
<td align="center"><span class="math inline">\(\backslash\)</span></td>
</tr>
<tr class="odd">
<td align="center">D-PSGD</td>
<td align="center"></td>
<td align="center">Lipschitz continuous</td>
<td align="center">Sym and doubly stochastic</td>
<td align="center"><span class="math inline">\(\lambda_2^2&lt;1\)</span></td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr class="even">
<td align="center">EXTRA</td>
<td align="center">convex (<span class="math inline">\(f\)</span> strongly convex leads to better rate)</td>
<td align="center">Lipschitz continuous</td>
<td align="center">Sym and <span class="math inline">\(W\mathbf{1}=\mathbf{1}\)</span></td>
<td align="center"><span class="math inline">\(\lambda_\min(W)\geq0\)</span></td>
<td align="center"><span class="math inline">\(\backslash\)</span></td>
<td align="center"><span class="math inline">\(\backslash\)</span></td>
</tr>
<tr class="odd">
<td align="center">DSGD</td>
<td align="center">strongly convex</td>
<td align="center">Lipschitz continuous</td>
<td align="center">Sym and doubly stochastic</td>
<td align="center"><span class="math inline">\(\rho_w\)</span></td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
<tr class="even">
<td align="center">DSGT</td>
<td align="center">strongly convex</td>
<td align="center">Lipschitz continuous</td>
<td align="center">doubly stochastic</td>
<td align="center"><span class="math inline">\(\rho_w\)</span></td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(D^2\)</span></td>
<td align="center"></td>
<td align="center">Lipschitz continuous</td>
<td align="center">Sym and <span class="math inline">\(W\mathbf{1}=\mathbf{1}\)</span></td>
<td align="center"><span class="math inline">\(\lambda_2&lt;1,\lambda_\min\geq-\frac{1}{3}\)</span></td>
<td align="center">Y</td>
<td align="center"><span class="math inline">\(\zeta_0\)</span></td>
</tr>
</tbody>
</table>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> \</p>
<ul>
<li><p>EXTRA assumes a less restrictive assumption on <span class="math inline">\(W\)</span> which leads to <span class="math inline">\(W\mathbf{1}=\mathbf{1}\)</span> and <span class="math inline">\(\lambda(W)\in(-1, 1]\)</span>;</p></li>
<li><p>In EXTRA, if <span class="math inline">\(\lambda_{\min}(W)&lt;0\)</span>, we can replace <span class="math inline">\(W\)</span> by <span class="math inline">\(\frac{I+W}{2}\)</span>;</p></li>
<li><p>In EXTRA, if we assume <span class="math inline">\(g(x)=\frac{1}{n}\sum\limits_{i=1}^n f_i(x) + \frac{1}{4\alpha}\Vert x\Vert_{\tilde{W}-W}\)</span> is restricted strongly convex w.r.t. <span class="math inline">\(x^*\)</span>, we can derive a convergence rate <span class="math inline">\(\mathcal{O}(1+\delta)^{-k}\)</span>. This conditon does not require all <span class="math inline">\(f_i\)</span> to be individually restricted strongly convex, which is less restrictive to that in DSGT and DSGD.</p></li>
<li><p>DSGD addtionally assume <span class="math inline">\(\sum\limits_{i=1}^{n}\left\|x_{i}(0)-x^{*}\right\|^{2}=\mathcal{O}(n)\)</span> and <span class="math inline">\(\sum\limits_{i=1}^{n}\left\|\nabla f_{i}\left(x^{*}\right)\right\|^{2}=\mathcal{O}(n)\)</span>;</p></li>
<li><p><span class="math inline">\(D^2\)</span> is less senstive to the data variance across workers compared to D-PSGD</p></li>
</ul>
</div>

<p>options:</p>
<ul>
<li><p>What is the convergence rate when assuming <span class="math inline">\(f_i\)</span> to be strongly convex in <span class="math inline">\(D^2\)</span>?</p></li>
<li><p><span class="citation">Li, Shi, and Yan (<a href="#ref-li2019decentralized">2019</a>)</span> assume a sturcture of <span class="math inline">\(f_i(x)=s_i(x)+r_i(x)\)</span> on the objective function, where <span class="math inline">\(s_i\)</span> is differentiable and has a Lipschitz continuous gradient with parameter <span class="math inline">\(L\)</span> and <span class="math inline">\(r_i\)</span> is proximable. They improve The PG-EXTRA in the speed and the dependency of convergence over networks. If we assume the same structure on DSGT or DSGD, what is the convergence rate?</p></li>
</ul>
</div>
<div id="convergence-rate" class="section level2">
<h2><span class="header-section-number">10.2</span> Convergence rate</h2>
<p>The following table lists the convergence rate of the above algorithms.</p>
<table>
<colgroup>
<col width="3%" />
<col width="34%" />
<col width="27%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Name</th>
<th align="center">nonconvex</th>
<th align="center">convex</th>
<th align="center">strongly convex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">c-SGD</td>
<td align="center"><span class="math inline">\(\mathcal{O}(\frac{1}{\sqrt{nT}}+\frac{1}{T})\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\mathcal{O}(\frac{1}{nk}+\frac{1}{k^2})\)</span></td>
</tr>
<tr class="even">
<td align="center">DGD</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\mathcal{O}\left(\frac{\ln k}{\sqrt{k}}\right)\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">D-PSGD</td>
<td align="center"><span class="math inline">\(\mathcal{O}\left(\frac{\sigma}{\sqrt{n T}}+\frac{n^{\frac{1}{4}} \zeta^{\frac{2}{3}}}{T^{\frac{2}{3}}}+\frac{1}{T}\right)\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">EXTRA</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\mathcal{O}(\frac{1}{k})\)</span></td>
<td align="center"><span class="math inline">\(\mathcal{O}((1+\delta)^{-k})\)</span></td>
</tr>
<tr class="odd">
<td align="center">DSGD</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\mathcal{O}\left(\frac{1}{nk}+\frac{1}{k^{1.5}}+\frac{1}{k^2}\right)\)</span></td>
</tr>
<tr class="even">
<td align="center">DSGT</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\mathcal{O}\left(\frac{1}{nk} +\frac{1}{k^{\theta\mu}}+\frac{1}{k^2} \right)\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(D^2\)</span></td>
<td align="center"><span class="math inline">\(\mathcal{O}\left(\frac{\sigma}{\sqrt{n T}}+\frac{1}{T}+\frac{\zeta_{0}^{2}}{T+\sigma^{2} T^{2}}+\frac{\sigma^2}{1+\sigma^2T}\right)\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<ul>
<li>DSGT converges to the neighborhood of <span class="math inline">\(x^*\)</span> at the linear rate of <span class="math inline">\(\rho(A)^k\)</span> when choosing fixed <span class="math inline">\(\alpha\)</span> and converges to <span class="math inline">\(x^*\)</span> when choosing <span class="math inline">\(\alpha_k=\frac{\theta}{m+k}\)</span>. This implies us to choose stepsize adaptively.</li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-li2019decentralized">
<p>Li, Zhi, Wei Shi, and Ming Yan. 2019. “A Decentralized Proximal-Gradient Method with Network Independent Step-Sizes and Separated Convergence Rates.” <em>IEEE Transactions on Signal Processing</em> 67 (17). IEEE: 4494–4506.</p>
</div>
<div id="ref-lian2017can">
<p>Lian, Xiangru, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. 2017. “Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent.” In <em>Advances in Neural Information Processing Systems</em>, 5330–40.</p>
</div>
<div id="ref-pu2018distributed">
<p>Pu, Shi, and Angelia Nedić. 2018. “Distributed Stochastic Gradient Tracking Methods.”</p>
</div>
<div id="ref-pu2019sharp">
<p>Pu, Shi, Alex Olshevsky, and Ioannis Ch. Paschalidis. 2019a. “A Sharp Estimate on the Transient Time of Distributed Stochastic Gradient Descent.”</p>
</div>
<div id="ref-shi2015extra">
<p>Shi, Wei, Qing Ling, Gang Wu, and Wotao Yin. 2015. “Extra: An Exact First-Order Algorithm for Decentralized Consensus Optimization.” <em>SIAM Journal on Optimization</em> 25 (2). SIAM: 944–66.</p>
</div>
<div id="ref-tang2018d">
<p>Tang, Hanlin, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. 2018. “D <span class="math inline">\(\^{} 2\)</span>: Decentralized Training over Decentralized Data.” <em>arXiv Preprint arXiv:1803.07068</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-sharp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-ediff.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["DistributedOpt.pdf", "DistributedOpt.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
