# Exact diffusion{#sec:ediff}

Consider the following problem, 
\begin{equation}
x^*=\underset{x\in\mathbb{R}^p}{\arg\min}\sum_{i=1}^nf_i(x)\left(=E_{\xi_i}Q(x,\xi_i)\right)
(\#eq:eloss)
\end{equation}
The noisy gradient $\nabla Q(x_{i,k},\xi_{i,k})$ is more concrete than the $g(x_{i,k},\xi_{i,k})$ we use in the previous chapters, like chapter \@ref(sec:dsgt). In the following, we use $g(x_{i,k},\xi_{i,k})=\nabla Q(x_{i,k},\xi_{i,k})$.

We focus on the class of diffusion strategies here. The exact diffusion and the traditional diffusion strategy(take adapt-then-conbine formulartion of diffusion as an example) can be seen in the following table, 

|                Name                 |                            Scheme                            |
| :---------------------------------: | :----------------------------------------------------------: |
|   adapt-then-combine of diffusion   | $$X_{k+\frac{1}{2}}=X_{k}-\alpha\nabla G(X_k,\boldsymbol\xi_k)(\text{adaptation})\\X_{k+1}=WX_{k+\frac{1}{2}}(\text{combination})$$ |
| exact diffusion(stochastic version) | $$X_{k+\frac{1}{3}}=X_{k}-\alpha G(X_k,\boldsymbol\xi_k)(\text{adaptation})\\X_{k+\frac{2}{3}}=X_{k+\frac{1}{3}}+X_k-X_{k-1+\frac{1}{3}}(\text{correction})\\X_{k+1}=WX_{k+\frac{2}{3}}(\text{combination})$$ |


The performance of bias-correction methods under stochastic and adaptive settings remain unclear. @yuan2019performance show that the correction-step in exact diffusion leads to better standy-state performance under stochastic scenarios. The exact diffusion assume each $f_i$ to be **differentiable and $\mu-$strongly convex**. Under sufficiently small step sizes $\alpha$, the exact diffusion converges exponentially at the rate $1-\mathcal{O}(\alpha\mu)$ to a neiborhood of $x^*$, which can be characterized as 
\begin{equation}
\underset{k\to \infty}{\lim\sup}\frac{1}{n}E\left[\Vert X_k-\mathbf{1}x^*\Vert^2\right]_{ed}
=\mathcal{O}\left(\frac{\alpha\bar\sigma^2}{n\mu}+\frac{\delta^2}{n\mu^2}\cdot\frac{\alpha^2\bar\sigma^2}{1-\lambda}\right)
(\#eq:ned)
\end{equation}

In comparison, the traditional diffusion strategy converges at the similar rate to the following neighborhood of $x^*$,
\begin{equation}
\underset{k\to \infty}{\lim\sup}\frac{1}{n}E\left[\Vert X_k-\mathbf{1}x^*\Vert^2\right]_{d}
=\mathcal{O}\left(\frac{\alpha\bar\sigma^2}{n\mu}+\frac{\delta^2}{n\mu^2}\cdot(\frac{\alpha^2\bar\sigma^2}{1-\lambda}+
\frac{\alpha^2b^2}{(1-\lambda)^2})\right)
(\#eq:nd)
\end{equation}

where $\bar\sigma^2$  measures the size of gradient noise,$\delta$ bounds the hessian matrix of $f_i$, $b^2=\sum\limits_{i=1}^n\Vert \nabla f_i(x^*)\Vert^2$, and $\lambda=\max\{|\lambda_2(W)|,|\lambda_n(W)|\}\in(0,1)$ stands for the spectral gap.

```{exercise, edfi}
Each $f_i$ is $\mu-$strongly convex and twice differentiable, and 
$$
\mu I_p\leq\nabla^2 f_i(x)\leq\delta I_p
$$

```

We have a more smooth function $f_i$ here compared to assuming $\nabla f_i$ is $L-$Lipschitz continuous.

```{exercise, edw}
The network is undirected and stronly connected, and the mixing matrix $W$ is symmetric and doubly-stochastic
```

For the noisy gradient, we assume differently compared to assumption \@ref(exr:estg), where awe assume the variance of the noisy gradient can be bound be a uniform $\sigma^2$.

```{exercise, edg}
Each agent $i$ and the iteration step $k$, we have, 
\begin{align}
E\left[\nabla g(x_{i,k},\xi_{i,k})|\mathcal{F}_{k-1}\right]&=\nabla f_i(x_{i,k})\\
E\left[\Vert \nabla g(x_{i,k},\xi_{i,k})-\nabla f_i(x_{i,k})\Vert^2|\mathcal{F}_{k-1}\right]&\leq \beta_i\Vert x_{i,k-1}-x^*\Vert^2 + \sigma^2_i
\end{align}
$\bar\sigma^2=\frac{1}{n}\sum_{i=1}^n\sigma_i^2$
```

Compare the two characterization \@ref(eq:ned) and \@ref(eq:nd), we see that the exact diffusion outperforms the traditional diffusion strategy in the following, 

- The exact diffusion removes the bias $\frac{\alpha^2b^2}{(1-\lambda^2)}$

- When we can obtain the exact gradient(deterministic setting), i.e. $\sigma_i^2=0,i=1,2,...,n$, the exact diffusion converge exactly to $x^*$.

- When the bias $\frac{\alpha^2b^2}{(1-\lambda^2)}$ is significant, i.e. $b^2$ is large or the network is badly-connected(i.e. $\lambda$ is close to 1), and the step size is not sufficiently small($\alpha\leq d_3(1-\lambda)^{(2+y)},y>0$), the exact diffusion outperforms the traditional one. 

## Decreasing stepsize

Here we can derive the information of Hessian matrices of $f_i,i=1,2,...,n$, then let $\tilde x_{i,k}=x^*-x_{i,k}$ we have,
\begin{equation}
\nabla f_i(x_{i,k})-\nabla f_i(x^*)=-H_{i,k}\tilde x_{i,k}
\end{equation}
where 
\begin{equation}
H_{i,k}=\int_{0}^1\nabla^2 f_i(x^*-r\tilde x_{i,k})dr\in\mathbb{R}^{p\times p}
\end{equation}
then we can denote 
\begin{equation}
\nabla F(X_k)-\nabla F(\mathbf{1}x^*):= 
-\left(
\begin{array}{c}
(H_{1,k}\tilde x_{1,k})^T\\
\vdots)\\
(H_{n,k}\tilde x_{n,k})^T
\end{array}\right)
\in\mathbb{R}^{n,p}
\end{equation}

However, this notation cannot have the form of $\mathcal{H}_k\tilde X_k$, which helps us to separate the Hessian matrices and the iterated values. To ensure such an advantage, we stack the local vairables of different agents as a long vector, i.e., 
\begin{equation}
\mathcal{X}_k = 
\left(
\begin{array}{c}
x_{1,k}\\
x_{2,k}\\
\vdots\\
x_{n,k}
\end{array}\right):=\mathrm{col}\{x_{1,k},...,x_{n,k}\}\in\mathbb{R}^{np}
\end{equation}

Similarly, we denote $\mathcal{G}_k:=\mathrm{col}\{g_1(x_{1,k},\xi_{1,k}),...,g_n(x_{n,k},\xi_{n,k})\}$. Then the exact diffusion with decreasing stepsize $\alpha_k$ can be rewritten globally as the following, 
\begin{align}
\mathcal{X}_{k+1} &= \overline{\mathcal{W}} (2 \mathcal{X}_k-\mathcal{X}_{k-1}-\alpha_k\mathcal{G}_k+\alpha_{k-1}\mathcal{G}_{k-1})\\
\mathcal{X}_1&=\overline{\mathcal{W}} (\mathcal{X}_0-\alpha_0\mathcal{G}_0),\mathcal{X}_0=\mathbf{0}
(\#eq:extraak)
\end{align}
where $\overline{\mathcal{W}}=(\mathcal{W}+I_{np})/2$, $\mathcal{W}=W\otimes I_{p}\in\mathbb{R}^{np\times np}$. Essentially, exact diffusion is a primal-dual method. Since $W$ is symmetric and doubly-stochastic, then so does $\overline W$, thus $I_n-\overline W$ is positive semi-definite(p.s.d.) and symmetric, we have spectral decomposition 
$I_n-\overline W = U\Sigma U^T:=V^2$. Let $\mathcal{V}=V\otimes I_p$, then $\mathcal{V}^2=I_{np}-\overline{\mathcal{W}}$, we have the primal-dual version of exact diffusion \@ref(eq:pdextra).
\begin{equation}
\begin{cases}
\mathcal{X}_{k+1}=\overline{\mathcal{W}}\left(\mathcal{X}_k-\alpha_k\mathcal{G}_k\right)-\mathcal{V}\mathcal{Y}_k\\
\mathcal{Y}_{k+1}=\mathcal{Y}_k+\mathcal{V}\mathcal{X}_{k+1}
\end{cases}
(\#eq:pdextra)
\end{equation}
This is because,
\begin{align}
\mathcal{X}_{k+1}-\mathcal{X}_{k}&=\overline{\mathcal{W}}\left(\mathcal{X}_{k}-\mathcal{X}_{k-1}-\alpha_k\mathcal{G}_k
+\alpha_{k-1}\mathcal{G}_{k-1}\right)-\mathcal{V}\mathcal{Y}_k+\mathcal{V}\mathcal{Y}_{k-1}\\
&=\overline{\mathcal{W}}\left(\mathcal{X}_{k}-\mathcal{X}_{k-1}-\alpha_k\mathcal{G}_k
+\alpha_{k-1}\mathcal{G}_{k-1}\right)-\mathcal{V}(\mathcal{Y}_{k-1}+\mathcal{V}\mathcal{X}_k)+\mathcal{V}\mathcal{Y}_{k-1}\\
&=\overline{\mathcal{W}} (2 \mathcal{X}_k-\mathcal{X}_{k-1}-\alpha_k\mathcal{G}_k+\alpha_{k-1}\mathcal{G}_{k-1})-\mathcal{X}_{k}
\end{align}
We can also have $\mathcal{Y}_k=\mathcal{Y}_0+\mathcal{V}\sum\limits_{i=1}^k \mathcal{X}_i$.

