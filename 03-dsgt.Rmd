# Distributed Stochastic Gradient Tracking(DSGT) Method {#dsgt}

## Introduction

The idea is similar to that in the Push-Pull method. However, since $Y_k\in\mathbb{R}^{n\times p}$ is used to track the average stochastic gradients in the $k$th iteration, i.e. $\bar y_k = \frac{1}{n}\mathbf{1}^T Y_k=\frac{1}{n}\sum\limits_{i=1}^ng_i(x_{i,k},\xi_{i,k})$ provided $y_{i,0}=g(x_{i,0},\xi_{i,0})$, which is random. Hence we now bound $E\left[\left\|\bar{x}_{k+1}-x^{*}\right\|^{2}\right]$,
$E\left[\left\|X_{k+1}-1 \bar{x}_{k+1}\right\|^{2}\right]$, and $E\left[\left\|Y_{k+1}-\mathbf{1} \bar{y}_{k+1}\right\|^{2}\right]$, which can be seen as variances of $\bar x_k, X_k$, and $Y_k$. Thus we need to assume $g_i(x,\xi_i),i\in\mathcal{N}$ have the finite variances and also assume they are good estimates of $\nabla f_i(x), i\in\mathcal{N}$. 

Insead of the definition \@ref(def:normpp) used in the Push-Pull method, we denote $\Vert\cdot\Vert$ as the $\ell_2-$norm for vectors and as Frobenius norms for matrices. Hence they are consistent. 


## Analysis of Convergence

```{exercise, estg}

$\forall i\in\mathcal{N},x\in\mathbb{R}^p$, eahc random vector $\xi_i\in\mathbb{R}^m$ is independent, and 
\begin{align}
E[g_i(x,\xi_i)|x]&=\nabla f_i(x)\\
E[\Vert g_i(x,\xi_i)-\nabla f_i(x)\Vert^2 |x]&\leq \sigma, \exists \sigma
(\#eq:auef)
\end{align}
```


Denote $\mathcal{F}_k$ as the $\sigma-$algebra generated by $\{\xi_0,...,\xi_{k-1}\}$. We first reveal some properties of the introduced auxiliary variable $\bar y_k=\frac{1}{n}g_i(x_{i,k},\xi_{i,k})$ provided $y_{i,0}=g(x_{i,0},\xi_{i,0})$. 

```{lemma, lem2}
Under Assumption \@ref(eq:auef), $h(X)=\frac{1}{n}\mathbf{1}^T\nabla F(X), X\in\mathbb{R}^{n\times p},\forall k\geq0$, we have 
\begin{align}
E\left[ \bar y_k - h(X_k)|\mathcal{F}_k\right]&=0\\
E\left[\left\|\bar{y}_{k}-h(X_k)\right\|^{2} | \mathcal{F}_{k}\right] &\leq \frac{\sigma^{2}}{n}

\end{align}
```

### Relationship between two iteration steps

From \@ref(eq:xbar2pp), we make $\alpha'=\frac{1}{n}\mathbf{1}^T\boldsymbol\alpha\mathbf{1}=\alpha,\boldsymbol\alpha=\text{diag}(\alpha,\alpha,...,\alpha)$ and $g_k=\nabla f(\bar x_k)=\frac{1}{n}\sum\limits_{i=1}^n\nabla f_i(\bar x_k)=\frac{1}{n}\mathbf{1}^T\nabla F(\mathbf{1}\bar x_k)$, we have, 

\begin{equation}
\bar x_{k+1}-x^* = \bar x_k - \alpha(h(X_k)-\nabla f(\bar x_k))-\alpha \nabla f(\bar x_k)-\alpha (\bar y_k-h(X_k))-x^*
(\#eq:xbar1dsgt)
\end{equation}

Take norm at both sides, we have 

\begin{align}
\Vert \bar x_{k+1}-x^*\Vert^2&=\Vert (\bar x_k-\alpha \nabla f(\bar x_k)-x^*)-\alpha(h(X_k)-\nabla f(\bar x_k))-\alpha(\bar y_k-h(X_k))\Vert^2\\
&=\Vert \bar x_k-\alpha \nabla f(\bar x_k)-x^*\Vert^2 + \alpha^2\Vert h(X_k)-\nabla f(\bar x_k)\Vert ^2 + \alpha^2\Vert \bar y_k-h(X_k)\Vert^2\\ &+ 2\alpha\left\langle\bar x_k-\alpha \nabla f(\bar x_k)-x^*, \nabla f(\bar x_k)-h(X_k)\right\rangle\\&-2\alpha\left\langle\bar x_k-\alpha \nabla f(\bar x_k)-x^*, \bar y_k-h(X_k)\right\rangle\\ &+ 2\alpha^2\left\langle h(X_k)-\nabla f(\bar x_k), \bar y_k-h(X_k)\right\rangle
(\#eq:xbar1normdsgt)
\end{align}
Where $\left\langle\cdot\right\rangle$ denotes the Frobinus inner product. At both sides, take conditional expectation given $\mathcal{F}_k$, use lemma \@ref(lem:lem31) and lemma \@ref(lem:lem2), we have 
\begin{align}
E\left[\Vert \bar x_{k+1}-x^*\Vert^2|\mathcal{F}_k\right]&\leq 
(1-\alpha\mu)^2\Vert \bar x_k - x^*\Vert^2 + \alpha^2\frac{L^2}{n}\Vert X_k-\mathbf{1}\bar x_k\Vert^2+\alpha^2\frac{\sigma^2}{n}\\&+ 2\alpha (1-\alpha\mu)\Vert \bar x_k-x^*\Vert\cdot\frac{L}{\sqrt{n}}\Vert X_k-\mathbf{1}\bar x_k\Vert\\
&\leq (1-\alpha\mu)^2\Vert \bar x_k - x^*\Vert^2 + \alpha^2\frac{L^2}{n}\Vert X_k-\mathbf{1}\bar x_k\Vert^2+\alpha^2\frac{\sigma^2}{n}\\
&+ \alpha\left((1-\alpha\mu)^{2} \mu\left\|\bar{x}_{k}-x^{*}\right\|^{2}+\frac{L^{2}}{\mu n}\left\|X_{k}-1 \bar{x}_{k}\right\|^{2}\right)\\
&=(1-\alpha\mu)(1-(\alpha\mu)^2)\Vert \bar x_k-x^*\Vert^2+\frac{\alpha L^2}{\mu n}(1+\alpha\mu)\Vert X_k-\mathbf{1}\bar x_k\Vert^2 + \frac{\alpha^2\sigma^2}{n}\\
&\leq 
(1-\alpha\mu)\Vert \bar x_k-x^*\Vert^2+\frac{\alpha L^2}{\mu n}(1+\alpha\mu)\Vert X_k-\mathbf{1}\bar x_k\Vert^2 + \frac{\alpha^2\sigma^2}{n}
(\#eq:ineq1dsgt)
\end{align}


```{remark}
\\

- When each agent $i$ takes different step size $\alpha_i,i\in\mathcal{N}$, $\alpha(\bar y_k-h(\bar x_k))$ in \@ref(eq:xbar1dsgt) becomes $\frac{1}{n}\mathbf{1}^T\boldsymbol\alpha(Y_k-\mathbf{1}h(\bar x_k))$, then we may use the following to continuou the steps in \@ref(eq:ineq1dsgt).  
\begin{align}
\frac{1}{n}\mathbf{1}^T\boldsymbol\alpha(Y_k-\mathbf{1}h(\bar x_k))&=\frac{1}{n}\mathbf{1}^T\boldsymbol\alpha\left[(Y_k-\mathbf{1}\bar y_k)+\mathbf{1}(\bar y_k-h(\bar x_k))\right]
\end{align}

- $0<(1-(\alpha\mu)^2)<1$ will be guranteed by lemma \@ref(lem:lem37). This also hints us how to separate $\frac{2 \alpha (1-\alpha\mu) L}{\sqrt{n}}\left\|\bar{x}_{k}-x^{*}\right\|\left\|X_{k}-1 \bar{x}_{k}\right\|$

```

In the Push-Pull method, we see matrices $R-\frac{\mathbf{1}u^T}{n}$ and $C-\frac{v\mathbf{1}^T}{n}$. Here for $W\in\mathbb{R}^{n\times n}$, $W-\frac{\mathbf{1}\mathbf{1}^T}{n}$ also has significant uses. $\mathbf{1}$ can be seen as left and right eigenvalue of $W$, and from $W\mathbf{1}=\mathbf{1}$ we have 

```{lemma, lem1}
Given the graph $\mathcal{G}$ corresponding to the network of agents is undirected and connected, and $W$ is doubly stochastic and $w_{ii}>0,\exists i\in\mathcal{N}$, we have the spectral norm $\rho_W$ of $W-\frac{\mathbf{1}\mathbf{1}^T}{n}$, $\rho_W<1$, and 
$$
  \|W \omega-\mathbf{1} \bar{\omega}\| \leq \rho_{w}\|\omega-1 \bar{\omega}\|, \forall \omega\in\mathbb{R}^{n\times p}, \bar\omega = \frac{1}{n}\mathbf{1}\omega
$$
```



Then 
\begin{align}

\Vert X_{k+1}-\mathbf{1}\bar x_{k+1}\Vert^2&\stackrel{\text{iterate}}{=}
\Vert WX_k-\alpha WY_k-\mathbf{1}(\bar x_k-\alpha\bar y_k)\Vert^2\\
&=\left\|W X_{k}-1 \bar{x}_{k}\right\|^{2}-2 \alpha\left\langle W X_{k}-1 \bar{x}_{k}, W Y_{k}-1 \bar{y}_{k}\right\rangle+\alpha^{2}\left\|W Y_{k}-1 \bar{y}_{k}\right\|^{2}\\
&\leq \rho_{w}^{2}\left\|\mathrm{x}_{k}-1 \bar{x}_{k}\right\|^{2}+\alpha \rho_{w}^{2}\left[\frac{\left(1-\rho_{w}^{2}\right)}{2 \alpha \rho_{w}^{2}}\left\|\mathrm{x}_{k}-1 \bar{x}_{k}\right\|^{2}+\frac{2 \alpha \rho_{w}^{2}}{\left(1-\rho_{w}^{2}\right)}\left\|\mathrm{y}_{k}-1 \bar{y}_{k}\right\|^{2}\right]+\alpha^{2} \rho_{w}^{2}\left\|\mathrm{y}_{k}-1 \bar{y}_{k}\right\|^{2}\\
&=
\frac{\left(1+\rho_{w}^{2}\right)}{2}\left\|X_{k}-1 \bar{x}_{k}\right\|^{2}+\alpha^{2} \frac{\left(1+\rho_{w}^{2}\right) \rho_{w}^{2}}{\left(1-\rho_{w}^{2}\right)}\left\|Y_{k}-1 \bar{y}_{k}\right\|^{2}
(\#eq:ineq2dsgt)
\end{align}

```{remark}
For lemma \@ref(lem:lem1), a counter example for not assuming assumption connected graph is that graph $\mathcal{G}$ induced by the identity matrix $I$, then the spectral norm of $I-\frac{\mathbf{1}\mathbf{1}^T}{n}$ is $\rho_w = 1$. 

```

For $E\left[\Vert Y_{k+1}-\mathbf{1}\bar y_{k+1}\Vert^2|\mathcal{F}_k\right]$. We write $G(X_k,\xi_k):=G_k,\nabla_k:=\nabla F(X_k)$ for simplicity, then we have,

\begin{align}
\Vert Y_{k+1}-\mathbf{1}\bar y_{k+1}\Vert^2&=

\Vert WY_k + G_{k+1} - G_k -\mathbf{1}\bar y_k + \mathbf{1}(\bar y_k-\bar y_{k+1})\Vert^2\\
&= \Vert WY_k -  \mathbf{1}\bar y_k\Vert^2 + 
\Vert G_{k+1} - G_k\Vert^2 + \Vert \mathbf{1}(\bar y_k-\bar y_{k+1})\Vert^2 + 2\langle WY_k - \mathbf{1}\bar y_k, G_{k+1}-G_k\rangle\\
&+ 2\langle WY_k-\mathbf{1}\bar y_k + G_{k+1}-G_k, \mathbf{1}(\bar y_k-\bar y_{k+1})\rangle \\
&=\Vert WY_k -  \mathbf{1}\bar y_k\Vert^2 + 
\Vert G_{k+1} - G_k\Vert^2 + \Vert \mathbf{1}(\bar y_k-\bar y_{k+1})\Vert^2 + 2\langle WY_k - \mathbf{1}\bar y_k, G_{k+1}-G_k\rangle\\
&+ 2\langle Y_{k+1}-\mathbf{1}\bar y_{k+1}-\mathbf{1}(\bar y_k-\bar y_{k+1}) , \mathbf{1}(\bar y_k-\bar y_{k+1})\rangle\\
&\stackrel{?}{=}
\Vert WY_k -  \mathbf{1}\bar y_k\Vert^2 + 
\Vert G_{k+1} - G_k\Vert^2 - n\Vert \bar y_k-\bar y_{k+1}\Vert^2 + 2\langle WY_k - \mathbf{1}\bar y_k, G_{k+1}-G_k\rangle \\
&\leq \rho^2_w\Vert Y_k-\mathbf{1}\bar y_k\Vert^2+\Vert G_{k+1} - G_k\Vert^2 + 2\langle WY_k - \mathbf{1}\bar y_k, G_{k+1}-G_k\rangle
(\#eq:dsgt31)
\end{align}

__Todo: $\Vert Y_{k+1}-\mathbf{1}\bar y_{k+1}\Vert^2$ __


### Linear system of inequalities

```{theorem, dsgtA}
\begin{equation}
A_{dsgt}=\left[\begin{array}{ccc}
{1-\alpha \mu} & {\frac{\alpha L^{2}}{\mu n}(1+\alpha \mu)} & {0} \\
{0} & {\frac{1}{2}\left(1+\rho_{w}^{2}\right)} & {\alpha^{2} \frac{\left(1+\rho_{w}^{2}\right) \rho_{w}^{2}}{\left(1-\rho_{w}^{2}\right)}} \\
2 \alpha n L^{3} & \left(\frac{1}{\beta}+2\right) {\|W-I\|^{2} L^{2}+3 \alpha L^{3}} & {\left(1+4 \alpha L+2 \alpha^{2} L^{2}+\beta\right) \rho_{w}^{2}}
\end{array}\right]
\end{equation}
where $\beta=\frac{1-\rho_{w}^{2}}{2 \rho_{w}^{2}}-4 \alpha L-2 \alpha^{2} L^{2}$
```

After getting the element of the matrix $A$ of the linear system of inequalities, we again use lemma \@ref(lem:lem37) to build conditions on step size $\alpha$ so that the spectral radius of $A$, $\rho(A)<1$.

### Spectral radius of A

Next, we derive the conditions on $\alpha$ so that $\rho(A_{dsgt})<1$ by computing $det(I-A_{dsgt})$ and make it greater than 0. We expand $det(I-A_{dsgt})$ according to the first column, 

\begin{align}
det(I-A_{dsgt}) &= (1-a_{11})[(1-a_{22})(1-a_{33})-a_{32}a_{23}]-a_{12}a_{23}a_{31}
(\#eq:detdsgt)
\end{align}

Notice that in lemma \@ref(lem:lem37), it requires $\lambda-a_{ii}>0, i=1,2,3$ and in \@ref(eq:detdsgt), we already have  $(1-a_{11})(1-a_{22})(1-a_{33})-C$. Hence we may expect $C$ to be bounded by the term of $c_0(1-a_{11})(1-a_{22})(1-a_{33})$, where $c_0<1$ is a positive number. So when we make 

\begin{align}
a_{23}a_{32}&\leq \frac{1}{\Gamma}(1-a_{22})(1-a_{33})=\frac{1}{\Gamma}(\frac{1-\rho_w^2}{2})^2,\\
(\#eq:gamma1)
\end{align}

\begin{align}
a_{12}a_{23}a_{31}&\leq \frac{1}{\Gamma+1}(1-a_{11})[(1-a_{22})(1-a_{33})-a_{32}a_{23}]
(\#eq:gamma2)
\end{align}

we have 

\begin{equation}
det(I-A_{dsgt}) \geq \frac{\Gamma-1}{\Gamma+1}(1-a_{11})(1-a_{22})(1-a_{33})>0
\end{equation}

Next, we derive what exactly conditions $\alpha$ should satisfy to make the inequalities \@ref(eq:gamma1) and \@ref(eq:gamma2) hold. Additionally, in the proof of building linear inequality system of $E\left[\Vert Y_{k+1}-\mathbf{1}\bar y_{k+1}\Vert^2|\mathcal{F}_k\right]$, the author uses
\begin{align}
2\|\mathbf{W}-\mathbf{I}\| L \rho_{w}\left\|\mathbf{y}_{k}-\mathbf{1} \bar{y}_{k}\right\|\left\|\mathbf{x}_{k}-1 \bar{x}_{k}\right\|\leq
\beta \rho_{w}^{2} \left\|\mathbf{y}_{k}-1 \bar{y}_{k}\right\|^{2} | \mathcal{F}_{k}+\frac{1}{\beta}\|\mathbf{W}-\mathbf{I}\|^{2} L^{2}\left\|\mathbf{x}_{k}-1 \bar{x}_{k}\right\|^{2}
\end{align}
Thus we also need $\beta>0$. Since$\beta$ is the quadratic about $\alpha>0$, so $\beta>0$ when 
\begin{equation}
\alpha < \frac{\sqrt{1+3\rho_w^2}}{2\rho_wL}-\frac{1}{L}
(\#eq:dsgtalpha1)
\end{equation}


The author uses $\alpha\leq \frac{1-\rho_w^2}{12\rho_w^2L}$ to gurantee $\beta>0$ since $0<\rho_w<1$. 
\begin{align}
\beta \geq \frac{1-\rho_{w}^{2}}{2 \rho_{w}^{2}}-\frac{1-\rho_{w}^{2}}{3 \rho_{w}}-\frac{\left(1-\rho_{w}^{2}\right)^{2}}{72 \rho_{w}^{2}} \geq \frac{11\left(1-\rho_{w}^{2}\right)}{72 \rho_{w}^{2}} \geq \frac{1-\rho_{w}^{2}}{8 \rho_{w}^{2}}>0
(\#eq:dsgtbeta)
\end{align}

The coefficient $C>0$ of $\frac{1-\rho_w^2}{C\rho_w^2L}$ can be chosen from $(\frac{2}{\sqrt{5}-2}\approx8.47,+\infty)$.

Figure \@ref(fig:f1) plots the function $\frac{\sqrt{1+3\rho_w^2}}{2\rho_w}-1-\frac{1-\rho_w^2}{12\rho_w^2}$, we see from figure \ref(fig:f1) that the constraint in \@ref(eq:dsgtalpha1) is not better than $\alpha\leq \frac{1-\rho_w^2}{12\rho_w^2L}$, especilly when $\rho_w$ is close to $0$.

```{r,f1, echo = FALSE, fig.cap="The difference between two constraints", out.width="50%"}
f1 = function(x) {sqrt(1+3*x^2)/(2*x)-(1-x^2)/(12*x^2) - 1}
curve(f1,0,0.2)
abline(h = 0, col = "red")
```

In short, a less strict $\alpha$ can be chosen as $\alpha\leq\frac{1-\rho_w^2}{9\rho_wL}$.

For inequality \@ref(eq:gamma1), substitute $\beta\geq\frac{1-\rho_w^2}{C'\rho_w^2}$, where $C'$ is subject to the chioce of $C\in(\frac{2}{\sqrt{5}-2}\approx8.47,+\infty)$. We follow the author's choice here, then the LHS of \@ref(eq:gamma1) is less than the following, 
\begin{equation}
\frac{\left(1+\rho_{w}^{2}\right) \rho_{w}^{2}}{\left(1-\rho_{w}^{2}\right)}\alpha^{2}\left[\frac{\left(2+6 \rho_{w}^{2}\right)}{\left(1-\rho_{w}^{2}\right)}\|\mathbf{W}-\mathbf{I}\|^{2} L^{2}+\frac{\left(1-\rho_{w}^{2}\right)}{4 \rho_{w}^{2}} L^{2}\right]\leq RHS=\frac{\left(1-\rho_{w}^{2}\right)^{2}}{4 \Gamma}
(\#eq:dsgtalpha21)
\end{equation}
Hence, it suffice to make $\alpha$,
\begin{equation}

\alpha\leq \frac{\left(1-\rho_{w}^{2}\right)^{2}}{2 \sqrt{\Gamma} L \max \left(6 \rho_{w}\|\mathbf{W}-\mathbf{I}\|, 1-\rho_{w}^{2}\right)}\leq \frac{\left(1-\rho_{w}^{2}\right)^{2}}{L \sqrt{\Gamma\left(1+\rho_{w}^{2}\right)} \sqrt{4 \rho_{w}^{2}\left(2+6 \rho_{w}^{2}\right)\|\mathbf{W}-\mathbf{I}\|^{2}+\left(1-\rho_{w}^{2}\right)^{2}}}

\end{equation}
The latter inequality comes from $\rho_w^2<1<\frac{7}{6}$ and $a+b\leq 2\max(a,b)$.

For the inequality \@ref(eq:gamma2), from the inequality \@ref(eq:gamma1), it is sufficient to have 
\begin{equation}

a_{12} a_{23} a_{31}\leq
\frac{(\Gamma-1)}{\Gamma(\Gamma+1)}\left(1-a_{11}\right)\left(1-a_{22}\right)\left(1-a_{33}\right)
\end{equation}

Thus, 

\begin{equation}
\alpha \leq \frac{\left(1-\rho_{w}^{2}\right)}{3 \rho_{w}^{2 / 3} L}\left[\frac{\mu^{2}}{L^{2}} \frac{(\Gamma-1)}{\Gamma(\Gamma+1)}\right]^{1 / 3}
\end{equation}

Then, when the step size $\alpha$ is chosen such that 
\begin{equation}
\alpha \leq \min \left\{\frac{\left(1-\rho_{w}^{2}\right)}{12 \rho_{w} L}, \frac{\left(1-\rho_{w}^{2}\right)^{2}}{2 \sqrt{\Gamma} L \max \left\{6 \rho_{w}\|\mathbf{W}-\mathbf{I}\|, 1-\rho_{w}^{2}\right\}}, \frac{\left(1-\rho_{w}^{2}\right)}{3 \rho_{w}^{2 / 3} L}\left[\frac{\mu^{2}}{L^{2}} \frac{(\Gamma-1)}{\Gamma(\Gamma+1)}\right]^{1 / 3}\right\}
(\#eq:dsgtalpha)
\end{equation}
we have $\rho(A_{dsgt})<1$.

```{remark}
\\
From the proof above, making $\beta = \frac{1-\rho_{w}^{2}}{2 \rho_{w}^{2}}-4 \alpha L-2 \alpha^{2} L^{2}$ is a little tricky. $(1-a_{22})=\frac{1-\rho_w^2}{2}$ can give us hints about such a choice.
```
