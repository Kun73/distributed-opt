# Asymptotic network independence

An undesirable property of distributed optimization method is that the increasing number of nodes may result in a large increase in the time to reach the same $\varepsilon$ accuracy(error $<\varepsilon$) under the centralized version. @pu2019asymptotic discuss this phenomenon under the following scenario

$$
\text { Time }_{n, \varepsilon} \text { (decentralized) } \leq p(\mathcal{G}) \text { Time }_{n, \varepsilon} \text { (centralized) }
$$


where $\text { Time }_{n, \varepsilon} \text { (decentralized) }$ denotes the time for the decentralized algorithm on n nodes
to reach $\varepsilon$ accuracy, and $\text { Time }_{n, \varepsilon} \text { (centralized) }$ is the time for the centralized algorithm which can query $n$ gradients per time step to reach $\varepsilon$ accuracy.$\mathcal{G}=(\mathcal{N},\mathcal{E})$ Typically, $p(\mathcal{G})$ is at least $\mathcal{O}(n^2)$^[smaller $p(\mathcal{G})$ is possible for some special cases], which is inpractical to use. 


$p(\mathcal{G})=\mathcal{O}(1)$ is a desirable setting, which means a decentralized algorithm converge to the optimal at a comparable rate to a centralized algorithm with the same computational power^[Computing Power: Two processors have the same computing power if they can run the same programs (after translation into each processor's machine language) and produce the same results]. Fortunately, it is possible for the iteration time $k$ to be large enough for some distributed stochastic optimization, which is the **asymptotic network independence** property: it is as if the network is not even there. 

## SGD and DSGD 

We consider a distributed stochastic gradient descent(DSGD) method, see algorithm  under assumptions \@ref(exr:muL), \@ref(exr:estg),\@ref(exr:dsgtgraph),and \@ref(exr:dsw) plus symmetric^[It seems $W$ in DSGT is also symmetric?], the similar settings as we discuss in chapter \@ref(sec:dsgt). By assumption \@ref(exr:muL), there exist a unique solution $x^*\in\mathbb{R}^p$ to the problem \@ref(eq:obj).



```{example, dsgd, name = "DSGD"}
Each agent $i$ choose the same step size $\alpha_k$ at the $k$th iteration and initilized with an arbitary $x_i(0)\in\mathbb{R}^p$

For k = 0, 1, ...,

  * For $i\in\mathcal{N}$, 
  
    * $x_i(k+1) = \sum\limits_{j=1}^nw_{ij}(x_j(k+1)-\alpha_k g_j(x_i(k),\xi_i(k)))$ 
  
```
$\{\alpha_k\}$ are a sequence of nonnegative non-increasing stepsizes. In the long run, $x_{i,k}=x_{j,k},\forall i,j\in\mathcal{N}$, i.e. DSGD belongs to the class of consensus-based distributed optimization methods, which can be achieved under assumptions \@ref(exr:dsgtgraph) and \@ref(exr:dsw) plus symmetric.

__Todo:$x_{i,k}\stackrel{?}=x_{j,k},\forall i,j\in\mathcal{N}$__ 

We compare DSGD with centralized stochastic gradient descent(SGD) which can query $n$ gradients at each iteration, 

```{example, sgd, name = "SGD"}
Initialize arbitrary $x_{0}\in\mathbb{R}^{p}$ and choose stepsize $\alpha_k$ for each step

For k=0,1,...,

  * $x(k+1)=x(k)-\alpha_k\bar g(k)$
  
```

where $\bar g(k)=\frac{1}{n}\sum\limits_{i=1}^n g_i(x(k),\xi_i(k))$, which is to make the gradient comparable to that in DSGD, i.e., $\sum\limits_{j=1}^n w_{ij} g_j(x_j(k),\xi_j(k))$. 

Choose $2-$norm as the loss function, the risk at the $k$th step is 
\begin{equation}
R(k)=E  \left[\Vert x(k)-x^*\Vert^2\right]=\frac{1}{n}\sum\limits_{i=1}^nE  \left[\Vert x(k)-x^*\Vert^2\right]
(\#eq:risksgd)
\end{equation} 
which hints us to evaluate the performance of DSGD by 
\begin{equation}
R'(k)=\frac{1}{n}\sum\limits_{i=1}^n E\left[\Vert x_i(k)-x^*\Vert^2\right]
(\#eq:riskdsgd)
\end{equation}

Additonnally, \@ref(eq:riskdsgd) can be divided into two sources, one from the optimization error, and one from the consensus error, i.e.,
\begin{equation}
R'(K)=\underbrace{E  \left[\Vert \bar x(k)-x^*\Vert^2\right]}_{\text{expected optimization error}} + 
\underbrace{\frac{1}{n}\sum_{i=1}^nE  \left[\Vert  x_i(k)-\bar x(k)^*\Vert^2\right]}_{\text{expected consensus error}}
(\#eq:drdsgd)
\end{equation}

## Bounds

We next compare SGD and DSGD by analyzing their error bounds.

```{lemma, risksgd}
Let assumptions \@ref(exr:muL), \@ref(exr:estg), \@ref(exr:dsgtgraph),and \@ref(exr:dsw) plus $W$ is symmetric hold, for SGD \@ref(exm:sgd), we have
$$
R(k+1) \leq\left(1-\alpha_{k} \mu\right)^{2} R(k)+\frac{\alpha_{k}^{2} \sigma^{2}}{n}
$$
```

Denote $U(k)=E  \left[\Vert \bar x(k)-x^*\Vert^2\right]$ and $V(k)=\sum\limits_{i=1}^nE  \left[\Vert x_i(k)-\bar x(k)\right]$, we have 

```{lemma, riskdsgd}
Let the same assumptions in \@ref(lem:risksgd) hold, 
$$
U(k+1) \leq\left(1-\frac{1}{k}\right)^{2} U(k)+\frac{2 L}{\sqrt{n} \mu} \frac{\sqrt{U(k) V(k)}}{k}+\frac{L^{2}}{n \mu^{2}} \frac{V(k)}{k^{2}}+\frac{\sigma^{2}}{n \mu^{2}} \frac{1}{k^{2}}
$$
```

__Todo:$R'(k)\stackrel{?}\leq \frac{\sigma^2}{n\mu^2 k}+\mathcal{O}(f(k))+h(U(k),V(k))$__

```{remark}
\\

- In a view that $R(k)$ and $R'(k)$ are both risk functions, if $V(k)$ decays fast enough compared to $U(k)$, we then have $R(k)\approx R'(k)$ for large $k$.

- the asymptotic network independence phenomenon: after a transient, DSGD performs comparably to a centralized stochastic gradient descent method with the same computational power.

```


## Possible ways to achieve asymptotic network independece

- Considering nonconvex objective functions(distributed training of deep neural networks);

- Explore communication reduction techniques that do not sacrifice the asymptotic network independece property;

- Redcing the transient time;

Additionnally, an unsolving question is can distributed methods compete with the centralized ones when the exact gradient is available?

__Todo: related reference__