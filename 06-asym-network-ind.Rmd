# Asymptotic network independence{#sec:asynt}

An undesirable property of distributed optimization method is that the increasing number of nodes may result in a large increase in the time to reach the same $\varepsilon$ accuracy(error $<\varepsilon$) under the centralized version. @pu2019asymptotic discuss this phenomenon under the following scenario

\begin{equation}
\text { Time }_{n, \varepsilon} \text { (decentralized) } \leq p(\mathcal{G}) \text { Time }_{n, \varepsilon} \text { (centralized) }
(\#eq:tscen)
\end{equation}


where $\text { Time }_{n, \varepsilon} \text { (decentralized) }$ denotes the time for the decentralized algorithm on n nodes
to reach $\varepsilon$ accuracy, and $\text { Time }_{n, \varepsilon} \text { (centralized) }$ is the time for the centralized algorithm which can query $n$ gradients per time step to reach $\varepsilon$ accuracy.$\mathcal{G}=(\mathcal{N},\mathcal{E})$ denotes the graph. Typically, $p(\mathcal{G})$ is at least $\mathcal{O}(n^2)$^[smaller $p(\mathcal{G})$ is possible for some special cases], which is inpractical to use. This is because, $p(\mathcal{G})=\mathcal{O}(n^2)$ implies the distributed version would be $n^2$ times slower than the centralized one with the same computational power.


$p(\mathcal{G})=\mathcal{O}(1)$ is a desirable setting, which means a decentralized algorithm converge to the optimal at a comparable rate to a centralized algorithm with the same computational power^[Computing Power: Two processors have the same computing power if they can run the same programs (after translation into each processor's machine language) and produce the same results]. Fortunately, it is possible for the iteration time $k$ to be large enough for some distributed stochastic optimization, which is the **asymptotic network independence** property: it is as if the network is not even there. In chapter \@ref(sec:sharp), we summarize some notes showing the asymptotic network independence property of algorithm \@ref(exm:dsgd) with $\alpha_k=\frac{\theta}{\mu(k+K)}$[@pu2019sharp].

## SGD and DSGD 

We consider a distributed stochastic gradient descent(DSGD) method, see algorithm  under assumptions \@ref(exr:muL), \@ref(exr:estg),\@ref(exr:dsgtgraph),and \@ref(exr:dsw) plus symmetric^[It seems $W$ in DSGT is also symmetric?], the similar settings as we discuss in chapter \@ref(sec:dsgt). By assumption \@ref(exr:muL), there exist a unique solution $x^*\in\mathbb{R}^p$ to the problem \@ref(eq:obj).



```{example, dsgd, name = "DSGD"}
Each agent $i$ choose the same step size $\alpha_k$ at the $k$th iteration and initilized with an arbitary $x_i(0)\in\mathbb{R}^p$

For k = 0, 1, ...,

  * For $i\in\mathcal{N}$, 
  
    * $x_i(k+1) = \sum\limits_{j=1}^nw_{ij}(x_j(k+1)-\alpha_k g_j(x_j(k),\xi_j(k)))$ 
  
```
$\{\alpha_k\}$ are a sequence of nonnegative non-increasing stepsizes. In the long run, $x_{i,k}=x_{j,k},\forall i,j\in\mathcal{N}$, i.e. DSGD belongs to the class of consensus-based distributed optimization methods, which can be achieved under assumptions \@ref(exr:dsgtgraph) and \@ref(exr:dsw) plus $W$ is symmetric. 

Let $X(k)=(x_1(k),...,x_n(k))^T\in\mathbb{R^{n\times p}}, G(k)=(g_1(x_1(k),\xi_1(k)),...,g_n(x_n(k),\xi_n(k)))^T\in\mathbb{R}^{n\times p}$, and $W=(w_{ij})\in\mathbb{R}^{n\times n}$, then we can rewrite \@ref(exm:dsgd) as 
\begin{equation}
X(k+1)=W(X(k)-\alpha_kG(k))
(\#eq:dsgdc)
\end{equation}

__Todo:$x_{i,k}\stackrel{?}=x_{j,k},\forall i,j\in\mathcal{N}$__ 

We compare DSGD with centralized stochastic gradient descent(SGD) which can query $n$ gradients at each iteration, 

```{example, sgd, name = "SGD"}
Initialize arbitrary $x_{0}\in\mathbb{R}^{p}$ and choose stepsize $\alpha_k$ for each step

For k=0,1,...,

  * $x(k+1)=x(k)-\alpha_k\bar g(k)$
  
```

where $\bar g(k)=\frac{1}{n}\sum\limits_{i=1}^n g_i(x(k),\xi_i(k)),\alpha_k=\frac{1}{\mu k}$, we use $\bar g(k)$ here to make the gradient comparable to that in DSGD, i.e., $\sum\limits_{j=1}^n w_{ij} g_j(x_j(k),\xi_j(k))$. 

Choose $2-$norm as the loss function, the distance between $x(k)$ and $x^*$ at the $k$th step is 
\begin{equation}
R(k)=E  \left[\Vert x(k)-x^*\Vert^2\right]=\frac{1}{n}\sum\limits_{i=1}^nE  \left[\Vert x(k)-x^*\Vert^2\right]
(\#eq:risksgd)
\end{equation} 
which hints us to evaluate the similar distance of DSGD by 
\begin{equation}
R'(k)=\frac{1}{n}\sum\limits_{i=1}^n E\left[\Vert x_i(k)-x^*\Vert^2\right]
(\#eq:riskdsgd)
\end{equation}

Additonnally, \@ref(eq:riskdsgd) can be divided into two sources, one from the optimization error, and one from the consensus error, i.e.,
\begin{equation}
R'(K)=\underbrace{E  \left[\Vert \bar x(k)-x^*\Vert^2\right]}_{\text{expected optimization error}} + 
\underbrace{\frac{1}{n}\sum_{i=1}^nE  \left[\Vert  x_i(k)-\bar x(k)^*\Vert^2\right]}_{\text{expected consensus error}}
(\#eq:drdsgd)
\end{equation}
This is because 
\begin{align}
\frac{1}{n}\sum_{i=1}^n E\left[\langle x_i(k)-\bar x, \bar x - x^*\rangle\right]&=E\left[\langle \frac{1}{n}\sum_{i=1}^nx_i(k)-\bar x, \bar x - x^*\rangle\right]\\
&=0
\end{align}

## Bounds

We next compare SGD and DSGD by analyzing their error bounds.

```{lemma, risksgd}
Let assumptions \@ref(exr:muL), \@ref(exr:estg), \@ref(exr:dsgtgraph),and \@ref(exr:dsw) plus $W$ is symmetric hold, for SGD \@ref(exm:sgd), we have
$$
R(k+1) \leq\left(1-\alpha_{k} \mu\right)^{2} R(k)+\frac{\alpha_{k}^{2} \sigma^{2}}{n}
$$
```


Denote $U(k)=E  \left[\Vert \bar x(k)-x^*\Vert^2\right]$ and $V(k)=\sum\limits_{i=1}^nE  \left[\Vert x_i(k)-\bar x(k)\right]$, we have 

```{lemma, riskdsgd}
Let the same assumptions in \@ref(lem:risksgd) hold, 
$$
U(k+1) \leq\left(1-\frac{1}{k}\right)^{2} U(k)+\frac{2 L}{\sqrt{n} \mu} \frac{\sqrt{U(k) V(k)}}{k}+\frac{L^{2}}{n \mu^{2}} \frac{V(k)}{k^{2}}+\frac{\sigma^{2}}{n \mu^{2}} \frac{1}{k^{2}}
$$
```

In chapter \@ref(sec:sharp), lemma \@ref(lem:sublrdsgd) shows that $\exists K_0, s.t.$ when $k>K_0, R'(k)\leq \frac{\hat W}{\tilde k}+\frac{\hat V}{\tilde k^2}$, where $\tilde k$ is some shift of $k$ with a choice of step size $\alpha_k=\frac{\theta}{\mu(k+K)},K:=\left\lceil\frac{2 \theta L^{2}}{\mu^{2}}\right\rceil$.

```{remark}
\\

- In a view that $R(k)$ and $R'(k)$ are both risk functions, if $V(k)$ decays fast enough compared to $U(k)$, we then have $R(k)\approx R'(k)$ for large $k$.

- the asymptotic network independence phenomenon: after a transient, DSGD performs comparably to a centralized stochastic gradient descent method with the same computational power.

```


## Possible ways to achieve asymptotic network independece

- Considering nonconvex objective functions(distributed training of deep neural networks);

- Explore communication reduction techniques that do not sacrifice the asymptotic network independece property;

- Redcing the transient time;

Additionnally, an unsolving question is can distributed methods compete with the centralized ones when the exact gradient is available?

__Todo: related reference__

### Compressed Communication

@koloskova2019decentralized proposed a decentralized stochastic method Choco-SGD(see algorithm \@ref(exm:chocosgd)) that converges at rate 
\begin{equation}
\mathcal{O}(\frac{1}{nk}+\frac{1}{(\delta^2w^k)^2})
\end{equation} 
where $\delta=1-|\lambda_2(W)|$ denotes the spectral gap of mixing matrix $W$, $w\leq1$ is the compression quality factor. This method conserve the asymptotic network independence and can be applied to a network where the nodes compress their model update with quality $0<w\leq1$. Addtionnally, the noise introduced by compression operator vanishes as $k\to\infty$. 

Different from DSGD, Choco-SGD \@ref(exm:chocosgd) use the averaged iterate $x_{\mathrm{avg}}=\frac{1}{S}\sum\limits_{k=1}^{k_{\mathrm{stop}}}w_k\bar x(k)$, i.e., theorem \@ref(thm:chocorate),

```{theorem, chocorate}
Let assumptions \@ref(exr:muL) and \@ref(exr:chocoF) hold, algorithm \@ref(exm:chocosgd) with SGD stepsize $\alpha_k=\frac{4}{\mu(a+k)}$, $a\geq\max\{\frac{410}{\delta^2w},16\kappa\}$ for $\kappa=\frac{L}{\mu}$, and consensus stepsize $\gamma=\frac{\delta^{2} \omega}{16 \delta+\delta^{2}+4 \beta^{2}+2 \delta \beta^{2}-8 \delta \omega}$ converges with the rate 
$$
\mathbb{E} \left[f(x_{\mathrm{avg}})-f(x^*)\right]=\mathcal{O}\left(\frac{\bar{\sigma}^{2}}{\mu n T}\right)+\mathcal{O}\left(\frac{\kappa G^{2}}{\mu \omega^{2} \delta^{4} T^{2}}\right)+\mathcal{O}\left(\frac{G^{2}}{\mu \omega^{3} \delta^{6} T^{3}}\right)
$$
where $x_{\mathrm{avg}}=\frac{1}{S}\sum\limits_{k=0}^{k_{\mathrm{stop}}}w_k\bar x(k)$ with weight $w_k=(a+k)^2$ and $S=\sum\limits_{k=0}^{k_{\mathrm{stop}}}w_k$
```

In the proof of theorem \@ref(thm:chocorate), the difference is that the author @koloskova2019decentralized deal the term 
$$
\bar x(k)-\alpha_k h(\bar x(k))-x^* 
$$
differently. They estimate $\Vert h(\bar x(k))\Vert^2$ and $\langle\bar x(k)-x^*,h(\bar x(k))\rangle$(from lemma \@ref(lem:Lmini), we have $f(\bar x(k))-f(x^*)$), while in DSGD, we use lemma \@ref(lem:contraction2L). Then based on a lemma from @stich2018sparsified, they finish the proof.

```{lemma, Lmini}
If $f$ has $L-$Lipschitz gradient with minimizer $x^*,s.t.\nabla f(x^*)=\mathbf{0}$, then 
$$
  \|\nabla f(x)\|^{2}=\left\|\nabla f(x)-\nabla f\left(x^{\star}\right)\right\|^{2} \leq 2 L\left(f(x)-f\left(x^{\star}\right)\right)
$$
```

Let $Q(\cdot):\mathbb{R}^{p}\to\mathbb{R}^p$ be a specific compression operator, which is known and satisfy assumption \@ref(exr:contractionEQ). $f_i(x):=E_{\xi_i\sim\mathcal{D}_i} F_i(x,\xi_i)$ for a loss function $F_i:\mathcal{R}^p\times \Omega_\boldsymbol\xi$, $f_i$ satisfy assumption \@ref(exr:muL) and distribution $\mathcal{D_1},...,\mathcal{D_n}$ can be different on every node.

```{exercise,contractionEQ}
The compression operator $Q(\cdot):\mathbb{R}^{p}\to\mathbb{R}^p$ satisfies, 
$$
  E_{Q}\|Q(x)-x\|^{2} \leq(1-\omega)\|x\|^{2}, x\in\mathbb{R}^p,
$$
for a parameter $w>0$. $E_Q$ denotes the expectation over the internal randomness of operator $Q$
```

The CHOCO-SGD method can be seen in algorithm \@ref(exm:chocosgd),
```{example, chocosgd, name = "CHOCO-SGD"}
Initialize $x_i(0)\in \mathbb{R}^{p}, \hat x_i(0)=\mathbf{0}\in\mathbb{R}^{p}$ for $i\in\mathcal{N}$, consensus stepsize $\gamma$, SGD stepsize $\alpha_k\geq0$,and mixing matrix $W=(w_{ij})\in\mathbb{R}^{n\times n}$

For $k=0,1,...$ do in parallel for all agents $i\in\mathcal{N}$,

  * Sample $\xi_i(k)$, compute  $g_i(k)=\nabla F_i(x_i(k),\xi_i(k))$
  
  * $x_i(k+\frac{1}{2})=x_i(k)-\alpha_kg_i(k)$ (stochastic gradient)

  * $q_i(k)=Q(x_i(k)-\hat x_i(k))$ (compression operator)

  * For all the neighbor $j$ of agents $i$, i.e., $(i,j)\in\mathcal{E}$,

    * Send $q_i(k)$ and receive $q_j(k)$
  
    * $\hat x_j(k+1)=\hat x_j(k)+q_j(k)$
  
  * $x_i(k+1)=x_i(k+\frac{1}{2})+\gamma \sum_{j:(i,j)\in\mathcal{E}} w_{ij}(\hat x_j(k+1)-\hat x_i(k+1))$

```


When the compression quality $w=1$,i.e. no communication compression, and consensus stepsize $\gamma=1$, the updation in algorithm \@ref(exm:choco) becomes 
$$
x_i(k+1)=\sum\limits_{i=1}^nw_{ij}(x_i(k)-\alpha_kg_i(k))
$$
which is the DSGD algorithm \@ref(exm:sgd). From the setting 
$$f_i(x):=E_{\xi_i\sim\mathcal{D}_i} F_i(x,\xi_i)$$
$g_i(k)$ satisify the unbiased property in assumption \@ref(exr:estg) automatically when $\nabla$ and expectation on $F_i$ can be interchanged. Despite the second property in assumption \@ref(exr:estg), we need to assum the second moment of $\nabla F_i(x,\xi_i)$ is finite,i.e., assumption \@ref(exr:chocoF). A little difference is that $\sigma$ can be different for each agent.

```{exercise, chocoF}
\begin{align}
E_{\xi_{i}}\left\|\nabla F_{i}\left(x, \xi_{i}\right)-\nabla f_{i}(x)\right\|^{2} &\leq \sigma_{i}^{2}, \quad \forall x \in \mathbb{R}^{d}, i \in\mathcal{N}\\
E_{\xi_{i}}\left\|\nabla F_{i}\left(x, \xi_{i}\right)\right\|^{2} &\leq G^{2},\quad\forall x \in \mathbb{R}^{d}, i \in\mathcal{N}
\end{align}

```



