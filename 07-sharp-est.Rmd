# A sharp estimate of the transient time of DSGD

In this chapter, we derive an estimate of transient time $K_T$ of DSGD and then show it is sharp[@pu2019sharp]. During this, we will prove lemma \@ref(lem:riskdsgd) in chapter \@ref(sec:asynt). 

Still, we set $f_i, i\in\mathcal{N}$ are $\mu-$strongly convex with $L-$Lipschitz continuous gradients, the graph $\mathcal{G}=(\mathcal{N},\mathcal{E})$ is connected and undirected, and we are able to obtain "good" gradient estimates $g_i(x_i(k),\xi_i(k))$, i.e. the assumptions \@ref(exr:muL), \@ref(exr:estg), \@ref(exr:dsgtgraph),and \@ref(exr:dsw) hold. 

We first derive the recursion of $R'(k),U(k),$ and $V(k)$ defined in chapter \@ref(sec:asynt).

## $U(k)$ and $V(k)$

To keep the consistency in notation, we still use $h(X)=\frac{1}{n}\mathbf{1}^T\nabla F(X)$, which the authors denote as $\bar\nabla F(X)$. Let $\bar g(k)=\frac{1}{n}\mathbf{1}^TG(X(k),\boldsymbol\xi(k))$^[In chapter \@ref(sec:asynt), $\bar g(k)=\frac{1}{n}\sum\limits_{i=1}^n g_i(x(k),\xi_i(k))$ ]. The idea is the same as that in \@ref(eq:xbar1dsgt), we have 
\begin{align}
\bar x(k+1)-x^*&=\bar x(k)-\alpha_k\left[\bar g(k)-\frac{1}{n}\mathbf{1}^T\nabla F(X(k))\right]-\\
&\alpha_k\left[\frac{1}{n}\mathbf{1}^T\nabla F(X(k))-\nabla f(\bar x(k))\right]-\alpha_k\nabla f(\bar x(k))-x^*\\
&:=(\bar x(k)-\alpha_k\nabla f(\bar x(k))-x^*)-\alpha_k(\bar g(k)-h(X(k)))-\\
&\alpha_k(h(X(k))-\nabla f(\bar x(k)))
(#eq:dsgdxbar1)
\end{align}

Then we can use lemma \@ref(lem:lem2) and assumption \@ref(exr:estg). Moreover, we use $|\langle a,b\rangle|\leq \Vert a\Vert \cdot\Vert b\Vert$ to deal with $<\bar x(k)-\alpha_k\nabla f(\bar x(k))-x^*,\nabla f(\bar x(k))-h(X(k))>$. Then take expectation conditioned on $X(k)$, we have lemma \@ref(lem:condxbardsgd). 


```{lemma, condxbardsgd}
For algorithm \@ref(exm:dsgd), $\forall k\geq0$, we have, 
\begin{align}
&E\left[\left\|\bar{x}(k+1)-x^{*}\right\|^{2} | X(k)\right] \leq\left\|\bar{x}(k)-\alpha_{k} \nabla f(\bar{x}(k))-x^{*}\right\|^{2}\\
&+\frac{2 \alpha_{k} L}{\sqrt{n}}\left\|\bar{x}(k)-\alpha_{k} \nabla f(\bar{x}(k))-x^{*}\right\|\|X(k)-\mathbf{1} \bar{x}(k)\|+\frac{\alpha_{k}^{2} L^{2}}{n}\|X(k)-\mathbf{1} \bar{x}(k)\|^{2}+\frac{\alpha_{k}^{2} \sigma^{2}}{n}
\end{align}
```

From lemma \@ref(lem:condxbardsgd), let $\alpha_k=\frac{1}{\mu k}<\frac{2}{\mu+L}$, use lemma \@ref(lem:lem31), and take full expectation on both sides, we derive lemma \@ref(lem:riskdsgd) in chapter \@ref(sec:asynt). 
\begin{align}
U(k+1)&\leq (1-\frac{1}{k})^2 U(k)+\frac{2L(1-\frac{1}{k})}{\mu k\sqrt{n}}E\left[\Vert \bar x(k)-x^*\Vert\cdot \Vert x(k)-\mathbf{1}\bar x(k)\Vert\right]+\\
&\frac{L^2}{\mu^2 k^2n}V(k)+\frac{\sigma^2}{\mu^2k^2n}\\
&\leq (1-\frac{1}{k})^2 U(k)+\frac{2L}{\mu\sqrt{n}}\frac{\sqrt{U(k)V(k)}}{k}+\frac{L^2}{\mu^2 k^2n}V(k)+\frac{\sigma^2}{\mu^2k^2n}
\end{align}
Where We use Cauchy-Schwartz inequality in the second inequality.

We can also separate $\frac{2 \alpha_{k} L}{\sqrt{n}}\left\|\bar{x}(k)-\alpha_{k} \nabla f(\bar{x}(k))-x^{*}\right\|\|X(k)-\mathbf{1} \bar{x}(k)\|$ by 
\begin{align}
&\frac{2 \alpha_{k} L}{\sqrt{n}}\left\|\bar{x}(k)-\alpha_{k} \nabla f(\bar{x}(k))-x^{*}\right\|\|X(k)-\mathbf{1} \bar{x}(k)\|\\
&\leq \lambda^2c\Vert \bar x(k)-x^*\Vert + \frac{L^2}{cn}\Vert X(k)-\mathbf{1}\bar x(k)\Vert
(\#eq:sepc)
\end{align}
for an arbitary $c>0$(in chapter \@ref(sec:dsgt), we let $c=\mu$). $\lambda$ comes from lemma \@ref(lem:contraction2L). Comparing lemma \@ref(lem:contraction2L) with lemma \@ref(lem:lem31), they only differ with the choice of $\alpha$. 

```{lemma, contraction2L}
Let assumption \@ref(exr:muL) holds, $\forall x\in\mathbb{R}^p$ and $\alpha\in(0,2/L)$, we have, 
$$
  \left\|x-\alpha \nabla f(x)-x^{*}\right\| \leq \lambda\left\|x-x^{*}\right\|
$$
where $\lambda=\max (|1-\alpha \mu|,|1-\alpha L|)$
```

From lemma \@ref(lem:condxbardsgd) and \@ref(eq:sepc), take the full expectation on both sides, we have for $\alpha_k\in(0,2/L)$, 

\begin{align}
U(k+1)\leq \lambda^2(1+c) U(k)+\frac{\alpha_k^2L^2}{cn}(1+\frac{1}{c})V(k)+\frac{\alpha_k\sigma^2}{n}
(\#eq:Uk1)
\end{align}

For $V(k+1)$, similar to \@ref(eq:ineq2dsgt), we denote $G(k):=G(X(k),\boldsymbol\xi(k))$ here, then 
\begin{align}
&E\left[\Vert X(k+1)-\mathbf{1}\bar x(k+1)\Vert^2|X(k)\right]\\
&=E\left[\Vert WX(k)-\alpha_kWG(k)-\mathbf{1}(\bar x(k)-\alpha_k\bar g(k))\Vert^2|X(k)\right]\\
&\leq \rho_w^2\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2+\alpha_k^2\rho_w^2E\left[\Vert G(k)-\mathbf{1}\bar g(k)\Vert^2|X(k)\right] -\\
&\quad 2\alpha_k\rho_w^2E\left[\langle X(k)-\mathbf{1}\bar x(k),G(k)-\mathbf{1}\bar g(k)\rangle|X(k)\right]\\
&\leq \rho_w^2\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2+\alpha_k^2\rho_w^2E\left[\Vert G(k)-\mathbf{1}\bar g(k)\Vert^2|X(k)\right] -\\
&\quad 2\alpha_k\rho_w^2E\left[\langle X(k)-\mathbf{1}\bar x(k),\nabla F(X(k))-\mathbf{1}h(X(k))\rangle|X(k)\right]\\
(\#eq:xbar1dsgd)
\end{align}

Next, we show 
\begin{align}
E\left[\Vert G(k)-\mathbf{1}\bar g(k)\Vert^2|X(k)\right]
&\leq\|\nabla F(X(k))-\mathbf{1} h(X(k))\|^{2}+n \sigma^{2}\\
\|\nabla F(X(k))-\mathbf{1} h(X(k))\|^{2}&\leq \Vert \nabla F(X(k))\Vert^2
(\#eq:xbar1dsgdl1)
\end{align}

This is because, 
\begin{align}
&E\left[\Vert G(k)-\mathbf{1}\bar g(k)\Vert^2|X(k)\right]\\
&=E\left[\Vert (G(k)-\nabla F(X(k)))-\mathbf{1}(\bar g(k)-h(X(k)))+(\nabla F(X(k))-\mathbf{1}h(X(k)))\Vert^2|X(k)\right]\\
&\leq E\left[\Vert G(k)-\nabla F(X(k)\Vert^2|X(k)\right]-nE\left[\Vert \bar g(k)-h(X(k)) \Vert^2|X(k)\right] + \\
&\quad\Vert \nabla F(X(k))-\mathbf{1}h(X(k))\Vert^2\\
&\leq n \sigma^{2}+ \|\nabla F(X(k))-\mathbf{1} h(X(k))\|^{2}\\
&\leq n\sigma^2 + \vert \nabla F(X(k))\Vert^2 +n\vert h(x)\Vert^2 - 2\langle\nabla F(X(k)),\mathbf{1}h(X(k))\rangle \\
&\leq n\sigma^2 + \Vert \nabla F(X(k))\Vert^2 
\end{align}

The last inequality is from $\langle\mathbf{A}, \mathbf{B}\rangle:=\sum_{i=1}^{n}\left\langle A_{i}, B_{i}\right\rangle$,i.e. 
\begin{align}
\langle\nabla F(X(k)),\mathbf{1}h(X(k))\rangle&=\sum_{i=1}^n\langle\nabla f_i(x_i(k)),\frac{1}{n}\sum_{j=1}^n\nabla f_j(x_j(k))\rangle\\
&=n\langle \frac{1}{n}\sum_{i=1}^n \nabla f_i(x_i(k)),\frac{1}{n}\sum_{j=1}^n\nabla f_j(x_j(k))\rangle\\
&=n\Vert h(X(k))\Vert^2
\end{align}


Next, we bound $\vert \nabla F(X(k))\Vert^2$. Recall $\nabla f$ is $L-$Lipschitz continuous, and we need $X(k)-\mathbf{1}\bar x(k)$ as well as $\bar x(k)-x^*$, so we have,

\begin{align}
&\Vert \nabla F(X(k))\Vert^2\\
&= \Vert \nabla F(X(k))-\nabla F(\mathbf{1}\bar x(k))+\nabla F(\mathbf{1}\bar x(k))-\nabla F(\mathbf{1}x^*)+\nabla F(\mathbf{1}x^*)\Vert^2\\
&\leq\left(L\|X(k)-\mathbf{1} \bar{x}(k)\|+\sqrt{n} L\left\|\bar{x}(k)-x^{*}\right\|+\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|\right)^{2}\\
&\stackrel{?}\leq 2 L^{2}\|X(k)-\mathbf{1} \bar{x}(k)\|^{2}+4 n L^{2}\left\|\bar{x}(k)-x^{*}\right\|^{2}+4\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|^{2}
(#eq:xbar1dsgdl2)
\end{align}

Use the two inequalities \@ref(eq:xbar1dsgdl1) and \@ref(eq:xbar1dsgdl2) in \@ref(eq:xbar1dsgd), we have 

\begin{align}
&\frac{1}{\rho_w^2}E\left[\Vert X(k+1)-\mathbf{1}\bar x(k+1)\Vert^2|X(k)\right]-\alpha_k^2n\sigma^2\\
&\leq \Vert X(k)-\mathbf{1}\bar x(k)\Vert^2 + \alpha_k^2 \Vert \nabla F(X(k))\Vert^2 + 2\alpha_k\Vert X(k)-\mathbf{1}\bar x(k)\Vert\cdot \Vert \nabla F(X(k))\Vert\\
\end{align}

For the last term, from \@ref(eq:xbar1dsgdl2), 
\begin{align}
&2\alpha_k\Vert X(k)-\mathbf{1}\bar x(k)\Vert\cdot \Vert \nabla F(X(k))\Vert\\
&\leq2\alpha_k \Vert X(k)-\mathbf{1}\bar x(k)\Vert\left(L\|X(k)-\mathbf{1} \bar{x}(k)\|+\sqrt{n} L\left\|\bar{x}(k)-x^{*}\right\|+\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|\right)\\
&\leq 2\alpha_kL\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2 + \left[c\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2+\frac{\alpha_k^2}{c}(\sqrt{n} L\left\|\bar{x}(k)-x^{*}\right\|+\| \nabla F\left(\mathbf{1} x^{*}\right)^2)\right]\\
&\leq (2\alpha_kL+c)\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2 + \frac{\alpha_k}{c}\left(2 n L^{2}\left\|\bar{x}(k)-x^{*}\right\|^{2}+2\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|^{2}\right)
\end{align}
For $\forall c>0$, the last inequality uses $(a+b)^2\leq 2(a^2+b^2)$. Let $c=\frac{1-\rho_w^2}{2}$, the same as that in \@ref(eq:ineq2dsgt) in chapter \@ref(sec:dsgt), then we have lemma \@ref(lem:Vk1).


```{lemma, Vk1}
Under algorithm \@ref(exm:dsgd), $\forall k\geq0$, 
\begin{align}
V(k+1)&\leq \rho_{w}^{2}\left(\frac{3-\rho_{w}^{2}}{2}+2 \alpha_{k} \rho_{w}^{2} L+2 \alpha_{k}^{2} \rho_{w}^{2} L^{2}\right) V(k) + \\
& \rho_{w}^{2} \alpha_{k}^{2}\left[\frac{8 n L^{2}}{\left(1-\rho_{w}^{2}\right)} U(k)+\frac{8\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|^{2}}{\left(1-\rho_{w}^{2}\right)}+n \sigma^{2}\right]
\end{align}
```

## Asymptotic network independence of DSGD 

In this section, we first show for algorithm \@ref(exm:dsgd), $U(k)=\mathcal O(\frac{1}{k})$ and $V(k)=\mathcal O(\frac{1}{k^2})$, i.e. algorithm \@ref(exm:dsgd) enjoys the sublinear convergence rate. More specifically, we show that $\exists N, s.t. k>N,$
\begin{equation}
U(k)\leq \frac{\hat W(1-\rho_w^2)}{\tilde k},\quad V(k)\leq \frac{\hat V((1-\rho_w^2,\hat W))}{\tilde k^2}
(\#eq:introsldsgd)
\end{equation}
where $\tilde k$ is some shift of $k$, $\hat W(\cdot)$ and $\hat V(\cdot)$ are functions. The goal is to show that asymptotically, $\frac{1}{n}\sum\limits_{i=1}^nE\left[\Vert x_i(k)-x^*\Vert^2\right]=R'(k)=U(k)+\frac{1}{n}V(k)$ has the same convergence rate with $R(k)$ in SGD. Notice $V(k)$ can be shown to decay faster than $U(k)$, so if we can bound $\hat W(1-\rho_w^2)$ by another quantity $C$ which does not depende on $\rho_w^2$, i.e. does not depende on the network, then we can have 
\begin{equation}
R'(k)\leq\frac{C}{\tilde k}+\frac{1}{n} \frac{V(1-\rho_w^2)}{\tilde k^2}
(\#eq:gdsgd)
\end{equation}
which shows the asymptotic newwork independence property of DSGD. Then by \@ref(eq:gdsgd), we can obtain the transient time for DSGD to reach the asymptotic convergence rate.

We first give a uniform bound for 
\begin{align}
E\left[\Vert X(k)-\mathbf{1}x^*\Vert^2\right]&=E\left[\sum_{i=1}^n\Vert x_i^T(k)-x^*\Vert^2\right]\\
&=\sum_{i=1}^n E\left[\Vert x_i^T(k)-x^*\right]\\
&=nR'(k)
\end{align}

```{lemma, unibounddsgd}
For algorithm \@ref(exm:dsgd), $\forall k\geq0$, 
$$
E\left[\left\|X(k)-1 x^{*}\right\|^{2}\right] \leq \hat{X}:=\max \left\{\left\|X(0)-1 x^{*}\right\|^{2}, \frac{9 \sum_{i=1}^{n}\left\|\nabla f_{i}\left(x^{*}\right)\right\|^{2}}{\mu^{2}}+\frac{n \sigma^{2}}{L^{2}}\right\}
$$
```

```{remark}
\\
- $X(k)-\mathbf{1}x^*=W^k(X(0)-\mathbf{1}x^*)-\sum\limits_{i+j=k}W^i\alpha_jG(j)\leqW^k(X(0)-\mathbf{1}x^*)$
  
- ?The author derive the second part in the $\max\{\cdot\}$ by $E\left[\|X(k)\|^{2}\right] \leq \max \left\{\|X(0)\|^{2}, \sum_{i=1}^{n} R_{i}\right\}$

```

