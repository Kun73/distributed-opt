# A sharp estimate of the transient time of DSGD{#sec:sharp}

In this chapter, we derive an estimate of transient time $K_T$ of DSGD and then show it is sharp[@pu2019sharp]. During this, we will prove lemma \@ref(lem:riskdsgd) in chapter \@ref(sec:asynt). 

Still, we set $f_i, i\in\mathcal{N}$ are $\mu-$strongly convex with $L-$Lipschitz continuous gradients, the graph $\mathcal{G}=(\mathcal{N},\mathcal{E})$ is connected and undirected, and we are able to obtain "good" gradient estimates $g_i(x_i(k),\xi_i(k))$, i.e. the assumptions \@ref(exr:muL), \@ref(exr:estg), \@ref(exr:dsgtgraph),and \@ref(exr:dsw) hold. 

We first derive the recursion of $R'(k),U(k),$ and $V(k)$ defined in chapter \@ref(sec:asynt).

## $U(k)$ and $V(k)$

To keep the consistency in notation, we still use $h(X)=\frac{1}{n}\mathbf{1}^T\nabla F(X)$, which the authors denote as $\bar\nabla F(X)$. Let $\bar g(k)=\frac{1}{n}\mathbf{1}^TG(X(k),\boldsymbol\xi(k))$^[In chapter \@ref(sec:asynt), $\bar g(k)=\frac{1}{n}\sum\limits_{i=1}^n g_i(x(k),\xi_i(k))$ ]. The idea is the same as that in \@ref(eq:xbar1dsgt), we have 
\begin{align}
\bar x(k+1)-x^*&=\bar x(k)-\alpha_k\left[\bar g(k)-\frac{1}{n}\mathbf{1}^T\nabla F(X(k))\right]-\\
&\alpha_k\left[\frac{1}{n}\mathbf{1}^T\nabla F(X(k))-\nabla f(\bar x(k))\right]-\alpha_k\nabla f(\bar x(k))-x^*\\
&:=(\bar x(k)-\alpha_k\nabla f(\bar x(k))-x^*)-\alpha_k(\bar g(k)-h(X(k)))-\\
&\alpha_k(h(X(k))-\nabla f(\bar x(k)))
(#eq:dsgdxbar1)
\end{align}

Then we can use lemma \@ref(lem:lem2) and assumption \@ref(exr:estg). Moreover, we use $|\langle a,b\rangle|\leq \Vert a\Vert \cdot\Vert b\Vert$ to deal with $<\bar x(k)-\alpha_k\nabla f(\bar x(k))-x^*,\nabla f(\bar x(k))-h(X(k))>$. Then take expectation conditioned on $X(k)$, we have lemma \@ref(lem:condxbardsgd). 


```{lemma, condxbardsgd}
For algorithm \@ref(exm:dsgd), $\forall k\geq0$, we have, 
\begin{align}
&E\left[\left\|\bar{x}(k+1)-x^{*}\right\|^{2} | X(k)\right] \leq\left\|\bar{x}(k)-\alpha_{k} \nabla f(\bar{x}(k))-x^{*}\right\|^{2}\\
&+\frac{2 \alpha_{k} L}{\sqrt{n}}\left\|\bar{x}(k)-\alpha_{k} \nabla f(\bar{x}(k))-x^{*}\right\|\|X(k)-\mathbf{1} \bar{x}(k)\|+\frac{\alpha_{k}^{2} L^{2}}{n}\|X(k)-\mathbf{1} \bar{x}(k)\|^{2}+\frac{\alpha_{k}^{2} \sigma^{2}}{n}
\end{align}
```

From lemma \@ref(lem:condxbardsgd), let $\alpha_k=\frac{1}{\mu k}<\frac{2}{\mu+L}$, use lemma \@ref(lem:lem31), and take full expectation on both sides, we derive lemma \@ref(lem:riskdsgd) in chapter \@ref(sec:asynt). 
\begin{align}
U(k+1)&\leq (1-\frac{1}{k})^2 U(k)+\frac{2L(1-\frac{1}{k})}{\mu k\sqrt{n}}E\left[\Vert \bar x(k)-x^*\Vert\cdot \Vert x(k)-\mathbf{1}\bar x(k)\Vert\right]+\\
&\frac{L^2}{\mu^2 k^2n}V(k)+\frac{\sigma^2}{\mu^2k^2n}\\
&\leq (1-\frac{1}{k})^2 U(k)+\frac{2L}{\mu\sqrt{n}}\frac{\sqrt{U(k)V(k)}}{k}+\frac{L^2}{\mu^2 k^2n}V(k)+\frac{\sigma^2}{\mu^2k^2n}
\end{align}
Where We use Cauchy-Schwartz inequality in the second inequality.

We can also separate $\frac{2 \alpha_{k} L}{\sqrt{n}}\left\|\bar{x}(k)-\alpha_{k} \nabla f(\bar{x}(k))-x^{*}\right\|\|X(k)-\mathbf{1} \bar{x}(k)\|$ by 
\begin{align}
&\frac{2 \alpha_{k} L}{\sqrt{n}}\left\|\bar{x}(k)-\alpha_{k} \nabla f(\bar{x}(k))-x^{*}\right\|\|X(k)-\mathbf{1} \bar{x}(k)\|\\
&\leq \lambda^2c\Vert \bar x(k)-x^*\Vert + \frac{L^2}{cn}\Vert X(k)-\mathbf{1}\bar x(k)\Vert
(\#eq:sepc)
\end{align}
for an arbitary $c>0$(in chapter \@ref(sec:dsgt), we let $c=\mu$). $\lambda$ comes from lemma \@ref(lem:contraction2L). Comparing lemma \@ref(lem:contraction2L) with lemma \@ref(lem:lem31), they only differ with the choice of $\alpha$. 

```{lemma, contraction2L}
Let assumption \@ref(exr:muL) holds, $\forall x\in\mathbb{R}^p$ and $\alpha\in(0,2/L)$, we have, 
$$
  \left\|x-\alpha \nabla f(x)-x^{*}\right\| \leq \lambda\left\|x-x^{*}\right\|
$$
where $\lambda=\max (|1-\alpha \mu|,|1-\alpha L|)$
```

From lemma \@ref(lem:condxbardsgd) and \@ref(eq:sepc), take the full expectation on both sides, we have for $\alpha_k\in(0,2/L)$, 

\begin{align}
U(k+1)\leq \lambda^2(1+c) U(k)+\frac{\alpha_k^2L^2}{cn}(1+\frac{1}{c})V(k)+\frac{\alpha_k\sigma^2}{n}
(\#eq:Uk1)
\end{align}

Take $c=\frac{3}{8}\alpha_k\mu$ and let $\alpha_k\leq\min\{\frac{1}{L},\frac{1}{3\mu}\}$ in \@ref(eq:Uk1), we derive lemma \@ref(lem:Uk1). $\alpha_k\leq \frac{1}{L}$ is to make $\lambda = 1-\alpha_k\mu$ in lemma \@ref(lem:contraction2L), $\alpha_k\leq \frac{1}{3\mu}$ is to make $(1+c)\lambda^2\leq 1-\frac{3}{2}\alpha_k\mu$.

__Todo:why $1-\frac{3}{2}\alpha_k\mu$__

```{lemma, Uk1}
Under algorithm \@ref(exm:dsgd), let $\alpha_k\leq\min\{\frac{1}{L},\frac{1}{3\mu}\}$, then 
$$
U(k+1) \leq\left(1-\frac{3}{2} \alpha_{k} \mu\right) U(k)+\frac{3 \alpha_{k} L^{2}}{n \mu} V(k)+\frac{\alpha_{k}^{2} \sigma^{2}}{n}
$$
```

For $V(k+1)$, similar to \@ref(eq:ineq2dsgt), we denote $G(k):=G(X(k),\boldsymbol\xi(k))$ here, then 
\begin{align}
&E\left[\Vert X(k+1)-\mathbf{1}\bar x(k+1)\Vert^2|X(k)\right]\\
&=E\left[\Vert WX(k)-\alpha_kWG(k)-\mathbf{1}(\bar x(k)-\alpha_k\bar g(k))\Vert^2|X(k)\right]\\
&\leq \rho_w^2\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2+\alpha_k^2\rho_w^2E\left[\Vert G(k)-\mathbf{1}\bar g(k)\Vert^2|X(k)\right] -\\
&\quad 2\alpha_k\rho_w^2E\left[\langle X(k)-\mathbf{1}\bar x(k),G(k)-\mathbf{1}\bar g(k)\rangle|X(k)\right]\\
&\leq \rho_w^2\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2+\alpha_k^2\rho_w^2E\left[\Vert G(k)-\mathbf{1}\bar g(k)\Vert^2|X(k)\right] -\\
&\quad 2\alpha_k\rho_w^2E\left[\langle X(k)-\mathbf{1}\bar x(k),\nabla F(X(k))-\mathbf{1}h(X(k))\rangle|X(k)\right]\\
(\#eq:xbar1dsgd)
\end{align}

Next, we show 
\begin{align}
E\left[\Vert G(k)-\mathbf{1}\bar g(k)\Vert^2|X(k)\right]
&\leq\|\nabla F(X(k))-\mathbf{1} h(X(k))\|^{2}+n \sigma^{2}\\
\|\nabla F(X(k))-\mathbf{1} h(X(k))\|^{2}&\leq \Vert \nabla F(X(k))\Vert^2
(\#eq:xbar1dsgdl1)
\end{align}

This is because, 
\begin{align}
&E\left[\Vert G(k)-\mathbf{1}\bar g(k)\Vert^2|X(k)\right]\\
&=E\left[\Vert (G(k)-\nabla F(X(k)))-\mathbf{1}(\bar g(k)-h(X(k)))+(\nabla F(X(k))-\mathbf{1}h(X(k)))\Vert^2|X(k)\right]\\
&\leq E\left[\Vert G(k)-\nabla F(X(k)\Vert^2|X(k)\right]-nE\left[\Vert \bar g(k)-h(X(k)) \Vert^2|X(k)\right] + \\
&\quad\Vert \nabla F(X(k))-\mathbf{1}h(X(k))\Vert^2\\
&\leq n \sigma^{2}+ \|\nabla F(X(k))-\mathbf{1} h(X(k))\|^{2}\\
&\leq n\sigma^2 + \vert \nabla F(X(k))\Vert^2 +n\vert h(x)\Vert^2 - 2\langle\nabla F(X(k)),\mathbf{1}h(X(k))\rangle \\
&\leq n\sigma^2 + \Vert \nabla F(X(k))\Vert^2 
\end{align}

The last inequality is from $\langle\mathbf{A}, \mathbf{B}\rangle:=\sum_{i=1}^{n}\left\langle A_{i}, B_{i}\right\rangle$,i.e. 
\begin{align}
\langle\nabla F(X(k)),\mathbf{1}h(X(k))\rangle&=\sum_{i=1}^n\langle\nabla f_i(x_i(k)),\frac{1}{n}\sum_{j=1}^n\nabla f_j(x_j(k))\rangle\\
&=n\langle \frac{1}{n}\sum_{i=1}^n \nabla f_i(x_i(k)),\frac{1}{n}\sum_{j=1}^n\nabla f_j(x_j(k))\rangle\\
&=n\Vert h(X(k))\Vert^2
\end{align}


Next, we bound $\vert \nabla F(X(k))\Vert^2$. Recall $\nabla f$ is $L-$Lipschitz continuous, and we need $X(k)-\mathbf{1}\bar x(k)$ as well as $\bar x(k)-x^*$, so we have,

\begin{align}
&\Vert \nabla F(X(k))\Vert^2\\
&= \Vert \nabla F(X(k))-\nabla F(\mathbf{1}\bar x(k))+\nabla F(\mathbf{1}\bar x(k))-\nabla F(\mathbf{1}x^*)+\nabla F(\mathbf{1}x^*)\Vert^2\\
&\leq\left(L\|X(k)-\mathbf{1} \bar{x}(k)\|+\sqrt{n} L\left\|\bar{x}(k)-x^{*}\right\|+\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|\right)^{2}\\
&\stackrel{?}\leq 2 L^{2}\|X(k)-\mathbf{1} \bar{x}(k)\|^{2}+4 n L^{2}\left\|\bar{x}(k)-x^{*}\right\|^{2}+4\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|^{2}
(#eq:xbar1dsgdl2)
\end{align}

Use the two inequalities \@ref(eq:xbar1dsgdl1) and \@ref(eq:xbar1dsgdl2) in \@ref(eq:xbar1dsgd), we have 

\begin{align}
&\frac{1}{\rho_w^2}E\left[\Vert X(k+1)-\mathbf{1}\bar x(k+1)\Vert^2|X(k)\right]-\alpha_k^2n\sigma^2\\
&\leq \Vert X(k)-\mathbf{1}\bar x(k)\Vert^2 + \alpha_k^2 \Vert \nabla F(X(k))\Vert^2 + 2\alpha_k\Vert X(k)-\mathbf{1}\bar x(k)\Vert\cdot \Vert \nabla F(X(k))\Vert\\
\end{align}

For the last term, from \@ref(eq:xbar1dsgdl2), 
\begin{align}
&2\alpha_k\Vert X(k)-\mathbf{1}\bar x(k)\Vert\cdot \Vert \nabla F(X(k))\Vert\\
&\leq2\alpha_k \Vert X(k)-\mathbf{1}\bar x(k)\Vert\left(L\|X(k)-\mathbf{1} \bar{x}(k)\|+\sqrt{n} L\left\|\bar{x}(k)-x^{*}\right\|+\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|\right)\\
&\leq 2\alpha_kL\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2 + \left[c\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2+\frac{\alpha_k^2}{c}(\sqrt{n} L\left\|\bar{x}(k)-x^{*}\right\|+\| \nabla F\left(\mathbf{1} x^{*}\right)^2)\right]\\
&\leq (2\alpha_kL+c)\Vert X(k)-\mathbf{1}\bar x(k)\Vert^2 + \frac{\alpha_k}{c}\left(2 n L^{2}\left\|\bar{x}(k)-x^{*}\right\|^{2}+2\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|^{2}\right)
\end{align}
For $\forall c>0$, the last inequality uses $(a+b)^2\leq 2(a^2+b^2)$. Let $c=\frac{1-\rho_w^2}{2}$, the same as that in \@ref(eq:ineq2dsgt) in chapter \@ref(sec:dsgt), then we have lemma \@ref(lem:Vk1).


```{lemma, Vk1}
Under algorithm \@ref(exm:dsgd), $\forall k\geq0$, 
\begin{align}
V(k+1)&\leq \rho_{w}^{2}\left(\frac{3-\rho_{w}^{2}}{2}+2 \alpha_{k} \rho_{w}^{2} L+2 \alpha_{k}^{2} \rho_{w}^{2} L^{2}\right) V(k) + \\
& \rho_{w}^{2} \alpha_{k}^{2}\left[\frac{8 n L^{2}}{\left(1-\rho_{w}^{2}\right)} U(k)+\frac{8\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|^{2}}{\left(1-\rho_{w}^{2}\right)}+n \sigma^{2}\right]
\end{align}
```

## Asymptotic network independence of DSGD 

In this section, we first show for algorithm \@ref(exm:dsgd), $U(k)=\mathcal O(\frac{1}{k})$ and $V(k)=\mathcal O(\frac{1}{k^2})$, i.e. algorithm \@ref(exm:dsgd) enjoys the sublinear convergence rate. More specifically, we show that $\exists N, s.t. k>N,$
\begin{equation}
U(k)\leq \frac{\hat W(1-\rho_w^2)}{\tilde k},\quad V(k)\leq \frac{\hat V((1-\rho_w^2,\hat W))}{\tilde k^2}
(\#eq:introsldsgd)
\end{equation}
where $\tilde k$ is some shift of $k$, $\hat W(\cdot)$ and $\hat V(\cdot)$ are functions. The goal is to show that asymptotically, $\frac{1}{n}\sum\limits_{i=1}^nE\left[\Vert x_i(k)-x^*\Vert^2\right]=R'(k)=U(k)+\frac{1}{n}V(k)$ has the same convergence rate with $R(k)$ in SGD. Notice $V(k)$ can be shown to decay faster than $U(k)$, so if we can bound $\hat W(1-\rho_w^2)$ by another quantity $C$ which does not depende on $\rho_w^2$, i.e. does not depende on the network, then we can have 
\begin{equation}
R'(k)\leq\frac{C}{\tilde k}+\frac{1}{n} \frac{V(1-\rho_w^2)}{\tilde k^2}
(\#eq:gdsgd)
\end{equation}
which shows the asymptotic newwork independence property of DSGD. Then by \@ref(eq:gdsgd), we can obtain the transient time for DSGD to reach the asymptotic convergence rate.

We first give a uniform bound for 
\begin{align}
E\left[\Vert X(k)-\mathbf{1}x^*\Vert^2\right]&=E\left[\sum_{i=1}^n\Vert x_i^T(k)-x^*\Vert^2\right]\\
&=\sum_{i=1}^n E\left[\Vert x_i^T(k)-x^*\right]\\
&=nR'(k)
\end{align}

```{lemma, unibounddsgd}
For algorithm \@ref(exm:dsgd), $\forall k\geq0$, 
$$
E\left[\left\|X(k)-1 x^{*}\right\|^{2}\right] \leq \hat{X}:=\max \left\{\left\|X(0)-1 x^{*}\right\|^{2}, \frac{9 \sum_{i=1}^{n}\left\|\nabla f_{i}\left(x^{*}\right)\right\|^{2}}{\mu^{2}}+\frac{n \sigma^{2}}{L^{2}}\right\}
$$
```

```{remark}
\\

- $X(k)-\mathbf{1}x^*=W^k(X(0)-\mathbf{1}x^*)-\sum\limits_{i+j=k}W^i\alpha_jG(j)\leq W^k(X(0)-\mathbf{1}x^*)$
  
- ?The author derive the second part in the $\max\{\cdot\}$ by $E\left[\|X(k)\|^{2}\right] \leq \max \left\{\|X(0)\|^{2}, \sum_{i=1}^{n} R_{i}\right\}$

```

### Sublinear rate

Recall $R'(k)=U(k)+\frac{1}{n}V(k)$, hence we introduce $W(k)$ as 
\begin{equation}
W(k):=U(k)+\omega(k)V(k),\quad \forall k\geq0
(\#eq:lya)
\end{equation}
where $\omega(k)>0$ is to be determined later. \@ref(eq:lya) is called the Lyapunov function.

From lemma \@ref(lem:unibounddsgd), we have a uniform bound for $R'(k)\leq \frac{\hat X}{n}$, we also want such a property on $W(k)$,i.e. $W(k)\leq\frac{\hat X}{n}$. This is how we determine $\omega(k)$. From lemma \@ref(lem:Uk1) and \@ref(lem:Vk1), we further establish a recursion of $W(k)$. By induction, it will have a term of $\prod\limits_{t=a}^{k-1}(1-\frac{\gamma}{t})$. Lemma \@ref(lem:prodineq) leads us to bound such a term. Then $U(k)$ is bounded from $U(k)\leq W(k)$. $V(k)$ is bound from lemma \@ref(lem:Vk1) and the bound for $U(k)$.

```{lemma, prodineq}
$\forall 1<a<k(a\in\mathbb{N})$ and $1<\gamma<a/2$, 
$$
\frac{a^{2 \gamma}}{k^{2 \gamma}} \leq \prod_{t=a}^{k-1}\left(1-\frac{\gamma}{t}\right) \leq \frac{a^{\gamma}}{k^{\gamma}}
$$  
```

Lemma \@ref(lem:sublrdsgd) shows the algorithm \@ref(exm:dsgd) with the stepsize policy being 
$$
\alpha_k=\frac{\theta}{\mu(k+K)},K:=\left\lceil\frac{2 \theta L^{2}}{\mu^{2}}\right\rceil
$$
enjoys sublinear rate. 

```{lemma sublrdsgd, name = "Sublinear rate of DSGD"}
For algorithm \@ref(exm:dsgd),let
\begin{equation*}
  K_{1}:=\left\lceil\frac{24 L^{2} \theta}{\left(1-\rho_{w}^{2}\right) \mu^{2}}\right\rceil
\end{equation*}
$\forall k\geq K_1-K$, we have 
\begin{equation*}
  U(k)\leq \frac{\hat W}{\tilde k},\quad V(k)\leq \frac{\hat V}{\tilde k^2}
\end{equation*}
where,
$$
  \hat{W}:=\frac{K_{1} \hat{X}}{n}+\frac{3}{(4 \theta-3)}\left(\frac{\sigma^{2} \theta^{2}}{n \mu^{2}}+\frac{\sigma^{2} \rho_{w}^{2} \theta^{2}}{2 \mu^{2}}\right)+\frac{12\left\|\nabla F\left(1 x^{*}\right)\right\|^{2} \rho_{w}^{2} \theta^{2}}{(4 \theta-3) n \mu^{2}\left(1-\rho_{w}^{2}\right)}
$$
  
$$
  \hat{V}:=\max \left\{K_{1}^{2} \hat{X}, \frac{8 \theta^{2} \rho_{w}^{2}}{\mu^{2}\left(1-\rho_{w}^{2}\right)}\left[\frac{4\left\|\nabla F\left(\mathbf{1} x^{*}\right)\right\|^{2}}{\left(1-\rho_{w}^{2}\right)}+n \sigma^{2}+\frac{4 n L^{2} \hat{W}}{\left(1-\rho_{w}^{2}\right) K_{1}}\right]\right\}
$$

$$
  \tilde k=k+K
$$

```

We introduce $\tilde k$ is based on $\alpha_k=\frac{c}{k+K}$. Moreover, we introduce the following shift of $U(k),V(k)$, and $W(k)$ for simplicity.
\begin{equation}
\tilde{U}(k):=U(k-K), \quad \tilde{V}(k):=V(k-K), \quad \tilde{W}(k):=W(k-K), \quad \forall k \geq K
(#eq:shift)
\end{equation}

The uniform bound of $R'(k)$ (lemma \@ref(lem:unibounddsgd)) gives bound for the above quantities. $\tilde U(k)\leq \frac{\hat X}{n}, \tilde V(k)\leq \hat X$, and $\tilde W(k)\leq\frac{\hat X}{n}$. This can also be seen from the definition in \@ref(eq:shift), which moves $U(k),V(k)$ and $W(k)$ horizontally.

### Asymptotic network independence

In this section, we show that the asymptotic network independence of algorithm \@ref(exm:dsgd) with the stepsize policy being 
$\alpha_k=\frac{\theta}{\mu(k+K)}$. That is, although $R'(k)=U(k)+\frac{1}{n}V(k)$ depends on the network involving the term of $1-\rho_w^2$, we show that part will decay faster. Then after some iterations ($\exists K_0$, when $k\geq K_0$), $R(k)\leq \frac{C}{\tilde k}, \exists C$.

From lemma \@ref(lem:Uk1), substitute $\alpha_k=\frac{\theta}{\mu(k+K)}$ and let $k'=k+K$, then 
\begin{equation}
\tilde{U}(k'-K+1) \leq\left(1-\frac{3 \theta}{2 k'}\right) \tilde{U}(k'-K)+\frac{3 \theta L^{2}}{n \mu^{2}} \frac{\tilde{V}(k'-K)}{k'}+\frac{\theta^{2} \sigma^{2}}{n \mu^{2}} \frac{1}{k'^{2}}, \quad \forall k' \geq K_{1}
\end{equation}

Then from \@ref(eq:shift), we simplify the above as 
\begin{equation}

\tilde{U}(k+1) \leq\left(1-\frac{3 \theta}{2 k}\right) \tilde{U}(k)+\frac{3 \theta L^{2}}{n \mu^{2}} \frac{\tilde{V}(k)}{k}+\frac{\theta^{2} \sigma^{2}}{n \mu^{2}} \frac{1}{k^{2}}, \quad \forall k \geq K_{1}
(#eq:Ushift)
\end{equation}

By induction and lemma \@ref(lem:prodineq), we bound $U(k)$ with the network dependent term decaying faster, i.e. theorem \@ref(thm:asymUk). Additionally, from lemma \@ref(lem:sublrdsgd), $\tilde V(k)=V(k-K)\leq \frac{\hat V}{k^2}$, \@ref(eq:Ushift) becomes 
\begin{align}
\tilde U(k)\leq \frac{K_{1}^{1.5 \theta}}{k^{1.5 \theta}} \tilde{U}\left(K_{1}\right)+\sum_{t=K_{1}}^{k-1} \frac{(t+1)^{1.5 \theta}}{k^{1.5 \theta}}\left(\frac{\theta^{2} \sigma^{2}}{n \mu^{2} t^{2}}+\frac{3 \theta L^{2}}{n \mu^{2}} \frac{\hat V}{t^3}\right)
(\#eq:inductiondsgd)
\end{align}
The inequality holds for $1<\frac{3\theta}{2}\leq \frac{K_1}{2}$, where $K_1:=\left\lceil\frac{24 L^{2} \theta}{\left(1-\rho_{w}^{2}\right) \mu^{2}}\right\rceil$. To simplify \@ref(eq:inductiondsgd), we bound 
$$
\sum_{t=K_1}^{k-1}\frac{(t+1)^{1.5\theta}}{t^2},\quad \sum_{t=K_1}^{k-1}\frac{(t+1)^{1.5\theta}}{t^3}
$$



```{theorem, asymUk}
Under algorithm \@ref(exm:dsgd) with $\alpha_k=\frac{\theta}{\mu(k+K)}$, let $\theta>2, K:=\left\lceil\frac{2 \theta L^{2}}{\mu^{2}}\right\rceil$, and $K_{1}:=\left\lceil\frac{24 L^{2} \theta}{\left(1-\rho_{w}^{2}\right) \mu^{2}}\right\rceil$, we have
$$
U(k) \leq \frac{\theta^{2} \sigma^{2}}{(1.5 \theta-1) n \mu^{2} \tilde{k}}+\left[\frac{3 \theta^{2}(1.5 \theta-1) \sigma^{2}}{(1.5 \theta-2) n \mu^{2}}+\frac{6 \theta L^{2} \hat{V}}{(1.5 \theta-2) n \mu^{2}}\right] \frac{1}{\tilde{k}^{2}}, \quad \forall k \geq K_{1}-K
$$
```
Where $\hat V=\mathcal{O}(\frac{n}{(1-\rho_w)^2})$ with some constraints on $\Vert X(0)-\mathbf{1}x^*\Vert^2$ and $\Vert \nabla F(\mathbf{1}x^*)\Vert^2$ according to lemma \@ref(lem:hatVdsgd). 

```{lemma, hatVdsgd}
Suppose $\sum\limits_{i=1}^{n}\left\|x_{i}(0)-x^{*}\right\|^{2}=\mathcal{O}(n) \text { and } \sum\limits_{i=1}^{n}\left\|\nabla f_{i}\left(x^{*}\right)\right\|^{2}=\mathcal{O}(n)$, then 
$$
\hat V=\mathcal{O}(\frac{n}{(1-\rho_w)^2})
$$

```


